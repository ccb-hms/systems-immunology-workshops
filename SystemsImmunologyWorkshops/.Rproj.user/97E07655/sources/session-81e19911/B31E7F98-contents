---
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: false
    toc_depth: 3
    fig_width: 5
vignette: >
  %\VignetteIndexEntry{Hypothesis testing essentials}
  %\VignetteEncoding[utf8]{inputenc}  
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

# Hypothesis testing

In biological data analysis, hypothesis testing is applied to screen
thousands or millions of possible hypotheses to find the ones that are
worth following up. For instance, researchers screen genetic variants
for associations with a phenotype, or gene expression levels for
associations with disease.

## An example: coin tossing

Suppose we are flipping a biased coin and want to test the null
hypothesis that the coin is fair. We simulate a 100 coin flips with a
probability for heads different from 0.5.

```{r}
numFlips <- 100
probHead <- 0.6
coinFlips <- sample(c("H", "T"), size = numFlips,
                    replace = TRUE, prob = c(probHead, 1 - probHead))
head(coinFlips)
```

If the coin were fair, we would expect half of the time to get heads.

```{r}
table(coinFlips)
```

That is different from 50/50. However, does the data deviates strong
enough to conclude that this coin isn't fair?

We know that the total number of heads seen in 100 coin tosses for a
fair coin follows $B(100, 0.5)$, making it a suitable test statistic.

Let's inspect the sampling distribution of our test statistic $k$.

```{r}
k <- 0:numFlips
numHeads <- sum(coinFlips == "H")
p <- dbinom(k, size = numFlips, prob = 0.5)
binomDensity <- data.frame(k = k, p = p)
head(binomDensity)
```

We can illustrate the distribution and mark the observed value
`r numHeads` with a vertical blue line.

```{r, message=FALSE}
library("ggplot2")
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p), stat = "identity") +
  geom_vline(xintercept = numHeads, col = "blue")
```

**Task**: Add a red horizontal line to the plot at a probability of
0.05.

Suppose we didn't know about the binomial distribution. We could still
use simulation to get an idea of the null distribution:

```{r, warning=FALSE}
numSimulations <- 10000

sim.flip <- function()
{
    coinFlips <- sample(c("H", "T"), size = numFlips,
                        replace = TRUE, prob = c(0.5, 0.5))
    sum(coinFlips == "H")
}

outcome <- replicate(numSimulations, sim.flip())

ggplot(data.frame(outcome)) + xlim(0, 100) +
  geom_histogram(aes(x = outcome), binwidth = 1, center = 0) +
  geom_vline(xintercept = numHeads, col = "blue")
```

As expected, the most likely number of heads is 50 (half the number of
coin flips). But other numbers near 50 are also quite likely.

How do we quantify whether the observed value, `r numHeads`, is among
those values that we are likely to see from a fair coin? Or whether its
deviation from the expected value is already large enough for us to
conclude with enough confidence that the coin is biased?

We divide the set of all possible $k$ (0 to 100) in two complementary
subsets, *the rejection region* and the region of no rejection.

In practice, we keep the probability of $k$ falling into the rejection
region below some threshold $\alpha$ (typically 0.05).

```{r}
alpha <- 0.05
binomDensity <- binomDensity[order(p),]
binomDensity$reject <- cumsum(binomDensity$p) <= alpha
head(binomDensity)

ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, col = reject), stat = "identity") +
  scale_colour_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) +
  geom_vline(xintercept = numHeads, col = "blue") +
  theme(legend.position = "none")
```

We sorted the $p$-values from lowest to highest (`order`), and added a
column `reject` by computing the cumulative sum (`cumsum`) of the
$p$-values and thresholding it against `alpha`.

The logical column `reject` therefore marks with `TRUE` a set of $k$s
whose total probability is less than $\alpha$.

The rejection region is marked in red, containing both very large and
very small values of $k$, which can be considered unlikely under the
null hypothesis.

`R` provides not only functions for the densities (e.g., `dbinom`) but
also for the cumulative distribution functions (`pbinom`). Those are
more precise and faster than `cumsum` over the probabilities.

The (cumulative) *distribution function* is defined as the probability
that a random variable $X$ will take a value less than or equal to $x$.

$$F(x) = P(X \le x)$$

We have just gone through the steps of a **binomial test**. This is a
frequently used test and therefore available in `R` as a single
function.

```{r}
binom.test(x = numHeads, n = numFlips, p = 0.5)
```

$p$-value: probability of observing a value of the test statistic *at
least as extreme* as observed (under the null), i.e.

$$P(X \ge x) = P(X > x - 1) = 1 - P(X \le x - 1) = 1 - F(x-1)$$

```{r}
(1 - pbinom(numHeads - 1, size = 100, prob = 0.5)) * 2
```

**Task**: Conduct a binomial test for the following scenario: out of 1
million reads, 19 reads are mapped to a gene of interest, with the
probability for mapping a read to that gene being $10^{-5}$. Are these
more or less reads than we would expect to be mapped to that gene? Is
the finding statistically significant?

## The five steps of hypothesis testing

1.  Collect the data (given suitable experiment or study).

2.  Set up null hypothesis: simple, computationally tractable model of
    reality; possible to compute the null distribution. (null
    distribution: represents possible values of test statistic and their
    probabilities *under assumption that null hypothesis is true*).

3.  Decide on *rejection region* (subset of possible values of test
    statistic, total probability is small).

4.  Define and compute test statistic.

5.  Make a decision: reject null hypothesis (conclude that it is
    unlikely to be true) if test statistic is in rejection region.

**Step 3: The rejection region**

How to choose the right rejection region for your test?

First: what should be its *size*?

That is your choice of the *significance level* $\alpha$ (also: false
positive rate). It is defined as the probability of the test statistic
falling into this region *even if the null hypothesis is true*. Typical
choices for $\alpha$ are 0.05, sometimes more stringent 0.01. (But this
is empirical, there is nothing special about 0.05).

Second: what should be the *shape* of the rejection region?

We distinguish between *two-sided* and *one-sided* tests. In the coin
tossing example we used a two-sided test: rejection region is split
between the two tails of the distribution. (we anticipate that unfair
coins could have a bias either towards head or toward tail; we don't
know.)

If we know that bias would be eg towards head: instead concentrate
rejection region on right tail, carrying out a one-sided test.

The shape is typically chosen for the test to have *high power*, or true
positive rate.

## Types of error

Having understood the basic steps of testing, we can now assess a test's
error rate.

Compare *reality* (whether or not the null hypothesis is in fact true)
with decision whether or not to reject null hypothesis given the data.

<img src="files/error.png" alt="overview" width="500"/>

It is helpful to think here about a clinical diagnostic test, and the
meaning of diagnosing a patient as HIV positive or HIV negative (in
comparison to the actual HIV status of the patient).

Type I error: calling a patient sick, although s/he is healthy (=
rejecting the null although true)

Type II error: calling a patient healthy, although s/he is sick (= not
rejecting the null, although false)

Always possible to reduce one error type at the cost of increasing the
other one.

The trade-off between type I and II errors:

<img src="files/typesoferror.png" alt="overview" width="400"/>

Left: Null distribution. Right: Alternative Distribution. Black line:
decision threshold, null is rejected if statistic falls to the right.

Dark red area: false positive rate (FPR). Dark blue area: false negative
rate (FNR)

Shifting threshold to the right decreases FPR (more *conservative*), but
increases FNR. Shifting threshold to left decreases FNR, at the price of
higher FPR (more *liberal*).

*Specificity* of a test: 1 - FPR (*true negative rate*, correctly
identify those without the disease)

*Sensitivity* of a test: 1 - FNR (also: *power* or *true positive rate*,
correctly identify those with the disease).

## The $t$-test

Many experimental measurements are reported as decimal numbers, which
often, at least approximately follow a normal distribution.

An important comparison: are experimental measurements similar between
two sample groups? (eg cells treated with substance compared to
untreated cells).

Basic test for such situations: the $t$-test.

The null hypothesis: means of sample groups are equal, i.e. $m_1 = m_2$.

The alternative hypothesis: means of sample groups are not equal, i.e.
$m_1 \ne m_2$.

The test statistic: $t = c \frac{m_1 − m_2}{s}$.

$m_1$ and $m_2$ are the mean of the values in the two groups. $s$ is the
standard deviation. $c$ is a constant that depends on the sample sizes
(number of observations in the two groups).

Let's try this out with the `PlantGrowth` dataset from `R`'s `datasets`
package.

```{r}
library("ggbeeswarm")
data("PlantGrowth")
head(PlantGrowth)

ggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +
  geom_beeswarm() + theme(legend.position = "none")

tt <- with(PlantGrowth,
          t.test(weight[group =="ctrl"],
                 weight[group =="trt2"],
                 var.equal = TRUE))
tt
```

**Question**: What do you get from the comparison with `trt1`? What for
`trt1` versus `trt2`?

**Question**: What does `var.equal = TRUE` in the above call to `t.test`
stands for?

**Task**: Rewrite the above call to `t.test` using the formula
interface, by using the notation `weight ∼ group`.

The $t$-test comes in multiple flavors, can be chosen through parameters
of `t.test` function. We did: a *two-sided two-sample unpaired test with
equal variance*.

-   *Two-sided*: mean in one group (treated) could be either larger or
    smaller than in the other group (untreated).

-   *Two-sample*: compares means of two groups; *one-sample*: compare
    the mean of one group against a given, fixed number.

-   *Unpaired*: no 1:1 mapping between measurements in the two groups.
    *paired*: eg. same plants before and after treatment.

-   *Equal variance* refers to the way the statistic is computed,
    assumes variances within each group are about the same. If they are
    very different, an alternative (Welch's $t$-test) should be used
    (invoke with `var.equal = FALSE`, the default).

**Task**: Test whether the mean in the `trt2` group is significantly
larger than 5.

**Task**: Test `trt1` against `trt2` with Welch's $t$-test. Compare
against the result assuming equal variance in both groups.

**The independence assumption**

Now let's try something peculiar: duplicate the data.

```{r}
with(rbind(PlantGrowth, PlantGrowth),
       t.test(weight[group == "ctrl"],
              weight[group == "trt2"],
              var.equal = TRUE))
```

Note that estimates of the group means (and thus the difference) are
unchanged, but the $p$-value is now much smaller!

Conclusions:

-   the power of the $t$-test depends on the sample size. Even if
    differences between groups are the same, a dataset with more
    observations tends to give more significant results

-   assumption of independence is important. Data duplication is an
    extreme form of dependence, but to some extent the same thing
    happens holds for replicated measurements. For instance: data from 8
    plants, treatment measured twice on each plant (technical
    replicates) - pretending that these are now 16 independent
    measurements is wrong.

**Things to remember about the** $t$-test:

-   the $t$-test for **equality of means** (consider outliers)

-   assumes the data to **roughly follow a normal distribution**
    (consider Wilcoxon's rank sum test, `wilcox.test`)

## Homework: permutation testing

To compute the $p$-value, the `t.test` function uses the $t$-statistic.

Under the null hypothesis (equal means in both groups), the statistic
follows a known, mathematical distribution, the so-called
$t$-distribution.

Additional assumptions: independence, normal distribution, equal
variance.

We could be worried about these assumptions.

For the plant growth data: weights always positive (normal distribution
also negative). The question is whether this deviation from the
theoretical assumption makes a real difference.

We can use a **permutation test** to figure this out.

```{r}
pg.sub <- subset(PlantGrowth, group %in% c("ctrl", "trt2"))
sampleT <- function() abs(t.test(weight ~ sample(group), pg.sub)$statistic)

abs_t_null <- replicate(10000, sampleT())

ggplot(data.frame(abs.t = abs_t_null), aes(x = abs.t)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  geom_vline(xintercept = abs(tt$statistic), col = "red")

mean(abs(tt$statistic) <= abs_t_null)

# compare against t-test
tt$p.value
```

Considerations:

-   $t$-test and permutation test with $t$-statistic: two different
    tests

-   $t$-test: null distribution = $t$-distribution (distribution of
    means of normally distributed data)

-   permutation: empirical distribution of test statistic by permuting
    sample labels (employs the $t$-statistic, but not $t$-distribution
    (nor the normal distribution)

**Question**: Why did we use the absolute value function (`abs`) in the
above code?

**Task**: Consider the `DNase` dataset that we used in Session 2 (R
graphics) of the first course (load with `data(DNase)`). Carry out a
permutation test to test for differences in mean density between the
first two runs of the ELISA assay of DNase.
