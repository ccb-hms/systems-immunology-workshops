{
  "hash": "91dea03eef81a603e61889d9e965e4fb",
  "result": {
    "markdown": "# Multiple Hypothesis Correction\n\nThere are a number of methods for transforming p values to correct for multiple hypotheses. These methods can vary greatly in how conservative they are. Most methods are test agnostic, and are performed seperatley after the hypothesis test is performed.\n\nIt is important to keep in mind that the transformed thresholds or p values (often called q values) resulting from a multiple hypothesis correction are **no longer p values**. They are now useful for choosing whether or not to reject the null hypothesis, but cannot be directly interpreted as the probability of seeing a result this extreme under the null hypothesis. Another important note is that the methods we will see here **assume that all hypotheses are independent**. \n\n## Definitions\n\nLet's redefine our error table from earlier, in the framework of multiple hypotheses. Thus, each of the following variables represents a count out of the total number of tests performed.\n\n| Test vs reality | Null is true | Null is false | Total   |\n|-----------------|--------------|---------------|---------|\n| Rejected        | $V$          | $S$           | $R$     |\n| Not Rejected    | $U$          | $T$           | $m - R$ |\n| Total           | $m_0$        | $m-m_0$       | $m$     |\n\n-   $m$: total number of tests (and null hypotheses)\n\n-   $m_0$: number of true null hypotheses\n\n-   $m - m_0$: number of false null hypotheses\n\n-   $V$: number of false positives (a measure of type I error)\n\n-   $T$: number of false negatives (a measure of type II error)\n\n-   $S, U$: number of true positives and true negatives\n\n-   $R$: number of rejections\n\n## Family wise error rate\n\nThe **family wise error rate** (FWER) is the probability that \\$V\\>0\\$, i.e., that we make one or more false positive errors.\n\nWe can compute it as the complement of making no false positive errors at all. Recall that $\\alpha$ is our probability threshold for rejecting the null hypothesis.\n\n$$\nP(V>0) = 1 - P(V=0) = 1 - (1-\\alpha)^{m_0}\n$$\n\nNote that, as $m_0$ approaches $\\infty$, the FWER approaches 1. In other words, with enough tests we are guaranteed to have at least 1 false positive.\n\n## Bonferroni method\n\nThe Bonferroni method uses the FWER to adjust $\\alpha$ such that we can choose a false positive rate across all tests. In other words, to control the FWER to the level $\\alpha_{FWER}$ a new threshold is chosen, $\\alpha = \\alpha_{FWER}/m$.\n\nThis means that, for $10000$ tests, to set $alpha_{FWER} = 0.05$ our new p value threshold for individual tests would be $5 \\times 10{-6}$.\nOften FWER control is too conservative, and would lead to an ineffective use of the time and money that was spent to generate and assemble the data.\n\n## False discovery rate\n\nThe false discovery rate takes a more relaxed approach than Bonferroni correction. Instead of trying to have no or a fixed total rate of false positives, what if we allowed a small proportion of our null hypothesis rejections to be false positives?\n\nIt uses the total number of null hypotheses rejected to inform what is an acceptable number of false positive errors to let through. \nIt makes the claim that, for instance, making $4$ type I errors out of $10$ rejected null hypotheses is a worse error than making $20$ type I errors out of $100$ rejected null hypotheses. \n\nTo see an example, we will load up the RNA-Seq dataset airway, which contains gene expression measurements (gene-level counts) of four primary human airway smooth muscle cell lines with and without treatment with dexamethasone, a synthetic glucocorticoid. \n\nConceptually, the tested null hypothesis is similar to that of the t-test, although the details are slightly more involved since we are dealing with count data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\nlibrary(\"tidyverse\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\n# This next lines filters out NA p values from the dataset\nawde = as.data.frame(results(aw)) |> dplyr::filter(!is.na(pvalue))\n```\n:::\n\n\nIn this dataset, we have performed a statistical test for each of $33,469$ measured genes. We can look at a histogram of the p values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)\n```\n\n::: {.cell-output-display}\n![](mult_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nLet's say we reject the null hypothesis for all p values less than $\\alpha$.\nWe can see how many null hypotheses we reject:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.025\n\n# Recall that TRUE and FALSE are stored as 0 and 1, so we can sum to get a count\nsum(awde$pvalue <= alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4772\n```\n:::\n:::\n\n\nAnd we can estimate $V$, how many false positives we have:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha * nrow(awde)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 836.725\n```\n:::\n:::\n\n\nWe can then estimate the fraction of false rejections as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(alpha * nrow(awde))/sum(awde$pvalue <= alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1753405\n```\n:::\n:::\n\n\nFormally, the **false discovery rate** (FDR) is defined as:\n$$\nFDR = E\\left[\\frac{V}{max(R,1)}\\right]\n$$\nWhich is the average proportion of rejections that are false rejections. \n\n## The Benjamini-Hochberg algorithm for controlling the FDR\n\n\nThe Benjamini-Hochberg algorithm controls for a chosen FDR threshold via the following steps:\n\n- First, order the p values in increasing order, $p_{(1)}...p_{(m)}$\n- Then for some choice of the target FDR, $\\varphi$, find the largest value of $k$ that satisfies $p_{(k)} < \\varphi k/m$\n- Reject hypotheses $1$ through $k$\n\nWe can see how this procedure works when applied to our RNA-Seq p value distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](mult_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nWe find the rightmost point where our p-values and the expected null false discoveries intersect, then reject all tests to the left. \n\n## Multiple Hypothesis Correction in R\n\nWe can use Bonferroni correction or the Benjamini-Hochberg algorithm using the function `p.adjust`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np.adjust(awde$pvalue, method=\"bonferroni\")\np.adjust(awde$pvalue, method=\"BH\")\n```\n:::\n",
    "supporting": [
      "mult_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}