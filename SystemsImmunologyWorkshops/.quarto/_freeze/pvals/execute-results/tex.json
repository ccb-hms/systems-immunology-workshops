{
  "hash": "9a7456e1836b4cb4c0ac9d643c43a210",
  "result": {
    "markdown": "# P Values and Multiple Hypotheses\n\n## Interpreting p values\n\nLet's start by checking our understanding of a p value.\n\n::: {.callout-caution appearance=\"simple\" icon=\"false\"}\nAre these statements correct or incorrect interpretations of p values?\n\n1.  We can use the quantity $1-p$ to represent the probability that the alternative hypothesis is true.\n\n2.  A p value can let us know how incompatible an observation is with a specified statistical model.\n\n3.  A p value tells us how likely we would be to randomly see the observed value with minimal assumptions.\n\n4.  A p value indicates an important result.\n:::\n\n## P-value hacking\n\nLet’s go back to the coin tossing example. We did not reject the null hypothesis (that the coin is fair) at a level of 5%—even though we “knew” that it is unfair. After all, probHead was chosen as 0.6. Let’s suppose we now start looking at different test statistics. \nPerhaps the number of consecutive series of 3 or more heads. Or the number of heads in the first 50 coin flips. \nAnd so on. A\nt some point we will find a test that happens to result in a small p-value, even if just by chance (after all, the probability for the p-value to be less than 0.05 under the null hypothesis—fair coin—is one in twenty).\n\nThere is a [xkcd comic](http://xkcd.com/882) which illustrates this issue in the context of selective reporting. We just did what is called p-value hacking. You see what the problem is: in our zeal to prove our point we tortured the data until some statistic did what we wanted. A related tactic is hypothesis switching or HARKing – hypothesizing after the results are known: we have a dataset, maybe we have invested a lot of time and money into assembling it, so we need results. We come up with lots of different null hypotheses and test statistics, test them, and iterate, until we can report something.\n\n\n::: {.cell}\n\n:::\n\n\nLet's try running our binomial test on a fair coin, and see what we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumFlips = 100\nprobHead = 0.5\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nnumHeads <- sum(coinFlips == \"H\")\npval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\npval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4841184\n```\n:::\n:::\n\nThis p value is probably relatively large. \nBut what if we keep on repeating the experiment?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Let's make a function for performing our experiment\nflip_coin <- function(numFlips, probHead){\n  numFlips = 100\n  probHead = 0.50\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n    replace = TRUE, prob = c(probHead, 1 - probHead))\n  numHeads <- sum(coinFlips == \"H\")\n  pval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\n  return(pval)\n}\n\n#And then run it 10000 times\nparray <- replicate(10000, flip_coin(1000, 0.5), simplify=TRUE)\nhist(parray, breaks=100)\n```\n\n::: {.cell-output-display}\n![](pvals_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmin(parray)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0001831432\n```\n:::\n:::\n\n\n## The Multiple Testing Problem\n\nIn modern biology, we are often conducting hundreds or thousands of statistical tests on high-throughput data. \nThis means that even a low false positive rate can cause there to be a large number of cases where we falsely reject the null hypothesis. \nLuckily, there are ways we can correct our rejection threshold or p values to limit the type I error. \n",
    "supporting": [
      "pvals_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}