{
  "hash": "6ad014b1def4cf0341ea7831dd403f0c",
  "result": {
    "markdown": "# Distributions to Hypothesis Tests\n\n## Calculating the chance of an event\n\nWhen testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes.\n\n*Epitope: A specific portion of a macromolecular antigen to which an antibody binds. In the case of a protein antigen recognized by a T-cell, the epitope or determinant is the peptide portion or site that binds to a Major Histocompatibility Complex (MHC) molecule for recognition by the T cell receptor (TCR).*\n\n[Enzyme-Linked ImmunoSorbent Assays](https://www.ncbi.nlm.nih.gov/books/NBK555922/) (ELISA) are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:\n\n-   The baseline noise level per position, or more precisely the **false positive rate**, is 1%. This is the probability of declaring a hit -- we think we have an epitope -- when there is none. We write this $P(declare epitope|no epitope)$\n-   The protein is tested at 100 different positions, supposed to be independent.\n-   We are going to examine a collection of 50 patient samples.\n\nThe data for one patient's assay look like this:\n\n    [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nwhere the 1 signifies a hit (and thus the potential for an allergic reaction), and the zeros signify no reaction at that position.\n\nWe're going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed $(0,1)$ variables to have a Poisson distribution with parameter $0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/e100.RData\")\nbarplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n  names.arg = seq(along = e100), col = \"darkolivegreen\")\n```\n\n::: {.cell-output-display}\n![](hypo_tests_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe spike is striking. What are the chances of seeing a value as large as 7, if no epitope is present? If we look for the probability of seeing a number as big as 7 (or larger) when considering one $Poisson(0.5)$ random variable, the answer can be calculated in closed form as\n\n$$\nP(X \\ge 7) = \\sum\\limits_{k=7}^{\\infty}P(X=k)\n$$ This is, of course, the same as $1-P(X \\le 6)$. The probability is the so-called **cumulative distribution** function at 6, and R has the function `ppois` for computing it, which we can use in either of the following two ways:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - ppois(6, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.00238e-06\n```\n:::\n\n```{.r .cell-code}\nppois(6, 0.5, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.00238e-06\n```\n:::\n:::\n\n\nYou can use the command `?ppois` to see the argument definitions for the function.\n\nWe denote this number, our chance of seeing such an extreme result, as $\\epsilon$. However, in this case it would be the incorrect calculation.\n\nInstead of asking what the chances are of seeing a `Poisson(0.5)` as large as 7, we need to instead ask, what are the chances that the *maximum of 100 `Poisson(0.5)` trials is as large as 7*? We order the data values $x_1, x_2, ... , x_{100}$ and rename them $x_{(1)}, x_{(2)}, ... , x_{(100)}$, so that denotes $x_{(1)}$ the smallest and $x_{(100)}$ the largest of the counts over the 100 positions. Together, are called the **rank statistic** of this sample of 100 values.\n\nThe maximum value being as large as 7 is the **complementary event** of having all 100 counts be smaller than or equal to 6. Two complementary events have probabilities that sum to 1. *Because the positions are supposed to be independent*, we can now do the computation:\n\n$$\nP(x_{(100)} \\ge 7) = \\prod\\limits_{i=1}^{100}P(x_i \\le 6) = (P(x_i \\le 6))^{100}\n$$ which, using our notation, is $(1-\\epsilon)^{100}$ and is approximately $10^{-4}$. This is a very small chance, so we would determine it is most likely that we did detect real epitopes.\n\n## Computing probabilities with simulations\n\nIn the case we just saw, the theoretical probability calculation was quite simple and we could figure out the result by an explicit calculation. In practice, things tend to be more complicated, and we are better to compute our probabilities using the **Monte Carlo** method: a computer simulation based on our generative model that finds the probabilities of the events we're interested in. Below, we generate 100,000 instances of picking the maximum from 100 Poisson distributed numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmaxes\n    1     2     3     4     5     6     7     8 \n    5 23433 60582 14322  1524   119    12     3 \n```\n:::\n:::\n\n\nSo we can approximate the probability of seeing a $7$ as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean( maxes >= 7 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00015\n```\n:::\n:::\n\n\nWe arrive at a similarly small number, and in both cases would determine that there are real epitopes in the dataset.\n\n## An example: coin tossing\n\nLet's look a simpler example: flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like `HHTTHTHTT...`\n\nLet's simulate the experiment in R, using a biased coin:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nhead(coinFlips)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n```\n:::\n:::\n\n\nNow, if the coin were fair, we would expect half of the time to get heads. Let's see.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(coinFlips)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncoinFlips\n H  T \n59 41 \n```\n:::\n:::\n\n\nThat is different from 50/50. However, does the data deviates strong\nenough to conclude that this coin isn't fair?\nWe know that the total number of heads seen in 100 coin tosses for a\nfair coin follows $B(100, 0.5)$, making it a suitable test statistic.\n\nTo decide, let's look at the sampling distribution of our test statistic -- the total number of heads seen in 100 coin tosses -- for a fair coin.\nAs we learned, we can do this with the binomial distribution. Let's plot a fair coin and mark our observation with a blue line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"dplyr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nk <- 0:numFlips\nnumHeads <- sum(coinFlips == \"H\")\np <- dbinom(k, size = numFlips, prob = 0.5)\nbinomDensity <- data.frame(k = k, p = p)\nhead(binomDensity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  k            p\n1 0 7.888609e-31\n2 1 7.888609e-29\n3 2 3.904861e-27\n4 3 1.275588e-25\n5 4 3.093301e-24\n6 5 5.939138e-23\n```\n:::\n\n```{.r .cell-code}\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](hypo_tests_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nHow do we quantify whether the observed value is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased? \n\nWe divide the set of all possible $k(0-100)$ in two complementary subsets, the **rejection region** and the region of no rejection. We want to make the rejection region as large as possible while keeping their total probability, assuming the null hypothesis, below some threshold $\\alpha$(say, 0.05).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nbinomDensity <- binomDensity[order(p),]\nbinomDensity$reject <- cumsum(binomDensity$p) <= alpha\nhead(binomDensity)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      k            p reject\n1     0 7.888609e-31   TRUE\n101 100 7.888609e-31   TRUE\n2     1 7.888609e-29   TRUE\n100  99 7.888609e-29   TRUE\n3     2 3.904861e-27   TRUE\n99   98 3.904861e-27   TRUE\n```\n:::\n\n```{.r .cell-code}\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](hypo_tests_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\nWe sorted the $p$-values from lowest to highest (`order`), and added a\ncolumn `reject` by computing the cumulative sum (`cumsum`) of the\n$p$-values and thresholding it against `alpha`.\n\nThe logical column `reject` therefore marks with `TRUE` a set of $k$s\nwhose total probability is less than $\\alpha$.\n\nThe rejection region is marked in red, containing both very large and\nvery small values of $k$, which can be considered unlikely under the\nnull hypothesis.\n\n`R` provides not only functions for the densities (e.g., `dbinom`) but\nalso for the cumulative distribution functions (`pbinom`). Those are\nmore precise and faster than `cumsum` over the probabilities.\n\nThe (cumulative) *distribution function* is defined as the probability\nthat a random variable $X$ will take a value less than or equal to $x$.\n\n$$F(x) = P(X \\le x)$$\n\nWe have just gone through the steps of a **binomial test**. This is a\nfrequently used test and therefore available in `R` as a single\nfunction.\n\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tExact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59 \n```\n:::\n:::\n\n\n## Hypothesis Tests\n\nWe can summarize what we just did with a series of steps:\n\n1.  Decide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\n2.  Set up a null hypothesis, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\n3.  Decide on the rejection region, i.e., a subset of possible outcomes whose total probability is small.\n4.  Do the experiment and collect the data; compute the test statistic.\n5.  Make a decision: reject the null hypothesis if the test statistic is in the rejection region.\n\n## Types of Error\n\n![From \"Modern Statistics for Modern Biology\"](img/error_vis.png)\n\nHaving set out the mechanics of testing, we can assess how well we are doing. The following table, called a **confusion matrix**, compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n| Test vs reality        | Null is true                  | Null is false                  |\n|--------------------|--------------------------|--------------------------|\n| **Reject null**        | Type I error (false positive) | True postitive                 |\n| **Do not reject null** | True negative                 | Type II error (false negative) |\n\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade-off between both of them. We can always decrease the **false positive rate** (FPR) by shifting the threshold to the right. We can become more \"conservative\". But this happens at the price of higher **false negative rate** (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. T he FPR is the same as the probability $\\alpha$ that we mentioned above. $1-\\alpha$ is also called the **specificity** of a test. The FNR is sometimes also called $\\beta$, and $1-\\beta$ the **power**, **sensitivity** or **true positive rate** of a test. The power of a test can be understood as the likelihood of it \"catching\" a true positive, or correctly rejecting the null hypothesis.\n\nGenerally, there are three factors that can affect statistical power:\n\n-   Sample size: Larger samples provide greater statistical power\n-   Effect size: A given design will always have greater power to find a large effect than a small effect (because finding large effects is easier)\n-   Type I error rate: There is a relationship between Type I error and power such that (all else being equal) decreasing Type I error will also decrease power.\n\nIn a future session, we will also see how hypothesis tests can be seen as types of **linear models**.\n\n------------------------------------------------------------------------\n\n*The materials in this lesson have been adapted from:* - [*Statistical Thinking for the 21st Century*](https://statsthinking21.github.io/statsthinking21-core-site/index.html) *by Russell A. Poldrack. This work is distributed under the terms of the [Attribution-NonCommercial 4.0 International](https://creativecommons.org/licenses/by-nc/4.0/) (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes.* - [*Modern Statistics for Modern Biology*](https://www.huber.embl.de/msmb/) *by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the [Attribution-NonCommercial-ShareAlike 2.0 Generic](https://creativecommons.org/licenses/by-nc-sa/2.0/) (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material.*\n",
    "supporting": [
      "hypo_tests_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}