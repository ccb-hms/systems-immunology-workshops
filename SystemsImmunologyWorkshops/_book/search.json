[
  {
    "objectID": "session4/session4.html#learning-objectives",
    "href": "session4/session4.html#learning-objectives",
    "title": "Session 4: Data visualization in R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCreate and export histograms, boxplots, line plots, and scatter plots using ggplot2.\nApply basic linear models and ANOVA tests.\nExplore analysis-specific common plots such as volcano plots, heatmaps, clustergrams, networks, and set enrichment visualizations.\nDefine R data structures.\nUse SummarizedExperiment objects."
  },
  {
    "objectID": "session4/session4.html#directions-for-feb.-24",
    "href": "session4/session4.html#directions-for-feb.-24",
    "title": "Session 4: Data visualization in R",
    "section": "Directions for Feb. 24",
    "text": "Directions for Feb. 24\nFor today please:\n\nGo through the final exercise, Adding data from biomaRt , in the Matching and Reordering Data in R lesson from Session 3.\nGo through the session 4 chapters in the order they appear. Complete the ggplot2 exercises.\nIf there is time, look through the Tidyverse lesson from session 3. Code to plot the enrichment results has been added.\nBegin problem set 4."
  },
  {
    "objectID": "session3/tidyverse.html",
    "href": "session3/tidyverse.html",
    "title": "19  Tidyverse",
    "section": "",
    "text": "20 Data Wrangling with Tidyverse\nThe Tidyverse suite of integrated packages are designed to work together to make common data science operations more user friendly. The packages have functions for data wrangling, tidying, reading/writing, parsing, and visualizing, among others. There is a freely available book, R for Data Science, with detailed descriptions and practical examples of the tools available and how they work together. We will explore the basic syntax for working with these packages, as well as, specific functions for data wrangling with the ‘dplyr’ package and data visualization with the ‘ggplot2’ package."
  },
  {
    "objectID": "session3/tidyverse.html#tidyverse-basics",
    "href": "session3/tidyverse.html#tidyverse-basics",
    "title": "19  Tidyverse",
    "section": "20.1 Tidyverse basics",
    "text": "20.1 Tidyverse basics\nThe Tidyverse suite of packages introduces users to a set of data structures, functions and operators to make working with data more intuitive, but is slightly different from the way we do things in base R. Two important new concepts we will focus on are pipes and tibbles.\nBefore we get started with pipes or tibbles, let’s load the library:\n\nlibrary(tidyverse)\n\n\n20.1.1 Pipes\nStringing together commands in R can be quite daunting. Also, trying to understand code that has many nested functions can be confusing.\nTo make R code more human readable, the Tidyverse tools use the pipe, %>%, which was acquired from the magrittr package and is now part of the dplyr package that is installed automatically with Tidyverse. The pipe allows the output of a previous command to be used as input to another command instead of using nested functions.\n\nNOTE: Shortcut to write the pipe is shift + command + M\n\nAn example of using the pipe to run multiple commands:\n\n## A single command\nsqrt(83)\n\n[1] 9.110434\n\n## Base R method of running more than one command\nround(sqrt(83), digits = 2)\n\n[1] 9.11\n\n## Running more than one command with piping\nsqrt(83) %>% round(digits = 2)\n\n[1] 9.11\n\n\nThe pipe represents a much easier way of writing and deciphering R code, and so we will be taking advantage of it, when possible, as we work through the remaining lesson.\n\n\n20.1.2 Tibbles\nA core component of the tidyverse is the tibble. Tibbles are a modern rework of the standard data.frame, with some internal improvements to make code more reliable. They are data frames, but do not follow all of the same rules. For example, tibbles can have numbers/symbols for column names, which is not normally allowed in base R.\nImportant: tidyverse is very opininated about row names. These packages insist that all column data (e.g. data.frame) be treated equally, and that special designation of a column as rownames should be deprecated. Tibble provides simple utility functions to handle rownames: rownames_to_column() and column_to_rownames().\nTibbles can be created directly using the tibble() function or data frames can be converted into tibbles using as_tibble(name_of_df).\n\nNOTE: The function as_tibble() will ignore row names, so if a column representing the row names is needed, then the function rownames_to_column(name_of_df) should be run prior to turning the data.frame into a tibble. Also, as_tibble() will not coerce character vectors to factors by default."
  },
  {
    "objectID": "session3/tidyverse.html#experimental-data",
    "href": "session3/tidyverse.html#experimental-data",
    "title": "19  Tidyverse",
    "section": "20.2 Experimental data",
    "text": "20.2 Experimental data\nWe’re going to explore the Tidyverse suite of tools to wrangle our data to prepare it for visualization. Make sure you have the file called gprofiler_results_Mov10oe.tsv.\nThe dataset:\n\nRepresents the functional analysis results, including the biological processes, functions, pathways, or conditions that are over-represented in a given list of genes.\nOur gene list was generated by differential gene expression analysis and the genes represent differences between control mice and mice over-expressing a gene involved in RNA splicing.\n\nThe functional analysis that we will focus on involves gene ontology (GO) terms, which:\n\ndescribe the roles of genes and gene products\norganized into three controlled vocabularies/ontologies (domains):\n\nbiological processes (BP)\ncellular components (CC)\nmolecular functions (MF)"
  },
  {
    "objectID": "session3/tidyverse.html#analysis-goal-and-workflow",
    "href": "session3/tidyverse.html#analysis-goal-and-workflow",
    "title": "19  Tidyverse",
    "section": "20.3 Analysis goal and workflow",
    "text": "20.3 Analysis goal and workflow\nGoal: Visually compare the most significant biological processes (BP) based on the number of associated differentially expressed genes (gene ratios) and significance values by creating the following plot:\n\n\n\ndotplot6\n\n\nTo wrangle our data in preparation for the plotting, we are going to use the Tidyverse suite of tools to wrangle and visualize our data through several steps:\n\nRead in the functional analysis results\nExtract only the GO biological processes (BP) of interest\nSelect only the columns needed for visualization\nOrder by significance (p-adjusted values)\nRename columns to be more intuitive\nCreate additional metrics for plotting (e.g. gene ratios)\nPlot results"
  },
  {
    "objectID": "session3/tidyverse.html#instructions",
    "href": "session3/tidyverse.html#instructions",
    "title": "19  Tidyverse",
    "section": "20.4 Instructions",
    "text": "20.4 Instructions\nFind a partner (or a group of 3 if needed). Choose one person to go through the following steps using Tidyverse, and the other using base R. It is recommended that the person with more experience attempt the steps in base R."
  },
  {
    "objectID": "session3/tidyverse.html#tidyverse-tools",
    "href": "session3/tidyverse.html#tidyverse-tools",
    "title": "19  Tidyverse",
    "section": "20.5 Tidyverse tools",
    "text": "20.5 Tidyverse tools\nWhile all of the tools in the Tidyverse suite are deserving of being explored in more depth, we are going to investigate more deeply the reading (readr), wrangling (dplyr), and plotting (ggplot2) tools."
  },
  {
    "objectID": "session3/tidyverse.html#read-in-the-functional-analysis-results",
    "href": "session3/tidyverse.html#read-in-the-functional-analysis-results",
    "title": "19  Tidyverse",
    "section": "20.6 1. Read in the functional analysis results",
    "text": "20.6 1. Read in the functional analysis results\n\nTidyverseBase R\n\n\nWhile the base R packages have perfectly fine methods for reading in data, the readr and readxl Tidyverse packages offer additional methods for reading in data. Let’s read in our tab-delimited functional analysis results gprofiler_results_Mov10oe.tsv using read_delim(). Name the dataframe functional_GO_results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Read in the functional analysis results\nfunctional_GO_results <- read_delim(file = \"../data/gprofiler_results_Mov10oe.tsv\", delim = \"\\t\" )\n\nRows: 3644 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): term.id, domain, term.name, intersection\ndbl (9): query.number, p.value, term.size, query.size, overlap.size, recall,...\nlgl (1): significant\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Take a look at the results\nhead(functional_GO_results)\n\n# A tibble: 6 × 14\n  query.…¹ signi…² p.value term.…³ query…⁴ overl…⁵ recall preci…⁶ term.id domain\n     <dbl> <lgl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <chr>   <chr> \n1        1 TRUE    0.00434     111    5850      52  0.009   0.468 GO:003… BP    \n2        1 TRUE    0.0033      110    5850      52  0.009   0.473 GO:003… BP    \n3        1 TRUE    0.0297       39    5850      21  0.004   0.538 GO:003… BP    \n4        1 TRUE    0.0193       70    5850      34  0.006   0.486 GO:003… BP    \n5        1 TRUE    0.0148       26    5850      16  0.003   0.615 GO:001… BP    \n6        1 TRUE    0.0187       22    5850      14  0.002   0.636 GO:008… BP    \n# … with 4 more variables: subgraph.number <dbl>, term.name <chr>,\n#   relative.depth <dbl>, intersection <chr>, and abbreviated variable names\n#   ¹​query.number, ²​significant, ³​term.size, ⁴​query.size, ⁵​overlap.size,\n#   ⁶​precision\n\n\n\n\n\n\n\nUse one of the base R read.X functions to read in the tab delimited file gprofiler_results_Mov10oe.tsv. Name the dataframe functional_GO_results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Read in the functional analysis results\nfunctional_GO_results <- read.delim(file = \"../data/gprofiler_results_Mov10oe.tsv\", sep = \"\\t\" )\n\n# Take a look at the results\nhead(functional_GO_results)\n\n\n\n\n\n\n\nDouble check the data types and format of your dataframe. Do the methods yield the same result? Convert anything you think should be a factor into a factor.\nNOTE: A large number of tidyverse functions will work with both tibbles and dataframes, and the data structure of the output will be identical to the input. However, there are some functions that will return a tibble (without row names), whether or not a tibble or dataframe is provided."
  },
  {
    "objectID": "session3/tidyverse.html#extract-only-the-go-biological-processes-bp-of-interest",
    "href": "session3/tidyverse.html#extract-only-the-go-biological-processes-bp-of-interest",
    "title": "19  Tidyverse",
    "section": "20.7 2. Extract only the GO biological processes (BP) of interest",
    "text": "20.7 2. Extract only the GO biological processes (BP) of interest\nNow that we have our data, we will need to wrangle it into a format ready for plotting. To extract the biological processes of interest, we only want those rows where the domain is equal to BP.\n\nTidyverseBase R\n\n\nFor all of our data wrangling steps we will be using tools from the dplyr package, which is a swiss-army knife for data wrangling of data frames.\nTo extract the biological processes of interest, we only want those rows where the domain is equal to BP, which we can do using the filter() function.\nTo filter rows of a data frame/tibble based on values in different columns, we give a logical expression as input to the filter() function to return those rows for which the expression is TRUE.\nPerform an additional filtering step to only keep those rows where the relative.depth is greater than 4.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Return only GO biological processes\nbp_oe <- functional_GO_results %>%\n  filter(domain == \"BP\")       %>% \n  filter(relative.depth > 4)\n\n\n\n\n\n\nUse a conditional expression and indexing ([]) to extract the rows where the domain is equal to BP.\nPerform an additional indexing step to only keep those rows where the relative.depth is greater than 4.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Return only GO biological processes\nidx <- functional_GO_results$domain == \"BP\"\nbp_oe2 <- functional_GO_results[idx,]\nbp_oe <- subset(bp_oe, relative.depth > 4)\n\n\n\n\n\n\n\nNow we have returned only those rows with a domain of BP. How have the dimensions of our results changed?"
  },
  {
    "objectID": "session3/tidyverse.html#select-only-the-columns-needed-for-visualization",
    "href": "session3/tidyverse.html#select-only-the-columns-needed-for-visualization",
    "title": "19  Tidyverse",
    "section": "20.8 3. Select only the columns needed for visualization",
    "text": "20.8 3. Select only the columns needed for visualization\nFor visualization purposes, we are only interested in the columns related to the GO terms, the significance of the terms, and information about the number of genes associated with the terms.\n\nTidyverseBase R\n\n\nTo extract columns from a data frame/tibble we can use the select() function. In contrast to base R, we do not need to put the column names in quotes for selection.\nSelect the columns term.id, term.name, p.value, query.size, term.size, overlap.size, intersection.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Selecting columns to keep\nbp_oe <- bp_oe %>%\n  select(term.id, term.name, p.value, query.size, term.size, overlap.size, intersection)\n\n\n\n\n\n\nIndex the columnsterm.id, term.name, p.value, query.size, term.size, overlap.size, intersection.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbp_oe <- bp_oe[, c(\"term.id\", \"term.name\", \"p.value\", \"query.size\", \"term.size\", \"overlap.size\", \"intersection\")]\n\n\n\n\n\n\n\nBoth indexing and the select() function also allows for negative selection. However, select allows for negative selection using column names, while in base R we can only do so with indexes. Note that we need to put the column names inside of the combine (c()) function with a - preceding it for this functionality.\nTo use column names in base R, we have to use %in%:\n# Selecting columns to keep\nidx <- !(colnames(functional_GO_results) %in% c(\"query.number\", \"significant\", \"recall\", \"precision\", \"subgraph.number\", \"relative.depth\", \"domain\"))"
  },
  {
    "objectID": "session3/tidyverse.html#order-go-processes-by-significance-adjusted-p-values",
    "href": "session3/tidyverse.html#order-go-processes-by-significance-adjusted-p-values",
    "title": "19  Tidyverse",
    "section": "20.9 4. Order GO processes by significance (adjusted p-values)",
    "text": "20.9 4. Order GO processes by significance (adjusted p-values)\nNow that we have only the rows and columns of interest, let’s arrange these by significance, which is denoted by the adjusted p-value.\n\nTidyverseBase R\n\n\nSort the rows by adjusted p-value with the arrange() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Order by adjusted p-value ascending\nbp_oe <- bp_oe %>%\n  arrange(p.value)\n\n\n\n\n\n\nSort the rows by adjusted p-value with the order() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Order by adjusted p-value ascending\nidx <- order(bp_oe$p.value)\nbp_oe <- bp_oe[idx,]\n\n\n\n\n\n\n\nNOTE: If you wanted to arrange in descending order, then you could have run the following instead:\n# Order by adjusted p-value descending\nfunctional_GO_results <- functional_GO_results %>%\narrange(desc(p.value))\nNOTE: Ordering variables in ggplot2 is a bit different. This post introduces a few ways of ordering variables in a plot."
  },
  {
    "objectID": "session3/tidyverse.html#rename-columns-to-be-more-intuitive",
    "href": "session3/tidyverse.html#rename-columns-to-be-more-intuitive",
    "title": "19  Tidyverse",
    "section": "20.10 5. Rename columns to be more intuitive",
    "text": "20.10 5. Rename columns to be more intuitive\nWhile not necessary for our visualization, renaming columns more intuitively can help with our understanding of the data. Let’s rename the term.id and term.name columns.\n\nTidyverseBase R\n\n\nRename term.id and term.name to GO_id and GO_term using the rename function. Note that you may need to call rename as dplyr::rename, since rename is a common function name in other packages.\nThe syntax is new_name = old_name.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Provide better names for columns\nbp_oe <- bp_oe %>% \n  dplyr::rename(GO_id = term.id, \n                GO_term = term.name)\n\n\n\n\n\n\nRename term.id and term.name to GO_id and GO_term using colnames and indexing.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Provide better names for columns\ncolnames(bp_oe)[colnames(bp_oe) == \"term.id\"] <- \"GO_id\"\ncolnames(bp_oe)[colnames(bp_oe) == \"term.name\"] <- \"GO_term\""
  },
  {
    "objectID": "session3/tidyverse.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "href": "session3/tidyverse.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "title": "19  Tidyverse",
    "section": "20.11 6. Create additional metrics for plotting (e.g. gene ratios)",
    "text": "20.11 6. Create additional metrics for plotting (e.g. gene ratios)\nFinally, before we plot our data, we need to create a couple of additional metrics. Let’s generate gene ratios to reflect the number of DE genes associated with each GO process relative to the total number of DE genes.\nThis is calculated as gene_ratio = overlap.size / query.size.\n\nTidyverseBase R\n\n\nThe mutate() function enables you to create a new column from an existing column.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbp_oe <- bp_oe %>%\n  mutate(gene_ratio = overlap.size / query.size)\n\n\n\n\n\n\nCreate a new column in the dataframe using the $ syntax or cbind.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Create gene ratio column based on other columns in dataset\nbp_oe <- cbind(bp_oe, gene_ratio = bp_oe$overlap.size / bp_oe$query.size)\n\n\n\n\n\n\n\nThe mutate() function enables you to create a new column from an existing column."
  },
  {
    "objectID": "session3/tidyverse.html#compare-code",
    "href": "session3/tidyverse.html#compare-code",
    "title": "19  Tidyverse",
    "section": "20.12 Compare code",
    "text": "20.12 Compare code\nTake a look at your code verses your partner’s code. Which method do you think results in cleaner, more readable code? Which steps were easier in base R, and which in Tidyverse?"
  },
  {
    "objectID": "session3/tidyverse.html#making-the-plot",
    "href": "session3/tidyverse.html#making-the-plot",
    "title": "19  Tidyverse",
    "section": "20.13 Making the Plot",
    "text": "20.13 Making the Plot\nLet’s start by making a scatterplot of the top 30 terms:\n\nbp_plot <- bp_oe[1:30, ]\nggplot(bp_plot) +\n  geom_point(aes(x = overlap.size, y = p.value))\n\n\n\n\nHowever, instead of a scatterplot with numeric values on both axes, we would like to create a dotplot for visualizing the top 30 functional categories in our dataset, and how prevalent they are. Basically, we want a dotplot for visualizing functional analysis data, which plots the gene ratio values on the x-axis and the GO terms on the y-axis.\nLet’s see what happens when we add a non-numeric value to the y-axis and change the x-axis to the “gene_ratio” column:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term))\n\n\n\n\nNow that we have the required aesthetics, let’s add some extras like color to the plot. Let’s say we wanted to quickly visualize significance of the GO terms in the plot, we can color the points on the plot based on p-values, by specifying the column header.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = p.value))\n\n\n\n\nYou will notice that there are a default set of colors that will be used so we do not have to specify which colors to use. Also, the legend has been conveniently plotted for us!\nAlternatively, we could color number of DE genes associated with each term (overlap.size).\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = overlap.size))\n\n\n\n\nMoving forward, we are going to stick with coloring the dots based on the p.value column. Let’s explore some of the other arguments that can be specified in the geom layer.\nTo modify the size of the data points we can use the size argument. * If we add size inside aes() we could assign a numeric column to it and the size of the data points would change according to that column. * However, if we add size inside the geom_point() but outside aes() we can’t assign a column to it, instead we have to give it a numeric value. This use of size will uniformly change the size of all the data points.\n\nNote: This is true for several arguments, including color, shape etc. E.g. we can change all shapes to square by adding this argument to be outside the aes() function; if we put the argument inside the aes() function we could change the shape according to a (categorical) variable in our data frame or tibble.\n\nWe have decided that we want to change the size of all the data point to a uniform size instead of typing it to a numeric column in the input tibble. Add in the size argument by specifying a number for the size of the data point:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2)\n\n\n\n\n\nNote: The size of the points is personal preference, and you may need to play around with the parameter to decide which size is best. That seems a bit too small, so we can try out a slightly larger size.\n\nAs we do that, let’s see how we can change the shape of the data point. Different shapes are available, as detailed in the RStudio ggplot2 cheatsheet. Let’s explore this parameter by changing all of the points to squares:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2, \n             shape = \"square\")\n\n\n\n\nNow we can start updating the plot to suit our preferences for how we want the data displayed. The labels on the x- and y-axis are also quite small and not very descriptive. To change their size and labeling, we need to add additional theme layers. The ggplot2 theme() system handles modification of non-data plot elements such as:\n\nAxis label aesthetics\nPlot background\nFacet label backround\nLegend appearance\n\nThere are built-in themes that we can use (i.e. theme_bw()) that mostly change the background/foreground colours, by adding it as additional layer. Alternatively, we can adjust specific elements of the current default theme by adding a theme() layer and passing in arguments for the things we wish to change. Or we can use both, a built-in theme layer and a custom theme layer!\nLet’s add a built-in theme layer theme_bw() first.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2) +\n  theme_bw()\n\n\n\n\nDo the axis labels or the tick labels get any larger by changing themes?\nNot in this case. But we can add arguments using theme() to change it ourselves. Since we are adding this layer on top (i.e later in sequence), any features we change will override what is set in the theme_bw(). Here we’ll increase the size of the axes labels to be 1.15 times the default size and the x-axis tick labels to be 1.15 times the default.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = p.value), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15)))\n\n\n\n\n\nNote #1: When modifying the size of text we often use the rel() function to specify the size we want relative to the default. We can also provide a numeric value as we did with the data point size, but it can be cumbersome if you don’t know what the default font size is to begin with.\nNote #2: You can use the example(\"geom_point\") function here to explore a multitude of different aesthetics and layers that can be added to your plot. As you scroll through the different plots, take note of how the code is modified. You can use this with any of the different geom layers available in ggplot2 to learn how you can easily modify your plots!\nNote #3: RStudio provides this very useful cheatsheet for plotting using ggplot2. Different example plots are provided and the associated code (i.e which geom or theme to use in the appropriate situation.)\n\n\n20.13.1 Customizing data point colors\nThe plot is looking better, but it is hard to distinguish differences in significance based on the colors used. There are cheatsheets available for specifying the base R colors by name or hexadecimal code. We could specify other colors available or use pre-created color palettes from an external R package.\nTo make additional color palettes available for plotting, we can load the RColorBrewer library, which contains color palettes designed specifically for the different types of data being compared.\n\n# Load the RColorBrewer library\nlibrary(RColorBrewer)\n\n# Check the available color palettes\ndisplay.brewer.all()\n\n\n\n\nThe output is separated into three sections based on the suggested palettes for sequential, qualitative, and diverging data.\n\nSequential palettes (top): For sequential data, with lighter colors for low values and darker colors for high values.\nQualitative palettes (middle): For categorical data, where the color does not denote differences in magnitude or value.\nDiverging palettes (bottom): For data with emphasis on mid-range values and extremes.\n\nSince our adjusted p-values are sequential, we will choose from these palettes. Let’s go with the “Yellow, orange, red” palette. We can choose how many colors from the palette to include, which may take some trial and error. We can test the colors included in a palette by using the display.brewer.pal() function, and changing if desired:\n\n# Testing the palette with six colors\ndisplay.brewer.pal(6, \"YlOrRd\")\n\n\n\n\nThe yellow might be a bit too light, and we might not need so many different colors. Let’s test with three different colors:\n\n# Testing the palette with three colors\ndisplay.brewer.pal(3, \"YlOrRd\")\n\n\n\n# Define a palette\nmypalette <- brewer.pal(3, \"YlOrRd\")\n\n# how are the colors represented in the mypalette vector?\nmypalette\n\n[1] \"#FFEDA0\" \"#FEB24C\" \"#F03B20\"\n\n\nThose colors look okay, so let’s test them in our plot. We can add a color scale layer, and most often one of the following two scales will work:\n\nscale_color_manual(): for categorical data or quantiles\nscale_color_gradient() family: for continuous data.\n\nBy default, scale_color_gradient() creates a two color gradient from low to high. Since we plan to use more colors, we will use the more flexible scale_color_gradientn() function. To make the legend a bit cleaner, we will also perform a -log10 transform on the p-values (higher values means more significant).\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = -log10(p.value)), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15))) +\n  xlab(\"Gene ratios\") +\n  ylab(\"Top 30 significant GO terms\") +\n  ggtitle(\"Dotplot of top 30 significant GO terms\") +\n  theme(plot.title = element_text(hjust=0.5, \n    face = \"bold\")) +\n  scale_color_gradientn(colors = mypalette)\n\n\n\n\nThis looks good, but we want to add better name for the legend and we want to make sure the legend title is centered and bold. To do this, we can add a name argument to scale_color_gradientn() and a new theme layer for the legend title.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = -log10(p.value)), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15))) +\n  xlab(\"Gene ratios\") +\n  ylab(\"Top 30 significant GO terms\") +\n  ggtitle(\"Dotplot of top 30 significant GO terms\") +\n  theme(plot.title = element_text(hjust=0.5, \n    face = \"bold\")) +\n  scale_color_gradientn(name = \"Significance \\n (-log10(padj))\", colors = mypalette) +\n  theme(legend.title = element_text(size=rel(1.15),\n    hjust=0.5, \n    face=\"bold\"))"
  },
  {
    "objectID": "session3/pSet3.html#instructions",
    "href": "session3/pSet3.html#instructions",
    "title": "20  Problem Set 3",
    "section": "20.1 Instructions",
    "text": "20.1 Instructions\nIn this problem set, you will be going through an analysis resolve a potential label swap in phosphoproteomic mass spec data.\nIt is recommended to create a Quarto notebook for your report. You can create a new notebook in RSTudio by going to file->new file->quarto document. Set the default output to be a PDF. As an example, the entire workbook is a quarto document. More information can be found here. However, if you are finding it difficult to render your document, feel free to instead submit a script and separate writeup document."
  },
  {
    "objectID": "session3/pSet3.html#data-description",
    "href": "session3/pSet3.html#data-description",
    "title": "20  Problem Set 3",
    "section": "20.2 Data Description",
    "text": "20.2 Data Description\nWe are collaborating with a lab that is studying phosphorylation changes during covid infection. This dataset consists of phosphoproteomic TMT mass spec data from 2 10-plexes. We took samples at 0, 5, and 60 minutes post-infection. We also wanted to explore the specific role of 2 genes thought to be used in covid infection, RAB7A and NPC1. To do this, we included cell lines with each of these genes knocked out.\nWe wanted to have 2 replicates for each condition we were looking at, so we have a total of 3X2X3 or 18 different samples we want to measure. We decide to replicate wild type at 0 minutes in each 10plex for our total 20 wells accross the 2 10-plexes.\nOur collaborator has alerted us that there may have been a label swap in the dataset. We need to see if we can find two samples which seem to have been swapped, and correct the error if we feel confident that we know what swap took place.\nNote: This data has been adapted with permission from an unpublished study. The biological context of the original data has been changed, and all gene names were shuffled."
  },
  {
    "objectID": "session3/pSet3.html#loading-data",
    "href": "session3/pSet3.html#loading-data",
    "title": "20  Problem Set 3",
    "section": "20.3 Loading Data",
    "text": "20.3 Loading Data\nLoad in the data phospho_exp2_safe.csv and phospho_exp2_safe.csv.\nThere are two variables of interest, the time, 0, 5, or 60 minutes post-infection, and the genotype, WT, NPC1 knockout and RAB7A knockout.\nUnfortunately, all of this data is embedded in the column names of the dataset.\nCreate a metadata_plex# dataframes to contain this data instead. You can try to do this programatically from the column names, or you can type out the data manually."
  },
  {
    "objectID": "session3/pSet3.html#pca",
    "href": "session3/pSet3.html#pca",
    "title": "20  Problem Set 3",
    "section": "20.4 PCA",
    "text": "20.4 PCA\nAs an initial quality check, let’s run PCA on our data. We can use prcomp to run pca, and autoplot to plot the result. Let’s try making 2 pca plots, 1 for each 10plex. We can set the color equal to the genotype and the shape of the points equal to the time.\nYou can call prcomp and autoplot like this:\nlibrary(ggfortify)\n#PCA Plots\npca_res2 <- prcomp(plex2_data, scale = FALSE)\nautoplot(pca_res2, data=metadata_plex2, color = 'condition', shape='time', size=3)\nHint: prcomp might be expecting data in a wide format as opposed to a long format, meaning that we need to make each peptide a column and each row a sample. We can use the t() function and convert the result to a dataframe to get our data into this format.\nNote: You may need to set the scale parameter to FALSE to avoid an error in prcomp.\nWe should look at how our replicates are clustered. Does everything look good in both 10-plexes?"
  },
  {
    "objectID": "session3/pSet3.html#heatmaps",
    "href": "session3/pSet3.html#heatmaps",
    "title": "20  Problem Set 3",
    "section": "20.5 Heatmaps",
    "text": "20.5 Heatmaps\nLet’s explore this more by looking at some heatmaps of our data. We can use the heatmap function to plot a heatmap of the correlation between each of the samples in each 10plex.\nBelow is how to calculate the correlation and call the heatmap function. You can try to use the RowSideColors argument or change the column names to improve the visualization.\nheatmap(x=cor(plex2_data))\n*Hint: heatmap only accepts numeric columns.`\nIs there anything unexpected in how the samples have clustered here?"
  },
  {
    "objectID": "session3/pSet3.html#resolving-the-issue",
    "href": "session3/pSet3.html#resolving-the-issue",
    "title": "20  Problem Set 3",
    "section": "20.6 Resolving the issue",
    "text": "20.6 Resolving the issue\nDecide what to do about the potential label swap and explain your reasoning. You could declare there to be too much uncertainty and report to your collaborator that they will have to redo the experiment, decide there is no label swap, or correct a label swap and continue the analysis.\nDo you feel confident enough to continue the analysis, or is there too much uncertainty to use this data? What other factors might influence your decision?\nIf there is additional analysis you want to perform or calculations you want to make to support your answer, feel free to do so. If you are unsure how to perform that analysis or it would be outside the scope of a problem set, instead describe what you would do and how you would use the results."
  },
  {
    "objectID": "session4/linear_models.html#returning-to-count-data",
    "href": "session4/linear_models.html#returning-to-count-data",
    "title": "21  Linear Models",
    "section": "21.1 Returning to count data",
    "text": "21.1 Returning to count data\n\nlibrary(tidyverse)\nlibrary(pasilla)\n\n\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\n\nRows: 7 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): file, condition, type, total number of reads\ndbl (2): number of lanes, exon counts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\n\nLet’s assume that in addition to the siRNA knockdown of the pasilla gene, we also want to test the effect of a certain drug. We could then envisage an experiment in which the experimenter treats the cells either with negative control, with the siRNA against pasilla, with the drug, or with both. To analyse this experiment, we can use the notation:\n\\[\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_2\n\\]\nThis equation can be parsed as follows. The left hand side, \\(y\\) , is the experimental measurement of interest. In our case, this is the suitably transformed expression level of a gene. Since in an RNA-Seq experiment there are lots of genes, we’ll have as many copies of Equation the above equation, one for each. The coefficient \\(\\beta_0\\) is the base level of the measurement in the negative control; often it is called the intercept.\nThe design factors \\(x_1\\) and \\(x_2\\) and are binary indicator variables, sometimes called dummy variables: \\(x_1\\) takes the value 1 if the siRNA was transfected and 0 if not, and similarly, \\(x_2\\) indicates whether the drug was administered. In the experiment where only the siRNA is used, \\(x_1 = 1\\) and \\(x_2 = 0\\), and the third and fourth terms of the equation vanish. Then, the equation simplifies to \\(y = \\beta+0 + \\beta_1\\). This means that \\(\\beta_1\\) represents the difference between treatment and control.\nWe can succinctly encode the design of the experiment in the design matrix. For instance, for the combinatorial experiment described above, the design matrix is\n\n\n\nx_0\nx_1\nx_2\n\n\n\n\n1\n0\n0\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n1\n1\n1\n\n\n\nMany R packges such as limma and edgeR use the design matrix to represent experimental design.\nThe columns of the design matrix correspond to the experimental factors, and its rows represent the different experimental conditions, four in our case since we are including an interaction effect.\nHowever, for the pasilla data we’re not done yet. While the above equation would function if our data was perfect, in reality we have small differences between our replicates and other sources of variation in our data. We need to slightly extend the equation,\n\\[\ny = x_{j0}\\beta_0 + x_{j1}\\beta_1 + x_{j2}\\beta_2 + x_{j1}x_{j2}\\beta_2 + \\epsilon_j\n\\]\nWe have added the index \\(j\\) and a new term \\(\\epsilon_j\\). The index \\(j\\) now explicitly counts over our individual replicate experiments; for instance, if for each of the four conditions we perform three replicates, then \\(j\\) counts from 1 to 12. The design matrix has now 12 rows, and \\(x_{jk}\\) is the value of the matrix in its \\(j\\)th row and \\(k\\)th column. The additional terms \\(\\epsilon_j\\), which we call the residuals, are there to absorb differences between replicates. Under the assumptions of our experimental design, we require the residuals to be small. For instance, we can minimize the sum of the square of all the residuals, which is called least sum of squares fitting. The R function lm performs least squares."
  },
  {
    "objectID": "session4/linear_models.html#defining-linear-models",
    "href": "session4/linear_models.html#defining-linear-models",
    "title": "21  Linear Models",
    "section": "21.2 Defining linear models",
    "text": "21.2 Defining linear models\nThe above is an example of a linear model. A linear model is a model for a continuous outcome Y of the form \\[Y = \\beta_0 + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\dots + \\beta_{p}X_{p} + \\epsilon\\] The covariates X can be:\n\na continuous variable (age, weight, temperature, etc.)\nDummy variables coding a categorical covariate (more later)\n\nThe \\(\\beta\\)’s are unknown parameters to be estimated.\nThe error term \\(\\epsilon\\) is assumed to be normally distributed with a variance that is constant across the range of the data.\nModels with all categorical covariates are referred to as ANOVA models and models with continuous covariates are referred to as linear regression models. These are all linear models, and R doesn’t distinguish between them.\nWe have already seen the t-test, but it can also be viewed as an application of the general linear model. In this case, the model would look like this:\n\\[\n{y} = {\\beta_1}*x_1 + {\\beta_0}\n\\] Many of the statistical tests we have seen can be represented as special cases of linear models."
  },
  {
    "objectID": "session4/linear_models.html#linear-models-in-r",
    "href": "session4/linear_models.html#linear-models-in-r",
    "title": "21  Linear Models",
    "section": "21.3 Linear models in R",
    "text": "21.3 Linear models in R\nR uses the function lm to fit linear models.\nRead in ’lm_example_data.csv`:\n\ndat <- read.csv(\"https://raw.githubusercontent.com/ucdavis-bioinformatics-training/2018-September-Bioinformatics-Prerequisites/master/friday/lm_example_data.csv\")\nhead(dat)\n\n  sample expression  batch treatment  time temperature\n1      1  1.2139625 Batch1         A time1    11.76575\n2      2  1.4796581 Batch1         A time2    12.16330\n3      3  1.0878287 Batch1         A time1    10.54195\n4      4  1.4438585 Batch1         A time2    10.07642\n5      5  0.6371621 Batch1         A time1    12.03721\n6      6  2.1226740 Batch1         B time2    13.49573\n\nstr(dat)\n\n'data.frame':   25 obs. of  6 variables:\n $ sample     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ expression : num  1.214 1.48 1.088 1.444 0.637 ...\n $ batch      : chr  \"Batch1\" \"Batch1\" \"Batch1\" \"Batch1\" ...\n $ treatment  : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ time       : chr  \"time1\" \"time2\" \"time1\" \"time2\" ...\n $ temperature: num  11.8 12.2 10.5 10.1 12 ...\n\n\nFit a linear model using expression as the outcome and treatment as a categorical covariate:\n\noneway.model <- lm(expression ~ treatment, data = dat)\n\nIn R model syntax, the outcome is on the left side, with covariates (separated by +) following the ~\n\noneway.model\n\n\nCall:\nlm(formula = expression ~ treatment, data = dat)\n\nCoefficients:\n(Intercept)   treatmentB   treatmentC   treatmentD   treatmentE  \n     1.1725       0.4455       0.9028       2.5537       7.4140  \n\nclass(oneway.model)\n\n[1] \"lm\"\n\n\nWe can look at the design matrix:\n\nX <- model.matrix(~treatment, data = dat)\nX\n\n   (Intercept) treatmentB treatmentC treatmentD treatmentE\n1            1          0          0          0          0\n2            1          0          0          0          0\n3            1          0          0          0          0\n4            1          0          0          0          0\n5            1          0          0          0          0\n6            1          1          0          0          0\n7            1          1          0          0          0\n8            1          1          0          0          0\n9            1          1          0          0          0\n10           1          1          0          0          0\n11           1          0          1          0          0\n12           1          0          1          0          0\n13           1          0          1          0          0\n14           1          0          1          0          0\n15           1          0          1          0          0\n16           1          0          0          1          0\n17           1          0          0          1          0\n18           1          0          0          1          0\n19           1          0          0          1          0\n20           1          0          0          1          0\n21           1          0          0          0          1\n22           1          0          0          0          1\n23           1          0          0          0          1\n24           1          0          0          0          1\n25           1          0          0          0          1\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$treatment\n[1] \"contr.treatment\"\n\n\nNote that this is a one-way ANOVA model.\nsummary() applied to an lm object will give p-values and other relevant information:\n\nsummary(oneway.model)\n\n\nCall:\nlm(formula = expression ~ treatment, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.5353  0.1790  0.7725  3.6114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.1725     0.7783   1.506    0.148    \ntreatmentB    0.4455     1.1007   0.405    0.690    \ntreatmentC    0.9028     1.1007   0.820    0.422    \ntreatmentD    2.5537     1.1007   2.320    0.031 *  \ntreatmentE    7.4140     1.1007   6.735 1.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 20 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7033 \nF-statistic: 15.22 on 4 and 20 DF,  p-value: 7.275e-06\n\n\nIn the output:\n\n“Coefficients” refer to the \\(\\beta\\)’s\n“Estimate” is the estimate of each coefficient\n“Std. Error” is the standard error of the estimate\n“t value” is the coefficient divided by its standard error\n“Pr(>|t|)” is the p-value for the coefficient\nThe residual standard error is the estimate of the variance of \\(\\epsilon\\)\nDegrees of freedom is the sample size minus # of coefficients estimated\nR-squared is (roughly) the proportion of variance in the outcome explained by the model\nThe F-statistic compares the fit of the model as a whole to the null model (with no covariates)\n\ncoef() gives you model coefficients:\n\ncoef(oneway.model)\n\n(Intercept)  treatmentB  treatmentC  treatmentD  treatmentE \n  1.1724940   0.4455249   0.9027755   2.5536669   7.4139642 \n\n\nWhat do the model coefficients mean?\nBy default, R uses reference group coding or “treatment contrasts”. For categorical covariates, the first level alphabetically (or first factor level) is treated as the reference group. The reference group doesn’t get its own coefficient, it is represented by the intercept. Coefficients for other groups are the difference from the reference:\nFor our simple design:\n\n(Intercept) is the mean of expression for treatment = A\ntreatmentB is the mean of expression for treatment = B minus the mean for treatment = A\ntreatmentC is the mean of expression for treatment = C minus the mean for treatment = A\netc.\n\n\n# Get means in each treatment\ntreatmentmeans <- tapply(dat$expression, dat$treatment, mean)\ntreatmentmeans[\"A\"] \n\n       A \n1.172494 \n\n# Difference in means gives you the \"treatmentB\" coefficient from oneway.model\ntreatmentmeans[\"B\"] - treatmentmeans[\"A\"] \n\n        B \n0.4455249 \n\n\nWhat if you don’t want reference group coding? Another option is to fit a model without an intercept:\n\nno.intercept.model <- lm(expression ~ 0 + treatment, data = dat) # '0' means 'no intercept' here\nsummary(no.intercept.model)\n\n\nCall:\nlm(formula = expression ~ 0 + treatment, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.5353  0.1790  0.7725  3.6114 \n\nCoefficients:\n           Estimate Std. Error t value Pr(>|t|)    \ntreatmentA   1.1725     0.7783   1.506 0.147594    \ntreatmentB   1.6180     0.7783   2.079 0.050717 .  \ntreatmentC   2.0753     0.7783   2.666 0.014831 *  \ntreatmentD   3.7262     0.7783   4.787 0.000112 ***\ntreatmentE   8.5865     0.7783  11.032 5.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 20 degrees of freedom\nMultiple R-squared:  0.8878,    Adjusted R-squared:  0.8598 \nF-statistic: 31.66 on 5 and 20 DF,  p-value: 7.605e-09\n\ncoef(no.intercept.model)\n\ntreatmentA treatmentB treatmentC treatmentD treatmentE \n  1.172494   1.618019   2.075270   3.726161   8.586458 \n\n\nWithout the intercept, the coefficients here estimate the mean in each level of treatment:\n\ntreatmentmeans\n\n       A        B        C        D        E \n1.172494 1.618019 2.075270 3.726161 8.586458 \n\n\nThe no-intercept model is the SAME model as the reference group coded model, in the sense that it gives the same estimate for any comparison between groups:\nTreatment B - treatment A, reference group coded model:\n\ncoefs <- coef(oneway.model)\ncoefs[\"treatmentB\"]\n\ntreatmentB \n 0.4455249 \n\n\nTreatment B - treatment A, no-intercept model:\n\ncoefs <- coef(no.intercept.model)\ncoefs[\"treatmentB\"] - coefs[\"treatmentA\"]\n\ntreatmentB \n 0.4455249"
  },
  {
    "objectID": "session4/linear_models.html#batch-adjustment",
    "href": "session4/linear_models.html#batch-adjustment",
    "title": "21  Linear Models",
    "section": "21.4 Batch Adjustment",
    "text": "21.4 Batch Adjustment\nSuppose we want to adjust for batch differences in our model. We do this by adding the covariate “batch” to the model formula:\n\nbatch.model <- lm(expression ~ treatment + batch, data = dat)\nsummary(batch.model)\n\n\nCall:\nlm(formula = expression ~ treatment + batch, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.8337  0.0415  0.7725  3.6114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.1725     0.7757   1.512 0.147108    \ntreatmentB    0.4455     1.0970   0.406 0.689186    \ntreatmentC    1.9154     1.4512   1.320 0.202561    \ntreatmentD    4.2414     1.9263   2.202 0.040231 *  \ntreatmentE    9.1017     1.9263   4.725 0.000147 ***\nbatchBatch2  -1.6877     1.5834  -1.066 0.299837    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.735 on 19 degrees of freedom\nMultiple R-squared:  0.7667,    Adjusted R-squared:  0.7053 \nF-statistic: 12.49 on 5 and 19 DF,  p-value: 1.835e-05\n\ncoef(batch.model)\n\n(Intercept)  treatmentB  treatmentC  treatmentD  treatmentE batchBatch2 \n  1.1724940   0.4455249   1.9153967   4.2413688   9.1016661  -1.6877019 \n\n\nFor a model with more than one coefficient, summary provides estimates and tests for each coefficient adjusted for all the other coefficients in the model."
  },
  {
    "objectID": "session4/linear_models.html#two-factor-analysis",
    "href": "session4/linear_models.html#two-factor-analysis",
    "title": "21  Linear Models",
    "section": "21.5 Two-factor analysis",
    "text": "21.5 Two-factor analysis\nSuppose our experiment involves two factors, treatment and time. lm can be used to fit a two-way ANOVA model:\n\ntwoway.model <- lm(expression ~ treatment*time, data = dat)\nsummary(twoway.model)\n\n\nCall:\nlm(formula = expression ~ treatment * time, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0287 -0.4463  0.1082  0.4915  1.7623 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           0.97965    0.69239   1.415  0.17752    \ntreatmentB            0.40637    1.09476   0.371  0.71568    \ntreatmentC            1.00813    0.97918   1.030  0.31953    \ntreatmentD            3.07266    1.09476   2.807  0.01328 *  \ntreatmentE            9.86180    0.97918  10.071 4.55e-08 ***\ntimetime2             0.48211    1.09476   0.440  0.66594    \ntreatmentB:timetime2 -0.09544    1.54822  -0.062  0.95166    \ntreatmentC:timetime2 -0.26339    1.54822  -0.170  0.86718    \ntreatmentD:timetime2 -1.02568    1.54822  -0.662  0.51771    \ntreatmentE:timetime2 -6.11958    1.54822  -3.953  0.00128 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.199 on 15 degrees of freedom\nMultiple R-squared:  0.912, Adjusted R-squared:  0.8591 \nF-statistic: 17.26 on 9 and 15 DF,  p-value: 2.242e-06\n\ncoef(twoway.model)\n\n         (Intercept)           treatmentB           treatmentC \n          0.97965110           0.40636785           1.00813264 \n          treatmentD           treatmentE            timetime2 \n          3.07265513           9.86179766           0.48210723 \ntreatmentB:timetime2 treatmentC:timetime2 treatmentD:timetime2 \n         -0.09544075          -0.26339279          -1.02568281 \ntreatmentE:timetime2 \n         -6.11958364 \n\n\nThe notation treatment*time refers to treatment, time, and the interaction effect of treatment by time.\nInterpretation of coefficients:\n\nEach coefficient for treatment represents the difference between the indicated group and the reference group at the reference level for the other covariates\nFor example, “treatmentB” is the difference in expression between treatment B and treatment A at time 1\nSimilarly, “timetime2” is the difference in expression between time2 and time1 for treatment A\nThe interaction effects (coefficients with “:”) estimate the difference between treatment groups in the effect of time\nThe interaction effects ALSO estimate the difference between times in the effect of treatment\n\nTo estimate the difference between treatment B and treatment A at time 2, we need to include the interaction effects:\n\n# A - B at time 2\ncoefs <- coef(twoway.model)\ncoefs[\"treatmentB\"] + coefs[\"treatmentB:timetime2\"]\n\ntreatmentB \n 0.3109271 \n\n\nWe can see from summary that one of the interaction effects is significant. Here’s what that interaction effect looks like graphically:\n\ninteraction.plot(x.factor = dat$time, trace.factor = dat$treatment, response = dat$expression)\n\n\n\n\nIn the pasilla data, we can consider the affects of both the type and condition variables.\n\npasillaTwoFactor = pasilla\ndesign(pasillaTwoFactor) = formula(~ type + condition)\npasillaTwoFactor = DESeq(pasillaTwoFactor)\n\nWe access the results using the results function, which returns a dataframe with the statistics of each gene.\n\nres2 = results(pasillaTwoFactor)\nhead(res2, n = 3)\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE       stat    pvalue      padj\n            <numeric>      <numeric> <numeric>  <numeric> <numeric> <numeric>\nFBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA\nFBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975\nFBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA\n\n\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material. and the UCDavis Bioinformatics Core"
  },
  {
    "objectID": "session4/ggplot.html#data-visualization-with-ggplot2",
    "href": "session4/ggplot.html#data-visualization-with-ggplot2",
    "title": "22  Data Visualization in R",
    "section": "22.1 Data Visualization with ggplot2",
    "text": "22.1 Data Visualization with ggplot2\n\nFor this lesson, you will need the new_metadata data frame. Load it into your environment as follows:\n\n## load the new_metadata data frame into your environment from a .RData object\nload(\"../data/new_metadata.RData\")\n\nNext, let’s check if it was successfully loaded into the environment:\n\n# this data frame should have 12 rows and 5 columns\nView(new_metadata)\n\n\nWhen we are working with large sets of numbers it can be useful to display that information graphically to gain more insight. In this lesson we will be plotting with the popular Bioconductor package ggplot2.\nThe ggplot2 syntax takes some getting used to, but once you get it, you will find it’s extremely powerful and flexible. We will start with drawing a simple x-y scatterplot of samplemeans versus age_in_days from the new_metadata data frame. Please note that ggplot2 expects a dataframe or a tibble (the Tidyverse version of a dataframe) as input.\nLet’s start by loading the ggplot2 library:\n\nlibrary(ggplot2)\n\nThe ggplot() function is used to initialize the basic graph structure, then we add to it. The basic idea is that you specify different parts of the plot using additional functions one after the other and combine them into a “code chunk” using the + operator; the functions in the resulting code chunk are called layers.\nLet’s start:\n\nload(\"../data/new_metadata.RData\")\nggplot(new_metadata) # what happens? \n\n\n\n\nYou get an blank plot, because you need to specify additional layers using the + operator.\nThe geom (geometric) object is the layer that specifies what kind of plot we want to draw. A plot must have at least one geom; there is no upper limit. Examples include:\n\npoints (geom_point, geom_jitter for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for, well, boxplots!)\n\nLet’s add a “geom” layer to our plot using the + operator, and since we want a scatter plot so we will use geom_point().\nggplot(new_metadata) +\n  geom_point() # note what happens here\nWhy do we get an error? Is the error message easy to decipher?\nWe get an error because each type of geom usually has a required set of aesthetics to be set. “Aesthetics” are set with the aes() function and can be set either nested within geom_point() (applies only to that layer) or within ggplot() (applies to the whole plot).\nThe aes() function has many different arguments, and all of those arguments take columns from the original data frame as input. It can be used to specify many plot elements including the following:\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nTo start, we will specify x- and y-axis since geom_point requires the most basic information about a scatterplot, i.e. what you want to plot on the x and y axes. All of the other plot elements mentioned above are optional.\n\nggplot(new_metadata) +\n     geom_point(aes(x = age_in_days, y= samplemeans))\n\n\n\n\nNow that we have the required aesthetics, let’s add some extras like color to the plot. We can color the points on the plot based on the genotype column within aes(). You will notice that there are a default set of colors that will be used so we do not have to specify. Note that the legend has been conveniently plotted for us.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype)) \n\n\n\n\nLet’s try to have both celltype and genotype represented on the plot. To do this we can assign the shape argument in aes() the celltype column, so that each celltype is plotted with a different shaped data point.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype)) \n\n\n\n\nThe data points are quite small. We can adjust the size of the data points within the geom_point() layer, but it should not be within aes() since we are not mapping it to a column in the input data frame, instead we are just specifying a number.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) \n\n\n\n\nThe labels on the x- and y-axis are also quite small and hard to read. To change their size, we need to add an additional theme layer. The ggplot2 theme system handles non-data plot elements such as:\n\nAxis label aesthetics\nPlot background\nFacet label backround\nLegend appearance\n\nThere are built-in themes we can use (i.e. theme_bw()) that mostly change the background/foreground colours, by adding it as additional layer. Or we can adjust specific elements of the current default theme by adding the theme() layer and passing in arguments for the things we wish to change. Or we can use both.\nLet’s add a layer theme_bw().\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=3.0) +\n  theme_bw() \n\n\n\n\nDo the axis labels or the tick labels get any larger by changing themes?\nNo, they don’t. But, we can add arguments using theme() to change the size of axis labels ourselves. Since we will be adding this layer “on top”, or after theme_bw(), any features we change will override what is set by the theme_bw() layer.\nLet’s increase the size of both the axes titles to be 1.5 times the default size. When modifying the size of text the rel() function is commonly used to specify a change relative to the default.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) +\n  theme_bw() +\n  theme(axis.title = element_text(size=rel(1.5)))           \n\n\n\n\nWe can also make a boxplot of the data:"
  },
  {
    "objectID": "session4/ggplot.html#histogram",
    "href": "session4/ggplot.html#histogram",
    "title": "22  Data Visualization in R",
    "section": "22.2 Histogram",
    "text": "22.2 Histogram\nTo plot a histogram we require another type of geometric object called geom_histogram, which requires a statistical transformation. Some plot types (such as scatterplots) do not require transformations, each point is plotted at x and y coordinates equal to the original value. Other plots, such as boxplots, histograms, prediction lines etc. need to be transformed. Usually these objects have has a default statistic for the transformation, but that can be changed via the stat_bin argument.\nLet’s plot a histogram of sample mean expression in our data:\n\nggplot(new_metadata) +\n  geom_histogram(aes(x = samplemeans))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou will notice that even though the histogram is plotted, R gives a warning message `stat_bin() using bins = 30. Pick better value with binwidth.` These are the transformations we discussed. Apparently the default is not good enough.\nLet’s change the binwidth values. How does the plot differ?\n\nggplot(new_metadata) +\n  geom_histogram(aes(x = samplemeans), stat = \"bin\", binwidth=0.8)\n\n\n\n\n\n\nNOTE: You can use the example(\"geom_point\") function here to explore a multitude of different aesthetics and layers that can be added to your plot. As you scroll through the different plots, take note of how the code is modified. You can use this with any of the different geometric object layers available in ggplot2 to learn how you can easily modify your plots!\n\n\nNOTE: RStudio provide this very useful cheatsheet for plotting using ggplot2. Different example plots are provided and the associated code (i.e which geom or theme to use in the appropriate situation.) We also encourage you to persuse through this useful online reference for working with ggplot2.\n\nExercise 1: Themeing\nLet’s return to our scatterplot:\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) +\n  theme_bw() +\n  theme(axis.title = element_text(size=rel(1.5)))           \n\n\n\n\n\n\n\n\n\n\nBasic\n\n\n\n\nThe current axis label text defaults to what we gave as input to geom_point (i.e the column headers). We can change this by adding additional layers called xlab() and ylab() for the x- and y-axis, respectively. Add these layers to the current plot such that the x-axis is labeled “Age (days)” and the y-axis is labeled “Mean expression”.\nUse the ggtitle layer to add a plot title of your choice.\nAdd the following new layer to the code chunk theme(plot.title=element_text(hjust=0.5)).\n\n\nWhat does it change?\nHow many theme() layers can be added to a ggplot code chunk, in your estimation?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nWhen publishing, it is helpful to ensure all plots have similar formatting. To do this we can create a custom function with our preferences for the theme. Create a function called personal_theme which takes no arguments and\n\ncalls one of the ggplot2 themes such as theme_bw()\nsets the title text size to size=rel(1.5)\nsets the axis text size (you can use axis.title)\n\nOnce you have your function, call it to change your histogram’s theme.\n\n\n\n\n\n\n\n\nChallenge: Interactive Plots\n\n\n\nPlotly is another plotting library which has packages for multiple programming languages, including R and Python.\nOne of Plotly’s strengths is it’s ability to create interactive plots.\nFirst try making a simply interactive scatterplot with new_metadata and the same axes as the ggplot scatterplot. If you are able to do so, try adding dropdown menus which allow you to choose which column of new_metadata to color the points by.\n\n\nExercise 2: Boxplots\nA boxplot provides a graphical view of the distribution of data based on a five number summary: * The top and bottom of the box represent the (1) first and (2) third quartiles (25th and 75th percentiles, respectively). * The line inside the box represents the (3) median (50th percentile). * The whiskers extending above and below the box represent the (4) maximum, and (5) minimum of a data set. * The whiskers of the plot reach the minimum and maximum values that are not outliers.\n\nIn this case, outliers are determined using the interquartile range (IQR), which is defined as: Q3 - Q1. Any values that exceeds 1.5 x IQR below Q1 or above Q3 are considered outliers and are represented as points above or below the whiskers.\n\n\nBoxplot\n\nGenerate a boxplot using the data in the new_metadata dataframe. Create a ggplot2 code chunk with the following instructions:\n\nUse the geom_boxplot() layer to plot the differences in sample means between the Wt and KO genotypes.\nUse the fill aesthetic to look at differences in sample means between the celltypes within each genotype.\nAdd a title to your plot.\nAdd labels, ‘Genotype’ for the x-axis and ‘Mean expression’ for the y-axis.\nMake the following theme() changes:\n\nUse the theme_bw() function to make the background white.\nChange the size of your axes labels to 1.25x larger than the default.\nChange the size of your plot title to 1.5x larger than default.\nCenter the plot title.\n\n\nAfter running the above code the boxplot should look something like that provided below.\n\n\n\n\nChanging the order of genotype on the Boxplot\n\nLet’s say you wanted to have the “Wt” boxplots displayed first on the left side, and “KO” on the right. How might you go about doing this?\nTo do this, your first question should be - How does ggplot2 determine what to place where on the X-axis? * The order of the genotype on the X axis is in alphabetical order. * To change it, you need to make sure that the genotype column is a factor * And, the factor levels for that column are in the order you want on the X-axis\n\nFactor the new_metadata$genotype column without creating any extra variables/objects and change the levels to c(\"Wt\", \"KO\")\nRe-run the boxplot code chunk you created for the “Boxplot!” exercise above.\nChanging default colors\n\nYou can color the boxplot differently by using some specific layers:\n\nAdd a new layer scale_color_manual(values=c(\"purple\",\"orange\")).\n\nDo you observe a change?\n\nReplace scale_color_manual(values=c(\"purple\",\"orange\")) with scale_fill_manual(values=c(\"purple\",\"orange\")).\n\nDo you observe a change?\nIn the scatterplot we drew in class, add a new layer scale_color_manual(values=c(\"purple\",\"orange\")), do you observe a difference?\nWhat do you think is the difference between scale_color_manual() and scale_fill_manual()?\n\nBack in your boxplot code, change the colors in the scale_fill_manual() layer to be your 2 favorite colors.\n\nAre there any colors that you tried that did not work?\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/exporting.html#writing-data-to-file",
    "href": "session4/exporting.html#writing-data-to-file",
    "title": "23  Saving Data and Figures in R",
    "section": "23.1 Writing data to file",
    "text": "23.1 Writing data to file\nEverything we have done so far has only modified the data in R; the files have remained unchanged. Whenever we want to save our datasets to file, we need to use a write function in R.\nTo write our matrix to file in comma separated format (.csv), we can use the write.csv function. There are two required arguments: the variable name of the data structure you are exporting, and the path and filename that you are exporting to. By default the delimiter or column separator is set, and columns will be separated by a comma:\n# Save a data frame to file\nwrite.csv(sub_meta, file=\"data/subset_meta.csv\")\nOftentimes the output is not exactly what you might want. You can modify the output using the arguments for the function. We can explore the arguments using the ?. This can help elucidate what each of the arguments can adjust the output.\n?write.csv\nSimilar to reading in data, there are a wide variety of functions available allowing you to export data in specific formats. Another commonly used function is write.table, which allows you to specify the delimiter or separator you wish to use. This function is commonly used to create tab-delimited files.\n\nNOTE: Sometimes when writing a data frame using row names to file with write.table(), the column names will align starting with the row names column. To avoid this, you can include the argument col.names = NA when writing to file to ensure all of the column names line up with the correct column values.\n\nWriting a vector of values to file requires a different function than the functions available for writing dataframes. You can use write() to save a vector of values to file. For example:\n# Save a vector to file\nwrite(glengths, file=\"data/genome_lengths.txt\")\nIf we wanted the vector to be output to a single column instead of five, we could explore the arguments:\n?write\nNote, the ncolumns argument that it defaults to five columns unless specified, so to get a single column:\n# Save a vector to file as a single column\nwrite(glengths, file=\"data/genome_lengths.txt\", ncolumns = 1)"
  },
  {
    "objectID": "session4/exporting.html#exporting-figures-to-file",
    "href": "session4/exporting.html#exporting-figures-to-file",
    "title": "23  Saving Data and Figures in R",
    "section": "23.2 Exporting figures to file",
    "text": "23.2 Exporting figures to file\nThere are two ways in which figures and plots can be output to a file (rather than simply displaying on screen).\n\nThe first (and easiest) is to export directly from the RStudio ‘Plots’ panel, by clicking on Export when the image is plotted. This will give you the option of png or pdf and selecting the directory to which you wish to save it to. It will also give you options to dictate the size and resolution of the output image.\nThe second option is to use R functions and have the write to file hard-coded in to your script. This would allow you to run the script from start to finish and automate the process (not requiring human point-and-click actions to save). In R’s terminology, output is directed to a particular output device and that dictates the output format that will be produced. A device must be created or “opened” in order to receive graphical output and, for devices that create a file on disk, the device must also be closed in order to complete the output.\n\nIf we wanted to print our scatterplot to a pdf file format, we would need to initialize a plot using a function which specifies the graphical format you intend on creating i.e.pdf(), png(), tiff() etc. Within the function you will need to specify a name for your image, and the with and height (optional). This will open up the device that you wish to write to:\n## Open device for writing\npdf(\"figures/scatterplot.pdf\")\nIf you wish to modify the size and resolution of the image you will need to add in the appropriate parameters as arguments to the function when you initialize. Then we plot the image to the device, using the ggplot scatterplot that we just created.\n## Make a plot which will be written to the open device, in this case the temp file created by pdf()/png()\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=rel(3.0)) \nFinally, close the “device”, or file, using the dev.off() function. There are also bmp, tiff, and jpeg functions, though the jpeg function has proven less stable than the others.\n## Closing the device is essential to save the temporary file created by pdf()/png()\ndev.off()\nNote 1: You will not be able to open and look at your file using standard methods (Adobe Acrobat or Preview etc.) until you execute the dev.off() function.\nNote 2: In the case of pdf(), if you had made additional plots before closing the device, they will all be stored in the same file with each plot usually getting its own page, unless otherwise specified.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/other_plots.html#pca-plot",
    "href": "session4/other_plots.html#pca-plot",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.1 PCA plot",
    "text": "24.1 PCA plot\nFirst we can create a principle component analysis (PCA) of the data.\n\nplotPCA(rld, intgroup = c(\"dex\", \"cell\"))\n\n\n\n\nPCA is a dimentionality reduction method. It reduced the dimensionality of our data while maintaining as much variation as possible. In the above data, it would be impossible to view how each of our samples compare across every gene at the same time. PCA finds linear combinations of genes which best explain the variance between each sample. We can see how much variance is explained by each principle component. When examining a PCA plot, we want to make sure that our samples group as expected, mainly, that replicates are closer to each other than to other samples.\nFor other data, we can use the prcomp function to perform a PCA analysis.\n\nlibrary(ggfortify, quietly=TRUE)\n\nWarning: package 'ggfortify' was built under R version 4.2.2\n\ndf <- iris[1:4]\npca_res <- prcomp(df, scale. = TRUE)\nautoplot(pca_res, data = iris, colour = 'Species')"
  },
  {
    "objectID": "session4/other_plots.html#tsne-plots",
    "href": "session4/other_plots.html#tsne-plots",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.2 tSNE Plots",
    "text": "24.2 tSNE Plots\nt-Distributed Neighbor Embedding (tSNE) is another dimensionality reduction method mainly used for visualization which can preform non-linear transformations. It finds the distances between points in the original, high-dimensional space, then attempts to find a low-dimensional space which maintains distances between points and their close neighbors. tSNE is stochastic, meaning that there is some randomness in its final embedding. Running tSNE multiple times on the same data will give slightly different results.\nMany biological analysis pipelines also have built-in TSNE analyses and visualizations. This one is a part of the Bioconductor scater package.\n\nlibrary(scRNAseq)\nlibrary(scater)\nlibrary(scran)\nlibrary(BiocSingular)\n\n\nload(\"../data/processedZeisel.RData\")\nsce.zeisel <- runTSNE(sce.zeisel, dimred=\"PCA\")\nplotTSNE(sce.zeisel, colour_by=\"label\")"
  },
  {
    "objectID": "session4/other_plots.html#volcano-plot",
    "href": "session4/other_plots.html#volcano-plot",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.3 Volcano plot",
    "text": "24.3 Volcano plot\nA volcano plot is a common visualization to see the distribution of fold-changes and p values accross our dataset. It plots the \\(log_2\\) fold-change against the p values from our statistical analysis.\n\nres %>% \n  mutate(sig = padj < 0.05 & abs(log2FoldChange) > 2) %>%\n  ggplot(aes(x = log2FoldChange, y = -log10(padj), col=sig)) +\n       geom_point() +\n       geom_vline(xintercept=c(-2, 2), col=\"red\") +\n       geom_hline(yintercept=-log10(0.05), col=\"red\")\n\nWarning: Removed 8077 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "session4/other_plots.html#heatmapclustergram",
    "href": "session4/other_plots.html#heatmapclustergram",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.4 Heatmap/Clustergram",
    "text": "24.4 Heatmap/Clustergram\nHeatmaps are a common visualization in a variety of analyses. By default in the heatmap package pheatmap, the rows and columns of our data are clustered using a hierarchical clustering method. This allows us to view which samples are most similar, and here how the most differentially expressed genes cluster and change across different samples.\nThe data we are using below is a microarray dataset investigating empryo development in mice which can be read about here.\n\nlibrary(pheatmap, quietly=TRUE)\n\nWarning: package 'pheatmap' was built under R version 4.2.2\n\nlibrary(\"Hiiragi2013\", quietly=TRUE)\n\nWarning: package 'boot' was built under R version 4.2.2\n\n\nWarning: package 'clue' was built under R version 4.2.2\n\n\nWarning: package 'cluster' was built under R version 4.2.2\n\n\nWarning: package 'genefilter' was built under R version 4.2.2\n\n\n\nAttaching package: 'genefilter'\n\n\nThe following object is masked from 'package:readr':\n\n    spec\n\n\nThe following objects are masked from 'package:MatrixGenerics':\n\n    rowSds, rowVars\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    rowSds, rowVars\n\n\n\nAttaching package: 'lattice'\n\n\nThe following object is masked from 'package:boot':\n\n    melanoma\n\n\n\nAttaching package: 'AnnotationDbi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nWarning: package 'XML' was built under R version 4.2.2\n\n\nWarning: package 'gplots' was built under R version 4.2.2\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:IRanges':\n\n    space\n\n\nThe following object is masked from 'package:S4Vectors':\n\n    space\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\nWarning: package 'gtools' was built under R version 4.2.2\n\n\n\nAttaching package: 'gtools'\n\n\nThe following objects are masked from 'package:boot':\n\n    inv.logit, logit\n\n\nWarning: package 'MASS' was built under R version 4.2.2\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:AnnotationDbi':\n\n    select\n\n\nThe following object is masked from 'package:genefilter':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n\n\n\n\n\nWarning: package 'xtable' was built under R version 4.2.2\n\nlibrary(\"dplyr\", quietly = TRUE)\ndata(\"x\") \n\n# Create a small dataframe summarizing each group\ngroups = group_by(pData(x), sampleGroup) %>%\n  summarise(n = n(), color = unique(sampleColour))\n# Get a color for that group\ngroupColor = setNames(groups$color, groups$sampleGroup)\n\ntopGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]\nrowCenter = function(x) { x - rowMeans(x) }\npheatmap( rowCenter(Biobase::exprs(x)[ topGenes, ] ),\n  show_rownames = FALSE, show_colnames = FALSE,\n  breaks = seq(-5, +5, length = 101),\n  annotation_col =\n    pData(x)[, c(\"sampleGroup\", \"Embryonic.day\", \"ScanDate\") ],\n  annotation_colors = list(\n    sampleGroup = groupColor,\n    genotype = c(`FGF4-KO` = \"chocolate1\", `WT` = \"azure2\"),\n    Embryonic.day = setNames(brewer.pal(9, \"Blues\")[c(3, 6, 9)],\n                             c(\"E3.25\", \"E3.5\", \"E4.5\")),\n    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), \"YlGn\"),\n                        levels(x$ScanDate))\n  ),\n  cutree_rows = 4\n)"
  },
  {
    "objectID": "session4/other_plots.html#network-visualization",
    "href": "session4/other_plots.html#network-visualization",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.5 Network visualization",
    "text": "24.5 Network visualization\nWe also sometimes want to visualize networks in R. Network visualization can be difficult, and it is reccomended to use programs like cytoscape for creating publication-ready network figures. However, igraph is a popular package for handling and visualizing network data in R.\n\nlibrary(igraph, quietly = TRUE)\n\n\ng <- make_ring(10)\nplot(g, layout=layout_with_kk, vertex.color=\"green\")\n\n\n\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/pSet4.html#problem-1",
    "href": "session4/pSet4.html#problem-1",
    "title": "25  Problem Set 4",
    "section": "25.1 Problem 1",
    "text": "25.1 Problem 1\nR actually also has built-in plotting functionality, though it is rarely used in modern analyses. Let’s make some visualizations of another ELISA assay dataset which is included with R, DNase.\n\ndata(DNase)\nhead(DNase)\n\n  Run       conc density\n1   1 0.04882812   0.017\n2   1 0.04882812   0.018\n3   1 0.19531250   0.121\n4   1 0.19531250   0.124\n5   1 0.39062500   0.206\n6   1 0.39062500   0.215\n\n\nThis assay was used to quantify the activity of the enzyme deoxyribonuclease (DNase).\nWe can make a boxplot of the density of each run:\n\nhist(DNase$density, breaks=25, main = \"\")\n\n\n\nboxplot(density ~ Run, data = DNase)\n\n\n\n\nCreate a ggplot2 boxplot displaying the density distribution for each run of the DNAse object. Order the boxes in numerical order along the \\(x\\)-axis instead of lexicographical order (hint: as.numeric). Display each box with a different color (hint: rainbow)."
  },
  {
    "objectID": "session4/pSet4.html#problem-2",
    "href": "session4/pSet4.html#problem-2",
    "title": "25  Problem Set 4",
    "section": "25.2 Problem 2",
    "text": "25.2 Problem 2\nWe continue working with a gene expression microarray dataset that reports the gene expression of around 100 individual cells from mouse embryos at different time points in early development (the Hiiragi2013 data: Ohnishi et al., 2014).\n\npdat <- read.delim(\"../data/Hiiragi2013_pData.txt\", as.is = TRUE)\nhead(pdat, n = 2)\n\n  File.name Embryonic.day Total.number.of.cells lineage genotype   ScanDate\n1  1_C32_IN         E3.25                    32               WT 2011-03-16\n2  2_C32_IN         E3.25                    32               WT 2011-03-16\n  sampleGroup sampleColour\n1       E3.25      #CAB2D6\n2       E3.25      #CAB2D6\n\n\nCreate a ggplot2 barplot displaying the total number of cells accross each sample group of the pdat dataset. You can use the aggregate function with sum to calculate these totals. Flip the \\(x\\)- and \\(y\\)-aesthetics to produce a horizontal barplot. Rotate the group labels by 90 degrees Hint, element_text has an angle argument, and a single axis’ text can be accessed by axis.text.x or axis.text.y."
  },
  {
    "objectID": "session4/pSet4.html#problem-3",
    "href": "session4/pSet4.html#problem-3",
    "title": "25  Problem Set 4",
    "section": "25.3 Problem 3",
    "text": "25.3 Problem 3\nChoose a plot you created during Session 4, or another plot from your own research. Show the original plot, then work to get the plot into a ‘publication-ready’ state, either for a paper, poster, or presentation. You can choose which one of these 3 scenarios you want to create your figure for. Some things to consider:\n\nAre your colors colorblind safe?\nFont sizes in posters need to be very large, followed by presentation and then paper font sizes. We also need to consider things like line thickness and the size of any points in a scatterplot. The Python plotting library Seaborn has nice examples of how the sizes should differ.\nText should not overlap.\nLegends should be clear and use neat, human readable labels as opposed to the names of columns in R (i.e. something like “Number of Cells” or “# Cells” as opposed to “Total.number.of.cells”).\nPoster and presentation figures typically have titles, while a paper figure typically does not.\n\nInclude code for saving your publication-ready figure as a pdf."
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This chapter contains relevant literature, additional educational resources, and some guides on specific actions you might want to take using the workbook."
  },
  {
    "objectID": "resources/git.html#what-is-git-github",
    "href": "resources/git.html#what-is-git-github",
    "title": "26  Getting Started with Git & Github",
    "section": "26.1 What is Git / GitHub",
    "text": "26.1 What is Git / GitHub\nGit is a file version control system that helps you keep track of any changes you make to specific documents (e.g., code). This a much more elegant solution than copying a file over and over and changing the name to things like: file_version1, file_version2, file_final, file_finalVersion, file_final_finalVersion …\nWatch this video to get a little more of an introduction to Git.\nGitHub is an online service that allows you to share Git repositories with other people. You can either Pull an existing repository to you machine and start working on it yourself, or you can Push any of your repositories (or changes made to someone else’s) to GitHub to share them with others (or even yourself if you have multiple computers).\nWatch this video to see how GitHub can help distribute code safely between many people without causing issues."
  },
  {
    "objectID": "resources/git.html#create-a-github-account",
    "href": "resources/git.html#create-a-github-account",
    "title": "26  Getting Started with Git & Github",
    "section": "26.2 Create a Github Account",
    "text": "26.2 Create a Github Account\nIf you do not have one already go to github.com and register for a new account. We recommend you use a personal email address to sign up as your GitHub is seen as a personal asset. However, with an academic email you can unlock more features so once registered you can add your Harvard or other .edu email address to get educational benefits as a student or as a teacher/researcher."
  },
  {
    "objectID": "resources/git.html#option-1-github-desktop-reccomended",
    "href": "resources/git.html#option-1-github-desktop-reccomended",
    "title": "26  Getting Started with Git & Github",
    "section": "26.3 Option 1: Github Desktop (reccomended)",
    "text": "26.3 Option 1: Github Desktop (reccomended)\nIf you are unfamiliar with using the command line, Github Desktop can be a good place to start.\nYou can then go to the workbook repository and connect it to your Github Desktop:\n\n\n\nConnecting the repo to Github Desktop\n\n\nFinally, you can click fetch (the button may say pull) from within the Github Desktop client to download files locally.\n\n\n\nFetching files"
  },
  {
    "objectID": "resources/git.html#option-2-command-line",
    "href": "resources/git.html#option-2-command-line",
    "title": "26  Getting Started with Git & Github",
    "section": "26.4 Option 2: Command line",
    "text": "26.4 Option 2: Command line\nFollow the instructions here to install Git or a Git client on your computer.\nIt is recommended for you to setup your local username and email address before using Git, and in some cases is required. This does not need to match GitHub (we’ll do that next). You will need to use a terminal window for this (Command Prompt in Windows or Terminal on Mac). You can also use the Terminal window in RStudio if you prefer (different from the Console window)\n`git config --global user.name \"First Last\"`\n\n`git config --global user.email \"me@email.com\"`\nWatch detailed instructions in this video if needed\nYou can then clone the repository locally.\ngit clone https://github.com/ccb-hms/systems-immunology-workshops.git\nWhenever the workbook is updated, you can pull it to download the changes. Within the systems-immunology-workshops directory, simply enter:\ngit pull"
  },
  {
    "objectID": "resources/git.html#option-3-integrate-git-github-with-rstudio",
    "href": "resources/git.html#option-3-integrate-git-github-with-rstudio",
    "title": "26  Getting Started with Git & Github",
    "section": "26.5 Option 3: Integrate Git /GitHub with RStudio",
    "text": "26.5 Option 3: Integrate Git /GitHub with RStudio\nRStudio has an integrated Git user interface that makes it very easy to use both Git and GitHub.\nTo get a copy of the workbook repository in RStudio do the following:\n\nClick File → New Project\nSelect Version Control → Git\nFor the URL choose: https://github.com/ccb-hms/systems-immunology-workshops.git\nYou can choose the name of the project directory.\nChoose the folder in which you want to store the R project and Git (depends on how you organize your files)\nClick Create Project\nCheck the Files tab to see if you have successfully created the project\n\nWhenever you are working in an RStudio project that has a dedicated Git repository, you can interact with Git through the Git tab (same pane as Environment tab)"
  },
  {
    "objectID": "resources/git.html#stashing-changes-if-needed",
    "href": "resources/git.html#stashing-changes-if-needed",
    "title": "26  Getting Started with Git & Github",
    "section": "26.6 Stashing changes if needed",
    "text": "26.6 Stashing changes if needed\nIf you edit your local copy of the workbook, when you try to pull or fetch files in the future you may run into an error. This is because Git isn’t sure whether you want to discard your local changes or not. You can stash your local changes, pull/fetch, and then pop your changes to download the new files and integrate your changes. However, if you have edited files which have also been updated, such as writing a solution in a file which then had an added solution, you may get a merge conflict."
  },
  {
    "objectID": "resources/git.html#resources",
    "href": "resources/git.html#resources",
    "title": "26  Getting Started with Git & Github",
    "section": "26.7 Resources:",
    "text": "26.7 Resources:\n\nA good git reference book\nGit desktop environments: https://desktop.github.com/ and those from https://git-scm.com/downloads\nRStudio and git guide\nA great interactive site for learning Git\nA useful git cheat sheet\nSoftware Carpentry’s git lessons\nGithub’s Git guides"
  },
  {
    "objectID": "session3/tidyverse.html#additional-resources",
    "href": "session3/tidyverse.html#additional-resources",
    "title": "19  Tidyverse",
    "section": "20.14 Additional resources",
    "text": "20.14 Additional resources\n\nR for Data Science\nteach the tidyverse\ntidy style guide\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  }
]