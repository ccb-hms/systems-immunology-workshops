[
  {
    "objectID": "session5/enrichment_analysis.html#where-does-it-all-come-from",
    "href": "session5/enrichment_analysis.html#where-does-it-all-come-from",
    "title": "28  Functional enrichment analysis",
    "section": "28.1 Where does it all come from?",
    "text": "28.1 Where does it all come from?\nTest whether known biological functions or processes are over-represented (= enriched) in an experimentally-derived gene list, e.g. a list of differentially expressed (DE) genes. See Goeman and Buehlmann, 2007 for a critical review.\nExample: Transcriptomic study, in which 12,671 genes have been tested for differential expression between two sample conditions and 529 genes were found DE.\nAmong the DE genes, 28 are annotated to a specific functional gene set, which contains in total 170 genes. This setup corresponds to a 2x2 contingency table,\n\ndeTable <-\n     matrix(c(28, 142, 501, 12000),\n            nrow = 2,\n            dimnames = list(c(\"DE\", \"Not.DE\"),\n                            c(\"In.gene.set\", \"Not.in.gene.set\")))\ndeTable\n\n       In.gene.set Not.in.gene.set\nDE              28             501\nNot.DE         142           12000\n\n\nwhere the overlap of 28 genes can be assessed based on the hypergeometric distribution. This corresponds to a one-sided version of Fisher’s exact test, yielding here a significant enrichment.\n\nfisher.test(deTable, alternative = \"greater\")\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  deTable\np-value = 4.088e-10\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 3.226736      Inf\nsample estimates:\nodds ratio \n  4.721744 \n\n\nThis basic principle is at the foundation of major public and commercial enrichment tools such as DAVID and Pathway Studio."
  },
  {
    "objectID": "session5/enrichment_analysis.html#gene-expression-based-enrichment-analysis",
    "href": "session5/enrichment_analysis.html#gene-expression-based-enrichment-analysis",
    "title": "28  Functional enrichment analysis",
    "section": "28.2 Gene expression-based enrichment analysis",
    "text": "28.2 Gene expression-based enrichment analysis\nThe EnrichmentBrowser package implements an analysis pipeline for high-throughput gene expression data as measured with microarrays and RNA-seq. In a workflow-like manner, the package brings together a selection of established Bioconductor packages for gene expression data analysis. It integrates a wide range of gene set enrichment analysis methods and facilitates combination and exploration of results across methods.\n\n\n\nEnrichmentBrowser workflow summary\n\n\n\nlibrary(EnrichmentBrowser)\n\nWarning: package 'matrixStats' was built under R version 4.2.2\n\n\nWarning: package 'GenomicRanges' was built under R version 4.2.2\n\n\nWarning: package 'S4Vectors' was built under R version 4.2.2\n\n\nWarning: package 'GenomeInfoDb' was built under R version 4.2.2\n\n\nFurther information can be found in the vignette and publication."
  },
  {
    "objectID": "session5/enrichment_analysis.html#data-types",
    "href": "session5/enrichment_analysis.html#data-types",
    "title": "28  Functional enrichment analysis",
    "section": "28.3 Data types",
    "text": "28.3 Data types\nFor RNA-seq data, we consider transcriptome profiles of four primary human airway smooth muscle cell lines in two conditions: control and treatment with dexamethasone Himes et al., 2014.\nLoad the airway dataset\n\nlibrary(airway)\ndata(airway)\n\nFor further analysis, we only keep genes that are annotated to an ENSEMBL gene ID.\n\nairSE <- airway[grep(\"^ENSG\", names(airway)), ]\ndim(airSE)\n\n[1] 63677     8\n\n\n\nassay(airSE)[1:4,1:4]\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513\nENSG00000000003        679        448        873        408\nENSG00000000005          0          0          0          0\nENSG00000000419        467        515        621        365\nENSG00000000457        260        211        263        164"
  },
  {
    "objectID": "session5/enrichment_analysis.html#differential-expression-analysis",
    "href": "session5/enrichment_analysis.html#differential-expression-analysis",
    "title": "28  Functional enrichment analysis",
    "section": "28.4 Differential expression analysis",
    "text": "28.4 Differential expression analysis\nThe EnrichmentBrowser incorporates established functionality from the limma package for differential expression analysis. This involves the voom transformation when applied to RNA-seq data. Alternatively, differential expression analysis for RNA-seq data can also be carried out based on the negative binomial distribution with edgeR and DESeq2.\nThis can be performed using the function EnrichmentBrowser::deAna and assumes some standardized variable names:\n\nGROUP defines the sample groups being contrasted,\nBLOCK defines paired samples or sample blocks, as e.g. for batch effects.\n\nFor more information on experimental design, see the limma user’s guide, chapter 9.\nFor the airway dataset, it indicates whether the cell lines have been treated with dexamethasone (1) or not (0).\nTask: Add a colData column named GROUP to the airSE. This column should be a binary vector which indicates wether the cell lines have been treated with dexamethasone (1) or not (0).\n\n\n\nPaired samples, or in general sample batches/blocks, can be defined via a BLOCK column in the colData slot. For the airway dataset, the sample blocks correspond to the four different cell lines.\n\nairSE$BLOCK <- airway$cell\ntable(airSE$BLOCK)\n\n\nN052611 N061011 N080611  N61311 \n      2       2       2       2 \n\n\nFor RNA-seq data, the deAna function can be used to carry out differential expression analysis between the two groups either based on functionality from limma (that includes the voom transformation), or alternatively, the frequently used edgeR or DESeq2 package. Here, we use the analysis based on edgeR.\n\nairSE <- deAna(airSE, de.method = \"edgeR\")\n\nExcluding 47751 genes not satisfying min.cpm threshold\n\n\n\nrowData(airSE)\n\nDataFrame with 15926 rows and 4 columns\n                        FC edgeR.STAT        PVAL   ADJ.PVAL\n                 <numeric>  <numeric>   <numeric>  <numeric>\nENSG00000000003 -0.3901002 31.0558140 0.000232422 0.00217355\nENSG00000000419  0.1978022  6.6454709 0.027419893 0.07560513\nENSG00000000457  0.0291609  0.0929623 0.766666551 0.84808859\nENSG00000000460 -0.1243820  0.3832263 0.549659194 0.67996523\nENSG00000000971  0.4172901 28.7686093 0.000312276 0.00272063\n...                    ...        ...         ...        ...\nENSG00000273373 -0.0438722  0.0397087   0.8460260   0.901607\nENSG00000273382 -0.8597567  7.7869742   0.0190219   0.057267\nENSG00000273448  0.0281667  0.0103270   0.9752405   0.984888\nENSG00000273472 -0.4642705  1.9010366   0.1978818   0.328963\nENSG00000273486 -0.1109445  0.1536285   0.7032766   0.802377\n\n\nExercise: Compare the number of differentially expressed genes as obtained on the airSE with limma/voom, edgeR, and DESeq2."
  },
  {
    "objectID": "session5/enrichment_analysis.html#gene-sets",
    "href": "session5/enrichment_analysis.html#gene-sets",
    "title": "28  Functional enrichment analysis",
    "section": "28.5 Gene sets",
    "text": "28.5 Gene sets\nWe are now interested in whether pre-defined sets of genes that are known to work together, e.g. as defined in the Gene Ontology or the KEGG pathway annotation, are coordinately differentially expressed.\nGene sets, pathways & regulatory networks\nGene sets are simple lists of usually functionally related genes without further specification of relationships between genes.\nPathways can be interpreted as specific gene sets, typically representing a group of genes that work together in a biological process. Pathways are commonly divided in metabolic and signaling pathways. Metabolic pathways such as glycolysis represent biochemical substrate conversions by specific enzymes. Signaling pathways such as the MAPK signaling pathway describe signal transduction cascades from receptor proteins to transcription factors, resulting in activation or inhibition of specific target genes.\nGene regulatory networks describe the interplay and effects of regulatory factors (such as transcription factors and microRNAs) on the expression of their target genes.\nResources\nGO and KEGG annotations are most frequently used for the enrichment analysis of functional gene sets. Despite an increasing number of gene set and pathway databases, they are typically the first choice due to their long-standing curation and availability for a wide range of species.\nGO: The Gene Ontology (GO) consists of three major sub-ontologies that classify gene products according to molecular function (MF), biological process (BP) and cellular component (CC). Each ontology consists of GO terms that define MFs, BPs or CCs to which specific genes are annotated. The terms are organized in a directed acyclic graph, where edges between the terms represent relationships of different types. They relate the terms according to a parent-child scheme, i.e. parent terms denote more general entities, whereas child terms represent more specific entities.\nKEGG: The Kyoto Encyclopedia of Genes and Genomes (KEGG) is a collection of manually drawn pathway maps representing molecular interaction and reaction networks. These pathways cover a wide range of biochemical processes that can be divided in 7 broad categories: metabolism, genetic and environmental information processing, cellular processes, organismal systems, human diseases, and drug development. Metabolism and drug development pathways differ from pathways of the other 5 categories by illustrating reactions between chemical compounds. Pathways of the other 5 categories illustrate molecular interactions between genes and gene products.\nThe function getGenesets can be used to download gene sets from databases such as GO and KEGG. Here, we use the function to download all KEGG pathways for a chosen organism (here: Homo sapiens) as gene sets.\n\nkegg.gs <- getGenesets(org = \"hsa\", db = \"kegg\")\nkegg.gs[1:2]\n\n$hsa01100_Metabolic_pathways\ncharacter(0)\n\n$hsa01200_Carbon_metabolism\ncharacter(0)\n\n\nAnalogously, the function getGenesets can be used to retrieve GO terms of a selected ontology (here: biological process, BP) as defined in the GO.db annotation package.\n\ngo.gs <- getGenesets(org = \"hsa\", db = \"go\", onto = \"BP\", mode = \"GO.db\")\ngo.gs[1:2]\n\n$`GO:0000002_mitochondrial_genome_maintenance`\n [1] \"291\"   \"1890\"  \"4205\"  \"4358\"  \"4976\"  \"9361\"  \"10000\" \"55186\" \"55186\"\n[10] \"80119\" \"84275\" \"84275\" \"92667\"\n\n$`GO:0000003_reproduction`\n[1] \"2796\"   \"2797\"   \"8510\"   \"286826\"\n\n\nIf provided a file, the function getGenesets parses user-defined gene sets from GMT file format. Here, we use this functionality for reading a list of already downloaded KEGG gene sets for Homo sapiens containing NCBI Entrez Gene IDs.\n\ndata.dir <- system.file(\"extdata\", package = \"EnrichmentBrowser\")\ngmt.file <- file.path(data.dir, \"hsa_kegg_gs.gmt\")\nhsa.gs <- getGenesets(gmt.file)\nhsa.gs[1:2]\n\n$hsa05416_Viral_myocarditis\n [1] \"100509457\" \"101060835\" \"1525\"      \"1604\"      \"1605\"      \"1756\"     \n [7] \"1981\"      \"1982\"      \"25\"        \"2534\"      \"27\"        \"3105\"     \n[13] \"3106\"      \"3107\"      \"3108\"      \"3109\"      \"3111\"      \"3112\"     \n[19] \"3113\"      \"3115\"      \"3117\"      \"3118\"      \"3119\"      \"3122\"     \n[25] \"3123\"      \"3125\"      \"3126\"      \"3127\"      \"3133\"      \"3134\"     \n[31] \"3135\"      \"3383\"      \"3683\"      \"3689\"      \"3908\"      \"4624\"     \n[37] \"4625\"      \"54205\"     \"5551\"      \"5879\"      \"5880\"      \"5881\"     \n[43] \"595\"       \"60\"        \"637\"       \"6442\"      \"6443\"      \"6444\"     \n[49] \"6445\"      \"71\"        \"836\"       \"841\"       \"842\"       \"857\"      \n[55] \"8672\"      \"940\"       \"941\"       \"942\"       \"958\"       \"959\"      \n\n$`hsa04622_RIG-I-like_receptor_signaling_pathway`\n [1] \"10010\"  \"1147\"   \"1432\"   \"1540\"   \"1654\"   \"23586\"  \"26007\"  \"29110\" \n [9] \"338376\" \"340061\" \"3439\"   \"3440\"   \"3441\"   \"3442\"   \"3443\"   \"3444\"  \n[17] \"3445\"   \"3446\"   \"3447\"   \"3448\"   \"3449\"   \"3451\"   \"3452\"   \"3456\"  \n[25] \"3467\"   \"3551\"   \"3576\"   \"3592\"   \"3593\"   \"3627\"   \"3661\"   \"3665\"  \n[33] \"4214\"   \"4790\"   \"4792\"   \"4793\"   \"5300\"   \"54941\"  \"55593\"  \"5599\"  \n[41] \"5600\"   \"5601\"   \"5602\"   \"5603\"   \"56832\"  \"57506\"  \"5970\"   \"6300\"  \n[49] \"64135\"  \"64343\"  \"6885\"   \"7124\"   \"7186\"   \"7187\"   \"7189\"   \"7706\"  \n[57] \"79132\"  \"79671\"  \"80143\"  \"841\"    \"843\"    \"8517\"   \"8717\"   \"8737\"  \n[65] \"8772\"   \"9140\"   \"9474\"   \"9636\"   \"9641\"   \"9755\"  \n\n\nNote #1: Gene set collections for 11 different species from the\nMolecular Signatures Database (MSigDB) can be obtained using getGenesets with db = \"msigdb\". For example, the Hallmark gene set collection can be obtained from MSigDB via:\n\nhall.gs <- getGenesets(org = \"hsa\", db = \"msigdb\", cat = \"H\") \n\nWarning: package 'msigdbr' was built under R version 4.2.2\n\nhall.gs[1:2]\n\n$M5890_HALLMARK_TNFA_SIGNALING_VIA_NFKB\n  [1] \"19\"     \"57007\"  \"374\"    \"467\"    \"490\"    \"2683\"   \"9334\"   \"597\"   \n  [9] \"602\"    \"604\"    \"8553\"   \"329\"    \"330\"    \"650\"    \"694\"    \"7832\"  \n [17] \"10950\"  \"10950\"  \"6347\"   \"6364\"   \"6351\"   \"6351\"   \"6351\"   \"6352\"  \n [25] \"6352\"   \"3491\"   \"595\"    \"57018\"  \"9034\"   \"960\"    \"969\"    \"941\"   \n [33] \"9308\"   \"1026\"   \"1051\"   \"1052\"   \"8837\"   \"23529\"  \"1435\"   \"1437\"  \n [41] \"2919\"   \"3627\"   \"6373\"   \"2920\"   \"2921\"   \"6372\"   \"23586\"  \"23258\" \n [49] \"11080\"  \"55332\"  \"1843\"   \"1844\"   \"1846\"   \"1847\"   \"1906\"   \"1942\"  \n [57] \"1958\"   \"1959\"   \"1960\"   \"10938\"  \"10209\"  \"2114\"   \"2150\"   \"2152\"  \n [65] \"24147\"  \"2353\"   \"2354\"   \"8061\"   \"2355\"   \"2526\"   \"50486\"  \"1647\"  \n [73] \"4616\"   \"2643\"   \"2669\"   \"9945\"   \"1880\"   \"1839\"   \"3280\"   \"3383\"  \n [81] \"23308\"  \"3398\"   \"9592\"   \"8870\"   \"8870\"   \"8870\"   \"8870\"   \"8870\"  \n [89] \"8870\"   \"8870\"   \"51278\"  \"64135\"  \"3433\"   \"3460\"   \"3460\"   \"3593\"  \n [97] \"3601\"   \"3606\"   \"3552\"   \"3553\"   \"51561\"  \"3569\"   \"3572\"   \"3575\"  \n[105] \"3624\"   \"3659\"   \"8660\"   \"182\"    \"3725\"   \"3726\"   \"23135\"  \"7071\"  \n[113] \"10365\"  \"9314\"   \"1316\"   \"687\"    \"8942\"   \"3914\"   \"3949\"   \"3976\"  \n[121] \"9516\"   \"23764\"  \"5606\"   \"1326\"   \"4082\"   \"4170\"   \"9242\"   \"4084\"  \n[129] \"4609\"   \"10135\"  \"10725\"  \"4780\"   \"4783\"   \"4790\"   \"4791\"   \"4792\"  \n[137] \"4794\"   \"4814\"   \"3164\"   \"4929\"   \"8013\"   \"4973\"   \"24145\"  \"5142\"  \n[145] \"10611\"  \"5187\"   \"5209\"   \"22822\"  \"7262\"   \"7262\"   \"5328\"   \"5329\"  \n[153] \"5341\"   \"10769\"  \"8613\"   \"56937\"  \"10957\"  \"23645\"  \"5734\"   \"5743\"  \n[161] \"5791\"   \"5806\"   \"1827\"   \"5966\"   \"5970\"   \"5971\"   \"388\"    \"8767\"  \n[169] \"127544\" \"6303\"   \"6385\"   \"5055\"   \"5271\"   \"5054\"   \"6446\"   \"150094\"\n[177] \"9120\"   \"6515\"   \"11182\"  \"11182\"  \"4088\"   \"8303\"   \"9021\"   \"6648\"  \n[185] \"8877\"   \"80176\"  \"8878\"   \"8878\"   \"6776\"   \"10010\"  \"6890\"   \"6890\"  \n[193] \"6890\"   \"6890\"   \"6890\"   \"6890\"   \"6890\"   \"6890\"   \"7050\"   \"25976\" \n[201] \"7097\"   \"3371\"   \"7124\"   \"7124\"   \"7124\"   \"7124\"   \"7124\"   \"7124\"  \n[209] \"7124\"   \"7124\"   \"7127\"   \"7128\"   \"7130\"   \"25816\"  \"3604\"   \"8744\"  \n[217] \"10318\"  \"79155\"  \"7185\"   \"10221\"  \"9322\"   \"8848\"   \"7280\"   \"7422\"  \n[225] \"79693\"  \"65986\"  \"80149\"  \"7538\"  \n\n$M5891_HALLMARK_HYPOXIA\n  [1] \"57007\"  \"133\"    \"136\"    \"205\"    \"9590\"   \"226\"    \"229\"    \"230\"   \n  [9] \"272\"    \"51129\"  \"55139\"  \"302\"    \"467\"    \"538\"    \"126792\" \"124872\"\n [17] \"63827\"  \"596\"    \"633\"    \"8553\"   \"665\"    \"680\"    \"694\"    \"771\"   \n [25] \"839\"    \"857\"    \"284119\" \"112464\" \"3491\"   \"1490\"   \"8839\"   \"901\"   \n [33] \"1026\"   \"1027\"   \"1028\"   \"1028\"   \"9435\"   \"9469\"   \"10370\"  \"1289\"  \n [41] \"1356\"   \"1466\"   \"7852\"   \"1634\"   \"1649\"   \"54541\"  \"10570\"  \"1837\"  \n [49] \"1843\"   \"1907\"   \"1942\"   \"1944\"   \"1956\"   \"2023\"   \"2026\"   \"2027\"  \n [57] \"30001\"  \"54206\"  \"2113\"   \"2131\"   \"2152\"   \"26355\"  \"2203\"   \"2353\"  \n [65] \"2355\"   \"2309\"   \"2548\"   \"2584\"   \"2597\"   \"26330\"  \"2632\"   \"2645\"  \n [73] \"2651\"   \"2651\"   \"2745\"   \"2817\"   \"2719\"   \"2239\"   \"2821\"   \"2821\"  \n [81] \"9380\"   \"2997\"   \"3036\"   \"3069\"   \"3073\"   \"3098\"   \"3099\"   \"3162\"  \n [89] \"3219\"   \"9957\"   \"3309\"   \"3423\"   \"8870\"   \"8870\"   \"8870\"   \"8870\"  \n [97] \"8870\"   \"8870\"   \"8870\"   \"3484\"   \"3486\"   \"3569\"   \"10994\"  \"3623\"  \n[105] \"8660\"   \"3669\"   \"23210\"  \"3725\"   \"11015\"  \"55818\"  \"3798\"   \"1316\"  \n[113] \"8609\"   \"54800\"  \"3906\"   \"9215\"   \"3939\"   \"3939\"   \"3948\"   \"4015\"  \n[121] \"56925\"  \"23764\"  \"4214\"   \"4282\"   \"4282\"   \"4493\"   \"4502\"   \"4601\"  \n[129] \"4627\"   \"55577\"  \"1463\"   \"10397\"  \"3340\"   \"8509\"   \"23327\"  \"4783\"  \n[137] \"25819\"  \"2908\"   \"5033\"   \"8974\"   \"5066\"   \"5105\"   \"5155\"   \"5163\"  \n[145] \"5165\"   \"5209\"   \"5211\"   \"5214\"   \"5224\"   \"5228\"   \"5230\"   \"5236\"  \n[153] \"55276\"  \"5260\"   \"5292\"   \"5313\"   \"5313\"   \"5317\"   \"51316\"  \"5329\"  \n[161] \"123\"    \"10957\"  \"10891\"  \"8497\"   \"23645\"  \"5507\"   \"25824\"  \"5578\"  \n[169] \"5837\"   \"3516\"   \"6095\"   \"58528\"  \"6275\"   \"8819\"   \"949\"    \"6383\"  \n[177] \"9672\"   \"6385\"   \"8991\"   \"5054\"   \"6478\"   \"6576\"   \"6513\"   \"6515\"  \n[185] \"6518\"   \"2542\"   \"2542\"   \"2542\"   \"6533\"   \"8406\"   \"8987\"   \"6781\"  \n[193] \"8614\"   \"6820\"   \"26136\"  \"7043\"   \"7045\"   \"7052\"   \"25976\"  \"8277\"  \n[201] \"55076\"  \"7128\"   \"7162\"   \"7162\"   \"7163\"   \"7167\"   \"8459\"   \"7360\"  \n[209] \"7422\"   \"7428\"   \"7436\"   \"26118\"  \"7511\"   \"7538\"   \"23036\" \n\n\nNote #2: The idMap function can be used to map gene sets from NCBI Entrez Gene IDs to other common gene ID types such as ENSEMBL gene IDs or HGNC symbols.\\ For example, to map the gene sets from Entrez Gene IDs to gene symbols:\n\nhsa.gs.sym <- idMap(hsa.gs, org = \"hsa\", from = \"ENTREZID\", to = \"SYMBOL\")\nhsa.gs.sym[1:2]\n\n$hsa05416_Viral_myocarditis\n [1] \"ABL1\"     \"ABL2\"     \"ACTB\"     \"ACTG1\"    \"BID\"      \"CASP3\"   \n [7] \"CASP8\"    \"CASP9\"    \"CAV1\"     \"CCND1\"    \"CD28\"     \"CD40\"    \n[13] \"CD40LG\"   \"CD55\"     \"CD80\"     \"CD86\"     \"CXADR\"    \"CYCS\"    \n[19] \"DAG1\"     \"DMD\"      \"EIF4G1\"   \"EIF4G2\"   \"EIF4G3\"   \"FYN\"     \n[25] \"HLA-A\"    \"HLA-B\"    \"HLA-C\"    \"HLA-DMA\"  \"HLA-DMB\"  \"HLA-DOA\" \n[31] \"HLA-DOB\"  \"HLA-DPA1\" \"HLA-DPB1\" \"HLA-DQA1\" \"HLA-DQA2\" \"HLA-DQB1\"\n[37] \"HLA-DRA\"  \"HLA-DRB1\" \"HLA-DRB3\" \"HLA-DRB4\" \"HLA-DRB5\" \"HLA-E\"   \n[43] \"HLA-F\"    \"HLA-G\"    \"ICAM1\"    \"ITGAL\"    \"ITGB2\"    \"LAMA2\"   \n[49] \"MYH6\"     \"MYH7\"     \"PRF1\"     \"RAC1\"     \"RAC2\"     \"RAC3\"    \n[55] \"SGCA\"     \"SGCB\"     \"SGCD\"     \"SGCG\"    \n\n$`hsa04622_RIG-I-like_receptor_signaling_pathway`\n [1] \"ATG12\"  \"ATG5\"   \"AZI2\"   \"CASP10\" \"CASP8\"  \"CHUK\"   \"CXCL10\" \"CXCL8\" \n [9] \"CYLD\"   \"DDX3X\"  \"DHX58\"  \"FADD\"   \"IFIH1\"  \"IFNA1\"  \"IFNA10\" \"IFNA13\"\n[17] \"IFNA14\" \"IFNA16\" \"IFNA17\" \"IFNA2\"  \"IFNA21\" \"IFNA4\"  \"IFNA5\"  \"IFNA6\" \n[25] \"IFNA7\"  \"IFNA8\"  \"IFNB1\"  \"IFNE\"   \"IFNK\"   \"IFNW1\"  \"IKBKB\"  \"IKBKE\" \n[33] \"IKBKG\"  \"IL12A\"  \"IL12B\"  \"IRF3\"   \"IRF7\"   \"ISG15\"  \"MAP3K1\" \"MAP3K7\"\n[41] \"MAPK10\" \"MAPK11\" \"MAPK12\" \"MAPK13\" \"MAPK14\" \"MAPK8\"  \"MAPK9\"  \"MAVS\"  \n[49] \"NFKB1\"  \"NFKBIA\" \"NFKBIB\" \"NLRX1\"  \"OTUD5\"  \"PIN1\"   \"RELA\"   \"RIGI\"  \n[57] \"RIPK1\"  \"RNF125\" \"SIKE1\"  \"STING1\" \"TANK\"   \"TBK1\"   \"TBKBP1\" \"TKFC\"  \n[65] \"TNF\"    \"TRADD\"  \"TRAF2\"  \"TRAF3\"  \"TRAF6\"  \"TRIM25\""
  },
  {
    "objectID": "session5/enrichment_analysis.html#gokegg-overrepresentation-analysis",
    "href": "session5/enrichment_analysis.html#gokegg-overrepresentation-analysis",
    "title": "28  Functional enrichment analysis",
    "section": "28.6 GO/KEGG overrepresentation analysis",
    "text": "28.6 GO/KEGG overrepresentation analysis\nA variety of gene set analysis methods have been proposed Khatri et al., 2012. The most basic, yet frequently used, method is the over-representation analysis (ORA) with gene sets defined according to GO or KEGG.\n\nOverrepresentation analysis (ORA), testing whether a gene set contains disproportional many genes of significant expression change. ORA tests the overlap between DE genes and genes in a gene set based on the hypergeometric distribution.\n\nAs ORA works on the list of DE genes and not the actual expression values, it can be straightforward applied to RNA-seq data. However, as the gene sets here contain NCBI Entrez gene IDs and the airway dataset contains ENSEMBL gene ids, we first map the airway dataset to Entrez IDs.\n\nairSE <- idMap(airSE, org = \"hsa\", from = \"ENSEMBL\", to = \"ENTREZID\")\n\nEncountered 99 from.IDs with >1 corresponding to.ID\n\n\n(the first to.ID was chosen for each of them)\n\n\nExcluded 1967 from.IDs without a corresponding to.ID\n\n\nEncountered 9 to.IDs with >1 from.ID (the first from.ID was chosen for each of them)\n\n\nMapped from.IDs have been added to the rowData column ENSEMBL\n\n\n\nora.air <- sbea(method = \"ora\", se = airSE, gs = go.gs, perm = 0)\ngsRanking(ora.air)\n\nDataFrame with 415 rows and 4 columns\n                  GENE.SET  NR.GENES NR.SIG.GENES      PVAL\n               <character> <numeric>    <numeric> <numeric>\n1   GO:0030198_extracell..       108           70  4.99e-10\n2   GO:0007155_cell_adhe..       296          148  1.72e-07\n3   GO:0030336_negative_..       106           62  1.12e-06\n4   GO:0010628_positive_..       291          142  1.80e-06\n5   GO:0009410_response_..       165           88  1.93e-06\n...                    ...       ...          ...       ...\n411 GO:0008542_visual_le..        31           16    0.0484\n412 GO:0010634_positive_..        31           16    0.0484\n413 GO:0019216_regulatio..        31           16    0.0484\n414 GO:0030041_actin_fil..        31           16    0.0484\n415 GO:0051781_positive_..        31           16    0.0484\n\n\nSuch a ranked list is the standard output of most existing enrichment tools. Using the eaBrowse function creates a HTML summary from which each gene set can be inspected in more detail.\n\neaBrowse(ora.air, nr.show = 5)\n\nWarning: package 'XML' was built under R version 4.2.2\n\n\nThe resulting summary page includes for each significant gene set\n\na gene report, which lists all genes of a set along with fold change and DE \\(p\\)-value (click on links in column NR.GENES),\ninteractive overview plots such as heatmap and volcano plot (column SET.VIEW, supports mouse-over and click-on),\nfor KEGG pathways: highlighting of differentially expressed genes on the pathway maps (column PATH.VIEW, supports mouse-over and click-on).\n\nNote #1: Young et al., 2010, have reported biased results for ORA on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts. The goseq package and limma::goana implement possibilities to adjust ORA for gene length and abundance bias.\nNote #2: Independent of the expression data type under investigation, overlap between gene sets can result in redundant findings. This is well-documented for GO (parent-child structure, Rhee et al., 2008) and KEGG (pathway overlap/crosstalk, Donato et al., 2013). The topGO package (explicitly designed for GO) and mgsa (applicable to arbitrary gene set definitions) implement modifications of ORA to account for such redundancies.\nExercise\nCarry out a GO overrepresentation analysis for the airSE based on the molecular function (MF) ontology. How many significant gene sets do you observe in each case?"
  },
  {
    "objectID": "session5/enrichment_analysis.html#functional-class-scoring-permutation-testing",
    "href": "session5/enrichment_analysis.html#functional-class-scoring-permutation-testing",
    "title": "28  Functional enrichment analysis",
    "section": "28.7 Functional class scoring & permutation testing",
    "text": "28.7 Functional class scoring & permutation testing\nA major limitation of ORA is that it restricts analysis to DE genes, excluding genes not satisfying the chosen significance threshold (typically the vast majority).\nThis is resolved by gene set enrichment analysis (GSEA), which scores the tendency of gene set members to appear rather at the top or bottom of the ranked list of all measured genes Subramanian et al., 2005.\nAs GSEA’s permutation procedure involves re-computation of per-gene DE statistics, adaptations are necessary for RNA-seq. When analyzing RNA-seq datasets with expression values given as logTPMs (or logRPKMs / logFPKMs), the available set-based enrichment methods can be applied as for microarray data. However, when given raw read counts as for the airway dataset, we recommend to first apply a variance-stabilizing transformation such as voom to arrive at library-size normalized logCPMs.\n\nairSE <- normalize(airSE, norm.method = \"vst\")\ngsea.air <- sbea(method = \"gsea\", se = airSE, gs = go.gs)  \n\nA selection of additional methods is also available:\n\nsbeaMethods()\n\n [1] \"ora\"        \"safe\"       \"gsea\"       \"gsa\"        \"padog\"     \n [6] \"globaltest\" \"roast\"      \"camera\"     \"gsva\"       \"samgs\"     \n[11] \"ebm\"        \"mgsa\""
  },
  {
    "objectID": "session5/enrichment_analysis.html#further-reading",
    "href": "session5/enrichment_analysis.html#further-reading",
    "title": "28  Functional enrichment analysis",
    "section": "28.8 Further Reading",
    "text": "28.8 Further Reading\nAlthough gene set enrichment methods have been primarily developed and applied on transcriptomic data, they have recently been modified, extended and applied also in other fields of genomic and biomedical research. This includes novel approaches for functional enrichment analysis of proteomic and metabolomic data as well as genomic regions and disease phenotypes, Lavallee and Yates, 2016, Chagoyen et al., 2016, McLean et al., 2010, Ried et al., 2012.\nThe statistical significance of the enrichment score (ES) of a gene set is assessed via sample permutation, i.e. (1) sample labels (= group assignment) are shuffled, (2) per-gene DE statistics are recomputed, and (3) the enrichment score is recomputed. Repeating this procedure many times allows to determine the empirical distribution of the enrichment score and to compare the observed enrichment score against it. Here, we carry out GSEA with 1000 permutations.\nGene set enrichment analysis (GSEA), testing whether genes of a gene set accumulate at the top or bottom of the full gene vector ordered by direction and magnitude of expression change Subramanian et al., 2005\nHowever, the term gene set enrichment analysis nowadays subsumes a general strategy implemented by a wide range of methods Huang et al., 2009. Those methods have in common the same goal, although approach and statistical model can vary substantially Goeman and Buehlmann, 2007, Khatri et al., 2012.\nTo better distinguish from the specific method, some authors use the term gene set analysis to denote the general strategy. However, there is also a specific method from Efron and Tibshirani, 2007 of this name.\n\n28.8.1 Network-based enrichment analysis\nHaving found gene sets that show enrichment for differential expression, we are now interested whether these findings can be supported by known regulatory interactions.\nFor example, we want to know whether transcription factors and their target genes are expressed in accordance to the connecting regulations (activation/inhibition). Such information is usually given in a gene regulatory network derived from specific experiments or compiled from the literature (Geistlinger et al., 2013 for an example).\nThere are well-studied processes and organisms for which comprehensive and well-annotated regulatory networks are available, e.g. the RegulonDB for E. coli and Yeastract for S. cerevisiae.\nHowever, there are also cases where such a network is missing or at least incomplete. A basic workaround is to compile a network from regulations in pathway databases such as KEGG.\n\nhsa.grn <- compileGRN(org = \"hsa\", db = \"kegg\")\nhead(hsa.grn)\n\n     FROM    TO          TYPE\n[1,] \"10000\" \"100132074\" \"-\" \n[2,] \"10000\" \"1026\"      \"-\" \n[3,] \"10000\" \"1026\"      \"+\" \n[4,] \"10000\" \"1027\"      \"-\" \n[5,] \"10000\" \"10488\"     \"+\" \n[6,] \"10000\" \"107\"       \"+\" \n\n\nSignaling pathway impact analysis (SPIA) is a network-based enrichment analysis method, which is explicitly designed for KEGG signaling pathways Tarca et al., 2009. The method evaluates whether expression changes are propagated across the pathway topology in combination with ORA.\n\nspia.air <- nbea(method = \"spia\", se = airSE, gs = kegg.gs, grn = hsa.grn)\ngsRanking(spia.air)\n\nMore generally applicable is gene graph enrichment analysis (GGEA), which evaluates consistency of interactions in a given gene regulatory network with the observed expression data Geistlinger et al., 2011.\n\nggea.air <- nbea(method = \"ggea\", se = airSE, gs = kegg.gs, grn = hsa.grn)\ngsRanking(ggea.air)\n\n\nnbeaMethods()\n\nNote #1: As network-based enrichment methods typically do not involve sample permutation but rather network permutation, thus avoiding DE re-computation, they can likewise be applied to RNA-seq data.\nNote #2: Given the various enrichment methods with individual benefits and limitations, combining multiple methods can be beneficial, e.g. combined application of a set-based and a network-based method. This has been shown to filter out spurious hits of individual methods and to reduce the outcome to gene sets accumulating evidence from different methods Geistlinger et al., 2016, Alhamdoosh et al., 2017.\nThe function combResults implements the straightforward combination of results, thereby facilitating seamless comparison of results across methods. For demonstration, we use the ORA and GSEA results for the ALL dataset from the previous section:\n\nres.list <- list(ora.air, roast.air)\ncomb.res <- combResults(res.list)\ngsRanking(comb.res)\n\n\nThis lesson was adapted from materials created by Ludwig Geistlinger"
  },
  {
    "objectID": "preWork/pre_work.html#install-r-and-rstudio",
    "href": "preWork/pre_work.html#install-r-and-rstudio",
    "title": "Pre-Work",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nBefore the first session, please install R and RStudio following the instructions Chapter 1.\nIf you already have R and RStudio installed, make sure that you have the latest versions installed (you can do this by simply following the installation instructions). While this will likely not cause an issue for the first few sessions, it will in later sessions when we use more advanced packages and software.\nIf you encounter issues installing R or RStudio, please reach out to christopher_magnano@hms.harvard.edu or one of the TAs. If we are unable to resolve your issue via email, we ask that you come 30 minutes early to the first session."
  },
  {
    "objectID": "preWork/pre_work.html#familiarize-yourself-with-rstudio",
    "href": "preWork/pre_work.html#familiarize-yourself-with-rstudio",
    "title": "Pre-Work",
    "section": "Familiarize yourself with RStudio",
    "text": "Familiarize yourself with RStudio\nIf you have never used RStudio or are completely new to programming, please review Chapter 2. This material will introduce you to the RStudio interface and how to assign values to variables in R."
  },
  {
    "objectID": "preWork/install.html#mac-users",
    "href": "preWork/install.html#mac-users",
    "title": "1  Installing R and RStudio",
    "section": "1.1 Mac Users",
    "text": "1.1 Mac Users\n\n1.1.1 To install R\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.1.2 To install RStudio\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder."
  },
  {
    "objectID": "preWork/install.html#windows-users",
    "href": "preWork/install.html#windows-users",
    "title": "1  Installing R and RStudio",
    "section": "1.2 Windows Users",
    "text": "1.2 Windows Users\n\n1.2.1 To install R\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.2.2 To install RStudio\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "preWork/install.html#reference",
    "href": "preWork/install.html#reference",
    "title": "1  Installing R and RStudio",
    "section": "1.3 Reference",
    "text": "1.3 Reference\nInstructions adapted from guide developed by HMS Research computing"
  },
  {
    "objectID": "preWork/rstudio.html#learning-objectives",
    "href": "preWork/rstudio.html#learning-objectives",
    "title": "2  Introduction to RStudio",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nDescribe what R and RStudio are.\nInteract with R using RStudio.\nFamiliarize various components of RStudio."
  },
  {
    "objectID": "preWork/rstudio.html#what-is-rstudio",
    "href": "preWork/rstudio.html#what-is-rstudio",
    "title": "2  Introduction to RStudio",
    "section": "2.2 What is RStudio?",
    "text": "2.2 What is RStudio?\nRStudio is freely available open-source Integrated Development Environment (IDE). RStudio provides an environment with many features to make using R easier and is a great alternative to working on R in the terminal.\n\nGraphical user interface, not just a command prompt\nGreat learning tool\nFree for academic use\nPlatform agnostic\nOpen source"
  },
  {
    "objectID": "preWork/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "href": "preWork/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "title": "2  Introduction to RStudio",
    "section": "2.3 Creating a new project directory in RStudio",
    "text": "2.3 Creating a new project directory in RStudio\nLet’s create a new project directory for Systems Immunology.\n\nOpen RStudio\nGo to the File menu and select New Project.\nIn the New Project window, choose New Directory. Then, choose New Project. Name your new directory whatever you want and then “Create the project as subdirectory of:” the Desktop (or location of your choice).\nClick on Create Project.\nAfter your project is completed, if the project does not automatically open in RStudio, then go to the File menu, select Open Project, and choose [your project name].Rproj.\nWhen RStudio opens, you will see three panels in the window.\nGo to the File menu and select New File, and select R Script. The RStudio interface should now look like the screenshot below.\n\n\n\n\nRStudio interface\n\n\n\n2.3.1 What is a project in RStudio?\nIt is simply a directory that contains everything related your analyses for a specific project. RStudio projects are useful when you are working on context- specific analyses and you wish to keep them separate. When creating a project in RStudio you associate it with a working directory of your choice (either an existing one, or a new one). A . RProj file is created within that directory and that keeps track of your command history and variables in the environment. The . RProj file can be used to open the project in its current state but at a later date.\nWhen a project is (re) opened within RStudio the following actions are taken:\n\nA new R session (process) is started\nThe .RData file in the project’s main directory is loaded, populating the environment with any objects that were present when the project was closed.\nThe .Rhistory file in the project’s main directory is loaded into the RStudio History pane (and used for Console Up/Down arrow command history).\nThe current working directory is set to the project directory.\nPreviously edited source documents are restored into editor tabs\nOther RStudio settings (e.g. active tabs, splitter positions, etc.) are restored to where they were the last time the project was closed.\n\nInformation adapted from RStudio Support Site"
  },
  {
    "objectID": "preWork/rstudio.html#rstudio-interface",
    "href": "preWork/rstudio.html#rstudio-interface",
    "title": "2  Introduction to RStudio",
    "section": "2.4 RStudio Interface",
    "text": "2.4 RStudio Interface\nThe RStudio interface has four main panels:\n\nConsole: where you can type commands and see output. The console is all you would see if you ran R in the command line without RStudio.\nScript editor: where you can type out commands and save to file. You can also submit the commands to run in the console.\nEnvironment/History: environment shows all active objects and history keeps track of all commands run in console\nFiles/Plots/Packages/Help"
  },
  {
    "objectID": "preWork/rstudio.html#organizing-your-working-directory-setting-up",
    "href": "preWork/rstudio.html#organizing-your-working-directory-setting-up",
    "title": "2  Introduction to RStudio",
    "section": "2.5 Organizing your working directory & setting up",
    "text": "2.5 Organizing your working directory & setting up\n\n2.5.1 Viewing your working directory\nBefore we organize our working directory, let’s check to see where our current working directory is located by typing into the console:\n\ngetwd()\n\nYour working directory should be the Intro-to-R folder constructed when you created the project. The working directory is where RStudio will automatically look for any files you bring in and where it will automatically save any files you create, unless otherwise specified.\nYou can visualize your working directory by selecting the Files tab from the Files/Plots/Packages/Help window.\n\n\n\nViewing your working directory\n\n\nIf you wanted to choose a different directory to be your working directory, you could navigate to a different folder in the Files tab, then, click on the More dropdown menu and select Set As Working Directory.\n\n\n\nSetting your working directory\n\n\n\n\n2.5.2 Structuring your working directory\nTo organize your working directory for a particular analysis, you typically want to separate the original data (raw data) from intermediate datasets. For instance, you may want to create a data/ directory within your working directory that stores the raw data, and have a results/ directory for intermediate datasets and a figures/ directory for the plots you will generate.\nLet’s create these three directories within your working directory by clicking on New Folder within the Files tab.\n\n\n\nStructuring your working directory\n\n\nWhen finished, your working directory should look like:\n\n\n\nYour organized working directory\n\n\n\n\n2.5.3 Setting up\nThis is more of a housekeeping task. We will be writing long lines of code in our script editor and want to make sure that the lines “wrap” and you don’t have to scroll back and forth to look at your long line of code.\nClick on “Tools” at the top of your RStudio screen and click on “Global Options” in the pull down menu.\n\n\n\noptions\n\n\nOn the left, select “Code” and put a check against “Soft-wrap R source files”. Make sure you click the “Apply” button at the bottom of the Window before saying “OK”.\n\n\n\nwrap_options"
  },
  {
    "objectID": "preWork/rstudio.html#interacting-with-r",
    "href": "preWork/rstudio.html#interacting-with-r",
    "title": "2  Introduction to RStudio",
    "section": "2.6 Interacting with R",
    "text": "2.6 Interacting with R\nNow that we have our interface and directory structure set up, let’s start playing with R! There are two main ways of interacting with R in RStudio: using the console or by using script editor (plain text files that contain your code).\n\n2.6.1 Console window\nThe console window (in RStudio, the bottom left panel) is the place where R is waiting for you to tell it what to do, and where it will show the results of a command. You can type commands directly into the console, but they will be forgotten when you close the session.\n\n\n\nRunning in the console\n\n\n\n\n2.6.2 Script editor\nBest practice is to enter the commands in the script editor, and save the script. You are encouraged to comment liberally to describe the commands you are running using #. This way, you have a complete record of what you did, you can easily show others how you did it and you can do it again later on if needed.\nThe Rstudio script editor allows you to ‘send’ the current line or the currently highlighted text to the R console by clicking on the Run button in the upper-right hand corner of the script editor. Alternatively, you can run by simply pressing the Ctrl and Enter keys at the same time as a shortcut.\nNow let’s try entering commands to the script editor and using the comments character # to add descriptions and highlighting the text to run:\n\n    # Session 1\n    # Feb 3, 2023\n\n    # Interacting with R\n    \n    # I am adding 3 and 5. \n    3+5\n\n\n\n\nRunning in the script editor\n\n\nYou should see the command run in the console and output the result.\n\n\n\nScript editor output\n\n\nWhat happens if we do that same command without the comment symbol #? Re-run the command after removing the # sign in the front:\n\nI am adding 3 and 5. R is fun!\n3+5\n\nNow R is trying to run that sentence as a command, and it doesn’t work. We get an error in the console “Error: unexpected symbol in”I am” means that the R interpreter did not know what to do with that command.”\n\n\n2.6.3 Console command prompt\nInterpreting the command prompt can help understand when R is ready to accept commands. Below lists the different states of the command prompt and how you can exit a command:\nConsole is ready to accept commands: >.\nIf R is ready to accept commands, the R console shows a > prompt.\nWhen the console receives a command (by directly typing into the console or running from the script editor (Ctrl-Enter), R will try to execute it.\nAfter running, the console will show the results and come back with a new > prompt to wait for new commands.\nConsole is waiting for you to enter more data: +.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. Often this can be due to you having not ‘closed’ a parenthesis or quotation.\nEscaping a command and getting a new prompt: esc\nIf you’re in Rstudio and you can’t figure out why your command isn’t running, you can click inside the console window and press esc to escape the command and bring back a new prompt >.\n\n\n2.6.4 Keyboard shortcuts in RStudio\nIn addition to some of the shortcuts described earlier in this lesson, we have listed a few more that can be helpful as you work in RStudio.\n\n\n\n\n\n\n\nkey\naction\n\n\n\n\nCtrl+Enter\nRun command from script editor in console\n\n\nESC\nEscape the current command to return to the command prompt\n\n\nCtrl+1\nMove cursor from console to script editor\n\n\nCtrl+2\nMove cursor from script editor to console\n\n\nTab\nUse this key to complete a file path\n\n\nCtrl+Shift+C\nComment the block of highlighted text"
  },
  {
    "objectID": "preWork/rstudio.html#r-syntax",
    "href": "preWork/rstudio.html#r-syntax",
    "title": "2  Introduction to RStudio",
    "section": "2.7 R syntax",
    "text": "2.7 R syntax\nNow that we know how to talk with R via the script editor or the console, we want to use R for something more than adding numbers. To do this, we need to know more about the R syntax.\nThe main “parts of speech” in R (syntax) include:\n\nthe comments # and how they are used to document function and its content\nvariables and functions\nthe assignment operator <-\nthe = for arguments in functions\n\nNOTE: indentation and consistency in spacing is used to improve clarity and legibility\nWe will go through each of these “parts of speech” in more detail, starting with the assignment operator."
  },
  {
    "objectID": "preWork/rstudio.html#assignment-operator",
    "href": "preWork/rstudio.html#assignment-operator",
    "title": "2  Introduction to RStudio",
    "section": "2.8 Assignment operator",
    "text": "2.8 Assignment operator\nTo do useful and interesting things in R, we need to assign values to variables using the assignment operator, <-. For example, we can use the assignment operator to assign the value of 3 to x by executing:\n\nx <- 3\n\nThe assignment operator (<-) assigns values on the right to variables on the left.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key, on Mac type option + -) will write <- in a single keystroke."
  },
  {
    "objectID": "preWork/rstudio.html#variables",
    "href": "preWork/rstudio.html#variables",
    "title": "2  Introduction to RStudio",
    "section": "2.9 Variables",
    "text": "2.9 Variables\nA variable is a symbolic name for (or reference to) information. Variables in computer programming are analogous to “buckets”, where information can be maintained and referenced. On the outside of the bucket is a name. When referring to the bucket, we use the name of the bucket, not the data stored in the bucket.\nIn the example above, we created a variable or a ‘bucket’ called x. Inside we put a value, 3.\nLet’s create another variable called y and give it a value of 5.\n\ny <- 5\n\nWhen assigning a value to an variable, R does not print anything to the console. You can force to print the value by using parentheses or by typing the variable name.\n\ny\n\nYou can also view information on the variable by looking in your Environment window in the upper right-hand corner of the RStudio interface.\n\n\n\nViewing your environment\n\n\nNow we can reference these buckets by name to perform mathematical operations on the values contained within. What do you get in the console for the following operation:\n\nx + y\n\nTry assigning the results of this operation to another variable called number.\n\nnumber <- x + y\n\n\n2.9.1 Tips on variable names\nVariables can be given almost any name, such as x, current_temperature, or subject_id. However, there are some rules / suggestions you should keep in mind:\n\nMake your names explicit and not too long.\nAvoid names starting with a number (2x is not valid but x2 is)\nAvoid names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data) as variable names. When in doubt check the help to see if the name is already in use.\nAvoid dots (.) within a variable name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them.\nUse nouns for object names and verbs for function names\nKeep in mind that R is case sensitive (e.g., genome_length is different from Genome_length)\nBe consistent with the styling of your code (where you put spaces, how you name variable, etc.). In R, two popular style guides are Hadley Wickham’s style guide and Google’s."
  },
  {
    "objectID": "preWork/rstudio.html#best-practices",
    "href": "preWork/rstudio.html#best-practices",
    "title": "2  Introduction to RStudio",
    "section": "2.10 Best practices",
    "text": "2.10 Best practices\nBefore we move on to more complex concepts and getting familiar with the language, we want to point out a few things about best practices when working with R which will help you stay organized in the long run:\n\nCode and workflow are more reproducible if we can document everything that we do. Our end goal is not just to “do stuff”, but to do it in a way that anyone can easily and exactly replicate our workflow and results. All code should be written in the script editor and saved to file, rather than working in the console.\nThe R console should be mainly used to inspect objects, test a function or get help.\nUse # signs to comment. Comment liberally in your R scripts. This will help future you and other collaborators know what each line of code (or code block) was meant to do. Anything to the right of a # is ignored by R. A shortcut for this is Ctrl+Shift+C if you want to comment an entire chunk of text.\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session1/session1.html#learning-objectives",
    "href": "session1/session1.html#learning-objectives",
    "title": "Session 1: Data Types and Hypothesis Tests",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExplore how probability distributions inform the mathematical form of statistical tests.\nExplore different types of hypothesis tests and when they should be used.\nApply hypothesis tests commonly used in biological systems analyses.\nInstall and manage packages from CRAN and Bioconductor.\nIdentify and use different data types in R."
  },
  {
    "objectID": "session1/session1.html#note",
    "href": "session1/session1.html#note",
    "title": "Session 1: Data Types and Hypothesis Tests",
    "section": "Note",
    "text": "Note\nIf you haven’t been to get R/RStudio running on your laptop, you can use this collab notebook today."
  },
  {
    "objectID": "session1/types.html#basic-data-types",
    "href": "session1/types.html#basic-data-types",
    "title": "3  R Syntax and Data Structures",
    "section": "3.1 Basic Data Types",
    "text": "3.1 Basic Data Types\nVariables can contain values of specific types within R. The six data types that R uses include:\n\n\"numeric\" for any numerical value, including whole numbers and decimals. This is the most common data type for performing mathematical operations.\n\"character\" for text values, denoted by using quotes (““) around value. For instance, while 5 is a numeric value, if you were to put quotation marks around it, it would turn into a character value, and you could no longer use it for mathematical operations. Single or double quotes both work, as long as the same type is used at the beginning and end of the character value.\n\"integer\" for whole numbers (e.g., 2L, the L indicates to R that it’s an integer). It behaves similar to the numeric data type for most tasks or functions; however, it takes up less storage space than numeric data, so often tools will output integers if the data is known to be comprised of whole numbers. Just know that integers behave similarly to numeric values. If you wanted to create your own, you could do so by providing the whole number, followed by an upper-case L.\n\"logical\" for TRUE and FALSE (the Boolean data type). The logical data type can be specified using four values, TRUE in all capital letters, FALSE in all capital letters, a single capital T or a single capital F.\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1+4i) and that’s all we’re going to say about them\n\"raw\" that we won’t discuss further\n\nThe table below provides examples of each of the commonly used data types:\n\n\n\nData Type\nExamples\n\n\n\n\nNumeric:\n1, 1.5, 20, pi\n\n\nCharacter:\n“anytext”, “5”, “TRUE”\n\n\nInteger:\n2L, 500L, -17L\n\n\nLogical:\nTRUE, FALSE, T, F\n\n\n\nThe type of data will determine what you can do with it. For example, if you want to perform mathematical operations, then your data type cannot be character or logical. Whereas if you want to search for a word or pattern in your data, then you data should be of the character data type. The task or function being performed on the data will determine what type of data can be used."
  },
  {
    "objectID": "session1/types.html#data-structures",
    "href": "session1/types.html#data-structures",
    "title": "3  R Syntax and Data Structures",
    "section": "3.2 Data Structures",
    "text": "3.2 Data Structures\nWe know that variables are like buckets, and so far we have seen that bucket filled with a single value. Even when number was created, the result of the mathematical operation was a single value. Variables can store more than just a single value, they can store a multitude of different data structures. These include, but are not limited to, vectors (c), factors (factor), matrices (matrix), data frames (data.frame) and lists (list).\n\n3.2.1 Vectors\nA vector is the most common and basic data structure in R, and is pretty much the workhorse of R. It’s basically just a collection of values, mainly either numbers,\n\n\n\nnumeric vector\n\n\nor characters,\n\n\n\ncharacter vector\n\n\nor logical values,\n\n\n\nlogical vector\n\n\nNote that all values in a vector must be of the same data type. If you try to create a vector with more than a single data type, R will try to coerce it into a single data type.\nFor example, if you were to try to create the following vector:\n\n\n\nmixed vector\n\n\nR will coerce it into:\n\n\n\ntransformed vector\n\n\nThe analogy for a vector is that your bucket now has different compartments; these compartments in a vector are called elements.\nEach element contains a single value, and there is no limit to how many elements you can have. A vector is assigned to a single variable, because regardless of how many elements it contains, in the end it is still a single entity (bucket).\nLet’s create a vector of genome lengths and assign it to a variable called glengths.\nEach element of this vector contains a single numeric value, and three values will be combined together into a vector using c() (the combine function). All of the values are put within the parentheses and separated with a comma.\n\n# Create a numeric vector and store the vector as a variable called 'glengths'\nglengths <- c(4.6, 3000, 50000)\nglengths\n\n[1]     4.6  3000.0 50000.0\n\n\nNote your environment shows the glengths variable is numeric (num) and tells you the glengths vector starts at element 1 and ends at element 3 (i.e. your vector contains 3 values) as denoted by the [1:3].\nA vector can also contain characters. Create another vector called species with three elements, where each element corresponds with the genome sizes vector (in Mb).\n\n# Create a character vector and store the vector as a variable called 'species'\nspecies <- c(\"ecoli\", \"human\", \"corn\")\nspecies\n\n[1] \"ecoli\" \"human\" \"corn\" \n\n\nWhat do you think would happen if we forgot to put quotations around one of the values? Let’s test it out with corn.\n\n# Forget to put quotes around corn\nspecies <- c(\"ecoli\", \"human\", corn)\n\nNote that RStudio is quite helpful in color-coding the various data types. We can see that our numeric values are blue, the character values are green, and if we forget to surround corn with quotes, it’s black. What does this mean? Let’s try to run this code.\nWhen we try to run this code we get an error specifying that object ‘corn’ is not found. What this means is that R is looking for an object or variable in my Environment called ‘corn’, and when it doesn’t find it, it returns an error. If we had a character vector called ‘corn’ in our Environment, then it would combine the contents of the ‘corn’ vector with the values “ecoli” and “human”.\nSince we only want to add the value “corn” to our vector, we need to re-run the code with the quotation marks surrounding corn. A quick way to add quotes to both ends of a word in RStudio is to highlight the word, then press the quote key.\n\n# Create a character vector and store the vector as a variable called 'species'\nspecies <- c(\"ecoli\", \"human\", \"corn\")\n\n\nExercise\nTry to create a vector of numeric and character values by combining the two vectors that we just created (glengths and species). Assign this combined vector to a new variable called combined. Hint: you will need to use the combine c() function to do this. Print the combined vector in the console, what looks different compared to the original vectors?\n\n\n3.2.2 Factors\nA factor is a special type of vector that is used to store categorical data. Each unique category is referred to as a factor level (i.e. category = level). Factors are built on top of integer vectors such that each factor level is assigned an integer value, creating value-label pairs.\nFor instance, if we have four animals and the first animal is female, the second and third are male, and the fourth is female, we could create a factor that appears like a vector, but has integer values stored under-the-hood. The integer value assigned is a one for females and a two for males. The numbers are assigned in alphabetical order, so because the f- in females comes before the m- in males in the alphabet, females get assigned a one and males a two. In later lessons we will show you how you could change these assignments.\n\n\n\nfactors\n\n\nLet’s create a factor vector and explore a bit more. We’ll start by creating a character vector describing three different levels of expression. Perhaps the first value represents expression in mouse1, the second value represents expression in mouse2, and so on and so forth:\n\n# Create a character vector and store the vector as a variable called 'expression'\nexpression <- c(\"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\", \"high\")\n\nNow we can convert this character vector into a factor using the factor() function:\n\n# Turn 'expression' vector into a factor\nexpression <- factor(expression)\n\nSo, what exactly happened when we applied the factor() function?\n\n\n\nfactor_new\n\n\nThe expression vector is categorical, in that all the values in the vector belong to a set of categories; in this case, the categories are low, medium, and high. By turning the expression vector into a factor, the categories are assigned integers alphabetically, with high=1, low=2, medium=3. This in effect assigns the different factor levels. You can view the newly created factor variable and the levels in the Environment window.\n\n\n\nFactor variables in environment\n\n\nSo now that we have an idea of what factors are, when would you ever want to use them?\nFactors are extremely valuable for many operations often performed in R. For instance, factors can give order to values with no intrinsic order. In the previous ‘expression’ vector, if I wanted the low category to be less than the medium category, then we could do this using factors. Also, factors are necessary for many statistical methods. For example, descriptive statistics can be obtained for character vectors if you have the categorical information stored as a factor. Also, if you want to denote which category is your base level for a statistical comparison, then you would need to have your category variable stored as a factor with the base level assigned to 1. Anytime that it is helpful to have the categories thought of as groups in an analysis, the factor function makes this possible. For instance, if you want to color your plots by treatment type, then you would need the treatment variable to be a factor.\nExercises\nLet’s say that in our experimental analyses, we are working with three different sets of cells: normal, cells knocked out for geneA (a very exciting gene), and cells overexpressing geneA. We have three replicates for each celltype.\n\nCreate a vector named samplegroup with nine elements: 3 control (“CTL”) values, 3 knock-out (“KO”) values, and 3 over-expressing (“OE”) values.\nTurn samplegroup into a factor data structure.\n\n\n\n3.2.3 Matrix\nA matrix in R is a collection of vectors of same length and identical datatype. Vectors can be combined as columns in the matrix or by row, to create a 2-dimensional structure.\n\n\n\nmatrix\n\n\nMatrices are used commonly as part of the mathematical machinery of statistics. They are usually of numeric datatype and used in computational algorithms to serve as a checkpoint. For example, if input data is not of identical data type (numeric, character, etc.), the matrix() function will throw an error and stop any downstream code execution.\n\n\n3.2.4 Data Frame\nA data.frame is the de facto data structure for most tabular data and what we use for statistics and plotting. A data.frame is similar to a matrix in that it’s a collection of vectors of the same length and each vector represents a column. However, in a dataframe each vector can be of a different data type (e.g., characters, integers, factors). In the data frame pictured below, the first column is character, the second column is numeric, the third is character, and the fourth is logical.\n\n\n\ndataframe\n\n\nA data frame is the most common way of storing data in R, and if used systematically makes data analysis easier.\nWe can create a dataframe by bringing vectors together to form the columns. We do this using the data.frame() function, and giving the function the different vectors we would like to bind together. This function will only work for vectors of the same length.\n\n# Create a data frame and store it as a variable called 'df'\ndf <- data.frame(species, glengths)\n\nWe can see that a new variable called df has been created in our Environment within a new section called Data. In the Environment, it specifies that df has 3 observations of 2 variables. What does that mean? In R, rows always come first, so it means that df has 3 rows and 2 columns. We can get additional information if we click on the blue circle with the white triangle in the middle next to df. It will display information about each of the columns in the data frame, giving information about what the data type is of each of the columns and the first few values of those columns.\nAnother handy feature in RStudio is that if we hover the cursor over the variable name in the Environment, df, it will turn into a pointing finger. If you click on df, it will open the data frame as it’s own tab next to the script editor. We can explore the table interactively within this window. To close, just click on the X on the tab.\nAs with any variable, we can print the values stored inside to the console if we type the variable’s name and run.\n\ndf\n\n  species glengths\n1   ecoli      4.6\n2   human   3000.0\n3    corn  50000.0\n\n\n\n\n3.2.5 Lists\nLists are a data structure in R that can be perhaps a bit daunting at first, but soon become amazingly useful. A list is a data structure that can hold any number of any types of other data structures.\nIf you have variables of different data structures you wish to combine, you can put all of those into one list object by using the list() function and placing all the items you wish to combine within parentheses:\n\nlist1 <- list(species, df, expression)\n\nWe see list1 appear within the Data section of our environment as a list of 3 components or variables. If we click on the blue circle with a triangle in the middle, it’s not quite as interpretable as it was for data frames.\nEssentially, each component is preceded by a colon. The first colon give the species vector, the second colon precedes the df data frame, with the dollar signs indicating the different columns, the last colon gives the single value, number.\nIf I click on list1, it opens a tab where you can explore the contents a bit more, but it’s still not super intuitive. The easiest way to view small lists is to print to the console.\nLet’s type list1 and print to the console by running it.\n\nlist1\n\n[[1]]\n[1] \"ecoli\" \"human\" \"corn\" \n\n[[2]]\n  species glengths\n1   ecoli      4.6\n2   human   3000.0\n3    corn  50000.0\n\n[[3]]\n[1] low    high   medium high   low    medium high  \nLevels: high low medium\n\n\nThere are three components corresponding to the three different variables we passed in, and what you see is that structure of each is retained. Each component of a list is referenced based on the number position.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session1/prob.html#defining-probability",
    "href": "session1/prob.html#defining-probability",
    "title": "4  Probability Primer",
    "section": "4.1 Defining Probability",
    "text": "4.1 Defining Probability\nInformally, we usually think of probability as a number that describes the likelihood of some event occurring, which ranges from zero (impossibility) to one (certainty).\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets. For a coin flip, the sample space is {heads, tails}. For a six-sided die, the sample space is each of the possible numbers that can appear: {1,2,3,4,5,6}. For the amount of time it takes to get to work, the sample space is all possible real numbers greater than zero (since it can’t take a negative amount of time to get somewhere, at least not yet).\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome. For example, this could be obtaining heads in a single coin flip, rolling a 4 on a throw of the die, or taking 21 minutes to get home by the new route.\n\nLet’s say that we have a sample space defined by N independent events, \\({E_1, E_2, ... , E_N}\\), and \\(X\\) is a random variable denoting which of the events has occurred. \\(P(X=E_i)\\) is the probability of event \\(i\\):\n\nProbability cannot be negative: \\(P(X=E_i) \\ge 0\\)\nThe total probability of all outcomes in the sample space is 1; that is, if the , if we take the probability of each Ei and add them up, they must sum to 1. We can express this using the summation symbol \\(\\sum\\): \\[\n\\sum_{i=1}^N{P(X=E_i)} = P(X=E_1) + P(X=E_2) + ... + P(X=E_N) = 1\n\\] This is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\n\nThe probability of any individual event cannot be greater than one: \\(P(X=E_i)\\le 1\\). This is implied by the previous point; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.\n\n\n4.1.1 Conditional probability\nThese definitions allow us to examine simple probabilities - that is, the probability of a single event or combination of events.\nHowever, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.\nTo compute the conditional probability of A given B (which we write as \\(P(A|B)\\), “probability of A, given B”), we need to know the joint probability (that is, the probability of both A and B occurring) as well as the overall probability of B:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThat is, we want to know the probability that both things are true, given that the one being conditioned upon is true.\n\n\n4.1.2 Independence\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other. This can be expressed as:\n\\[\nP(A|B) = P(A)\n\\]\nThat is, the probability of A given some value of B is just the same as the overall probability of A."
  },
  {
    "objectID": "session1/prob.html#probability-distributions",
    "href": "session1/prob.html#probability-distributions",
    "title": "4  Probability Primer",
    "section": "4.2 Probability distributions",
    "text": "4.2 Probability distributions\nA probability distribution describes the probability of all of the possible outcomes in an experiment. To help understand distributions and how they can be used, let’s look at a few discrete probability distributions, meaning distributions which can only output integers.\n\n4.2.1 Binomial success counts\nTossing a coin has two possible outcomes. This simple experiment, called a Bernoulli trial, is modeled using a so-called Bernoulli random variable.\nR has special functions tailored to generate outcomes for each type of distribution. They all start with the letter r, followed by a specification of the model, here rbinom, where binom is the abbreviation used for binomial.\nSuppose we want to simulate a sequence of 15 fair coin tosses. To get the outcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a fair coin), we write:\n\nrbinom(15, prob = 0.5, size = 1)\n\n [1] 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1\n\n\nWe use the rbinom function with a specific set of parameters (called arguments in programming): the first parameter is the number of trials we want to observe; here we chose 15. We designate by prob the probability of success. By size=1 we declare that each individual trial consists of just one single coin toss.\nFor binary events such as heads or tails, success or failure, CpG or non-CpG, M or F, Y = pyrimidine or R = purine, diseased or healthy, true or false, etc. we only need the probability \\(p\\) of one of the events (which we, often arbitrarily, will label “success”) because “failure” (the complementary event) will occur with probability \\(1-p\\). We can then simply count the number of successes for a certain number of trials:\n\nrbinom(1, prob = 0.3, size = 15)\n\n[1] 6\n\n\nThis gives us the number of successes for \\(15\\) trials where the probability of success was \\(0.3\\). We would call this number a binomial random variable or a random variable that follows the \\(B(15, 0.3)\\) distribution.\nWe can plot the probability mass distribution using dbinom:\n\nprobabilities <- dbinom(0:15, prob = 0.3, size = 15)\nbarplot(probabilities, names.arg = 0:15, col = \"brown\")\n\n\n\n\nFor \\(X\\) distributed as a binomial distribution with parameters \\((n,p)\\), written \\(X ~ B(n,p)\\) the probability of seeing \\(X=k\\) sucesses is: \\[\nP(k; n,p) = P(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}\n\\]\n\n\n4.2.2 Poisson distributions\nWhen the probability of success \\(p\\) is small and the number of trials \\(n\\) large, the binomial distribution \\(B(n,p)\\) can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter \\(\\lambda = np\\)\nThe Poisson distribution comes up often in biology as we often are naturally dealing very low probability events and large numbers of trials, such as mutations in a genome.\n\nsimulations = rbinom(n = 300000, prob = 5e-4, size = 10000)\nbarplot(table(simulations), col = \"lavender\")\n\n\n\nprobabilities <- dpois(0:18, lambda=(10000 * 5e-4))\nbarplot(probabilities, names.arg = 0:18, col = \"brown\")\n\n\n\n\n\n\n4.2.3 Multinomial distributions\nWhen modeling four possible outcomes, for instance when studying counts of the four nucleotides [A,C,G] and [T], we need to extend the binomial model.\nWe won’t go into detail on the formulation, but we can examine probabilities of observations using a vector of counts for each observed outcome, and a vector of probabilities for each outcome (which must sum to 1).\n\ncounts <- c(4,2,0,0)\nprobs <- c(0.25,0.25,0.25,0.25)\ndmultinom(counts, prob = probs)\n\n[1] 0.003662109\n\n\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session1/hypo_tests.html#sec-epitope",
    "href": "session1/hypo_tests.html#sec-epitope",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.1 Calculating the chance of an event",
    "text": "5.1 Calculating the chance of an event\nWhen testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes.\nEpitope: A specific portion of a macromolecular antigen to which an antibody binds. In the case of a protein antigen recognized by a T-cell, the epitope or determinant is the peptide portion or site that binds to a Major Histocompatibility Complex (MHC) molecule for recognition by the T cell receptor (TCR).\nEnzyme-Linked ImmunoSorbent Assays (ELISA) are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:\n\nThe baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this \\(P(declare epitope|no epitope)\\)\nThe protein is tested at 100 different positions, supposed to be independent.\nWe are going to examine a collection of 50 patient samples.\n\nThe data for one patient’s assay look like this:\n[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nwhere the 1 signifies a hit (and thus the potential for an allergic reaction), and the zeros signify no reaction at that position.\nWe’re going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed \\((0,1)\\) variables to have a Poisson distribution with parameter \\(0.5\\).\n\nload(\"../data/e100.RData\")\nbarplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n  names.arg = seq(along = e100), col = \"darkolivegreen\")\n\n\n\n\nThe spike is striking. What are the chances of seeing a value as large as 7, if no epitope is present? If we look for the probability of seeing a number as big as 7 (or larger) when considering one \\(Poisson(0.5)\\) random variable, the answer can be calculated in closed form as\n\\[\nP(X \\ge 7) = \\sum\\limits_{k=7}^{\\infty}P(X=k)\n\\] This is, of course, the same as \\(1-P(X \\le 6)\\). The probability is the so-called cumulative distribution function at 6, and R has the function ppois for computing it, which we can use in either of the following two ways:\n\n1 - ppois(6, 0.5)\n\n[1] 1.00238e-06\n\nppois(6, 0.5, lower.tail = FALSE)\n\n[1] 1.00238e-06\n\n\nYou can use the command ?ppois to see the argument definitions for the function.\nWe denote this number, our chance of seeing such an extreme result, as \\(\\epsilon\\). However, in this case it would be the incorrect calculation.\nInstead of asking what the chances are of seeing a Poisson(0.5) as large as 7, we need to instead ask, what are the chances that the maximum of 100 Poisson(0.5) trials is as large as 7? We order the data values \\(x_1, x_2, ... , x_{100}\\) and rename them \\(x_{(1)}, x_{(2)}, ... , x_{(100)}\\), so that denotes \\(x_{(1)}\\) the smallest and \\(x_{(100)}\\) the largest of the counts over the 100 positions. Together, are called the rank statistic of this sample of 100 values.\nThe maximum value being as large as 7 is the complementary event of having all 100 counts be smaller than or equal to 6. Two complementary events have probabilities that sum to 1. Because the positions are supposed to be independent, we can now do the computation:\n\\[\nP(x_{(100)} \\ge 7) = \\prod\\limits_{i=1}^{100}P(x_i \\le 6) = (P(x_i \\le 6))^{100}\n\\] which, using our notation, is \\((1-\\epsilon)^{100}\\) and is approximately \\(10^{-4}\\). This is a very small chance, so we would determine it is most likely that we did detect real epitopes."
  },
  {
    "objectID": "session1/hypo_tests.html#computing-probabilities-with-simulations",
    "href": "session1/hypo_tests.html#computing-probabilities-with-simulations",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.2 Computing probabilities with simulations",
    "text": "5.2 Computing probabilities with simulations\nIn the case we just saw, the theoretical probability calculation was quite simple and we could figure out the result by an explicit calculation. In practice, things tend to be more complicated, and we are better to compute our probabilities using the Monte Carlo method: a computer simulation based on our generative model that finds the probabilities of the events we’re interested in. Below, we generate 100,000 instances of picking the maximum from 100 Poisson distributed numbers.\n\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n    1     2     3     4     5     6     7     8 \n    6 23367 60625 14298  1566   129     7     2 \n\n\nSo we can approximate the probability of seeing a \\(7\\) as:\n\nmean( maxes >= 7 )\n\n[1] 9e-05\n\n\nWe arrive at a similarly small number, and in both cases would determine that there are real epitopes in the dataset."
  },
  {
    "objectID": "session1/hypo_tests.html#an-example-coin-tossing",
    "href": "session1/hypo_tests.html#an-example-coin-tossing",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.3 An example: coin tossing",
    "text": "5.3 An example: coin tossing\nLet’s look a simpler example: flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like HHTTHTHTT...\nLet’s simulate the experiment in R, using a biased coin:\n\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\n# Sample is a function in base R which let's us take a random sample from a vector, with or without replacement. \n# This line is sampling numFlips times from the vector ['H','T'] with replacement, with the probabilities for \n# each item in the vector being defined in the prob argument as [probHead, 1-probHead]\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\n# Thus, coinFlips is a character vector of a random sequence of 'T' and 'H'. \nhead(coinFlips)\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n\n\nNow, if the coin were fair, we would expect half of the time to get heads. Let’s see.\n\ntable(coinFlips)\n\ncoinFlips\n H  T \n59 41 \n\n\nThat is different from 50/50. However, does the data deviates strong enough to conclude that this coin isn’t fair? We know that the total number of heads seen in 100 coin tosses for a fair coin follows \\(B(100, 0.5)\\), making it a suitable test statistic.\nTo decide, let’s look at the sampling distribution of our test statistic – the total number of heads seen in 100 coin tosses – for a fair coin. As we learned, we can do this with the binomial distribution. Let’s plot a fair coin and mark our observation with a blue line:\n\nlibrary(\"dplyr\")\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(\"ggplot2\")\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n# This line sets k as the vector [0, 1, 2,...,numFlips]\nk <- 0:numFlips\n# Recall that binary variables (TRUE and FALSE) are interpreted as 1 and 0, so we can use this operation\n# to count the number of heads in coinFlips. We practice these kinds of operations in session 3. \nnumHeads <- sum(coinFlips == \"H\")\n# We use dbinom here to get the probability mass at every integer from 1-numFlips so that we can plot the distribution. \np <- dbinom(k, size = numFlips, prob = 0.5)\n# We then convert it into a dataframe for easier plotting. \nbinomDensity <- data.frame(k = k, p = p)\nhead(binomDensity)\n\n  k            p\n1 0 7.888609e-31\n2 1 7.888609e-29\n3 2 3.904861e-27\n4 3 1.275588e-25\n5 4 3.093301e-24\n6 5 5.939138e-23\n\n# Here, we are plotting the binomial distribution, with a vertical line representing\n# the number of heads we actually observed. We will learn how to create plots in session 4. \n# Thus, to complete our test we simply need to identify whether or not the blue line\n# is in our rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n\n\n\nHow do we quantify whether the observed value is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased?\nWe divide the set of all possible \\(k(0-100)\\) in two complementary subsets, the rejection region and the region of no rejection. We want to make the rejection region as large as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\alpha\\)(say, 0.05).\n\nalpha <- 0.05\n# We get the density of our plot in sorted order, meaning that we'll see binomDensity\n# jump back and forth between the distribution's tails as p increases. \nbinomDensity <- binomDensity[order(p),]\n# We then manually calculate our rejection region by finding where the cumulative sum in the distribution\n# is less than or equal to our chosen alpha level. \nbinomDensity$reject <- cumsum(binomDensity$p) <= alpha\nhead(binomDensity)\n\n      k            p reject\n1     0 7.888609e-31   TRUE\n101 100 7.888609e-31   TRUE\n2     1 7.888609e-29   TRUE\n100  99 7.888609e-29   TRUE\n3     2 3.904861e-27   TRUE\n99   98 3.904861e-27   TRUE\n\n# Now we recreate the same plot as before, but adding red borders around the parts of our distribution\n# in the rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nWe sorted the \\(p\\)-values from lowest to highest (order), and added a column reject by computing the cumulative sum (cumsum) of the \\(p\\)-values and thresholding it against alpha.\nThe logical column reject therefore marks with TRUE a set of \\(k\\)s whose total probability is less than \\(\\alpha\\).\nThe rejection region is marked in red, containing both very large and very small values of \\(k\\), which can be considered unlikely under the null hypothesis.\nR provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom). Those are more precise and faster than cumsum over the probabilities.\nThe (cumulative) distribution function is defined as the probability that a random variable \\(X\\) will take a value less than or equal to \\(x\\).\n\\[F(x) = P(X \\le x)\\]\nWe have just gone through the steps of a binomial test. This is a frequently used test and therefore available in R as a single function.\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\n\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59"
  },
  {
    "objectID": "session1/hypo_tests.html#hypothesis-tests",
    "href": "session1/hypo_tests.html#hypothesis-tests",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.4 Hypothesis Tests",
    "text": "5.4 Hypothesis Tests\nWe can summarize what we just did with a series of steps:\n\nDecide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\nSet up a null hypothesis, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\nDecide on the rejection region, i.e., a subset of possible outcomes whose total probability is small.\nDo the experiment and collect the data; compute the test statistic.\nMake a decision: reject the null hypothesis if the test statistic is in the rejection region."
  },
  {
    "objectID": "session1/hypo_tests.html#types-of-error",
    "href": "session1/hypo_tests.html#types-of-error",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.5 Types of Error",
    "text": "5.5 Types of Error\n\n\n\nFrom “Modern Statistics for Modern Biology”\n\n\nHaving set out the mechanics of testing, we can assess how well we are doing. The following table, called a confusion matrix, compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n\n\n\n\n\n\n\nTest vs reality\nNull is true\nNull is false\n\n\n\n\nReject null\nType I error (false positive)\nTrue postitive\n\n\nDo not reject null\nTrue negative\nType II error (false negative)\n\n\n\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade-off between both of them. We can always decrease the false positive rate (FPR) by shifting the threshold to the right. We can become more “conservative”. But this happens at the price of higher false negative rate (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. T he FPR is the same as the probability \\(\\alpha\\) that we mentioned above. \\(1-\\alpha\\) is also called the specificity of a test. The FNR is sometimes also called \\(\\beta\\), and \\(1-\\beta\\) the power, sensitivity or true positive rate of a test. The power of a test can be understood as the likelihood of it “catching” a true positive, or correctly rejecting the null hypothesis.\nGenerally, there are three factors that can affect statistical power:\n\nSample size: Larger samples provide greater statistical power\nEffect size: A given design will always have greater power to find a large effect than a small effect (because finding large effects is easier)\nType I error rate: There is a relationship between Type I error and power such that (all else being equal) decreasing Type I error will also decrease power.\n\nIn a future session, we will also see how hypothesis tests can be seen as types of linear models.\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session1/factors.html#factors",
    "href": "session1/factors.html#factors",
    "title": "6  Categorical Data in R",
    "section": "6.1 Factors",
    "text": "6.1 Factors\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression <- c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\")\n\nThe elements of this expression factor created previously has following categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression, and let’s using nesting here:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] \"high\" \"high\"\n\n\n\nNesting note:\nThe piece of code above was more efficient with nesting; we used a single step instead of two steps as shown below:\nStep1 (no nesting): idx <- expression == \"high\"\nStep2 (no nesting): expression[idx]"
  },
  {
    "objectID": "session1/factors.html#releveling-factors",
    "href": "session1/factors.html#releveling-factors",
    "title": "6  Categorical Data in R",
    "section": "6.2 Releveling factors",
    "text": "6.2 Releveling factors\nWe have briefly talked about factors, but this data type only becomes more intuitive once you’ve had a chance to work with it. Let’s take a slight detour and learn about how to relevel categories within a factor.\nTo view the integer assignments under the hood you can use str():\n\nexpression\n\n [1] \"high\"   \"low\"    \"low\"    \"medium\" \"high\"   \"medium\" \"medium\" \"low\"   \n [9] \"low\"    \"low\"   \n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3, i.e. it makes sense for us to “relevel” the categories in this factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression <- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can re-factor a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nNote: Releveling becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment."
  },
  {
    "objectID": "session1/choosing_tests.html#performing-a-hypothesis-test",
    "href": "session1/choosing_tests.html#performing-a-hypothesis-test",
    "title": "7  Performing and choosing hypothesis tests",
    "section": "7.1 Performing a Hypothesis Test",
    "text": "7.1 Performing a Hypothesis Test\nMany experimental measurements are reported as rational numbers, and the simplest comparison we can make is between two groups, say, cells treated with a substance compared to cells that are not. The basic test for such situations is the t-test. The test statistic is defined as\n\\[\nt = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\\]\nwhere \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups, \\(S^2_1\\) and \\(S^2_2\\) are the estimated variances of the groups, and \\(n_1\\) and \\(n_2\\) are the sizes of the two groups. Because the variance of a difference between two independent variables is the sum of the variances of each individual variable (\\(var(A - B) = var(A) + var(B)\\)), we add the variances for each group divided by their sample sizes in order to compute the standard error of the difference. Thus, one can view the the t statistic as a way of quantifying how large the difference between groups is in relation to the sampling variability of the difference between means.\nLet’s try this out with the PlantGrowth data from R’s datasets package.\n\nlibrary(\"ggbeeswarm\")\n\nWarning: package 'ggbeeswarm' was built under R version 4.2.2\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\ndata(\"PlantGrowth\")\nggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n  geom_beeswarm() + theme(legend.position = \"none\")\n\n\n\ntt1 = t.test(PlantGrowth$weight[PlantGrowth$group ==\"ctrl\"],\n      PlantGrowth$weight[PlantGrowth$group ==\"trt1\"],\n      var.equal = TRUE)\ntt2 = t.test(PlantGrowth$weight[PlantGrowth$group ==\"ctrl\"],\n      PlantGrowth$weight[PlantGrowth$group ==\"trt2\"],\n      var.equal = TRUE)\ntt1\n\n\n    Two Sample t-test\n\ndata:  PlantGrowth$weight[PlantGrowth$group == \"ctrl\"] and PlantGrowth$weight[PlantGrowth$group == \"trt1\"]\nt = 1.1913, df = 18, p-value = 0.249\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2833003  1.0253003\nsample estimates:\nmean of x mean of y \n    5.032     4.661 \n\ntt2\n\n\n    Two Sample t-test\n\ndata:  PlantGrowth$weight[PlantGrowth$group == \"ctrl\"] and PlantGrowth$weight[PlantGrowth$group == \"trt2\"]\nt = -2.134, df = 18, p-value = 0.04685\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.980338117 -0.007661883\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n\nTo compute the p-value, the t.test function uses the asymptotic theory for the t-statistic. This theory states that under the null hypothesis of equal means in both groups, the statistic follows a known, mathematical distribution, the so-called t-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. The theory uses additional technical assumptions, namely that the data are independent and come from a normal distribution with the same standard deviation.\nIn fact, most of the tests we will look at assume that the data come from a normal distribution. That the normal distribution comes up so often is largly explained by the central limit theorem in statistics. The Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is also known as the Gaussian distribution. The normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution).\nThe bell-like shape of the distribution never changes, only its location and width.\nAn important note about the central limit theorem is that it is asymptotic, meaning that it is true as the size of our dataset approaches infinity. For very small sample sizes, even if we are taking the mean of our samples the data might not follow the normal distribution closely enough for tests which assume it to make sense.\nThe independence assumption\nNow let’s try something peculiar: duplicate the data.\n\nwith(rbind(PlantGrowth, PlantGrowth),\n       t.test(weight[group == \"ctrl\"],\n              weight[group == \"trt2\"],\n              var.equal = TRUE))\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -3.1007, df = 38, p-value = 0.003629\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8165284 -0.1714716\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n\nNote that estimates of the group means (and thus the difference) are unchanged, but the \\(p\\)-value is now much smaller!"
  },
  {
    "objectID": "session1/choosing_tests.html#choosing-the-right-test",
    "href": "session1/choosing_tests.html#choosing-the-right-test",
    "title": "7  Performing and choosing hypothesis tests",
    "section": "7.2 Choosing the Right Test",
    "text": "7.2 Choosing the Right Test\n\n7.2.1 Variable Types (Effect)\nThe types of our variables need to be considered. We will go through some choices if our variables are quantitative (continuous; a number or qualitative (discrete; a category or factor). However, note that other tests exist for some specific properties like proportions.\nIf we wish to consider the relationship between two quantitative variables, we need to perform a correlation analysis. The Pearson correlation directly analyses the numbers (is parametric) while Spearman’s rank correlation considers ranks (and is nonparametric).\nFor two qualitative variables, we typically will use a Chi-square test of independence, though we may be able to use Fisher’s exact test if the dataset is small enough.\nWe often are interested in the case where we want to see the relationship between one quantitative variable and one qualitative variable. In this case, we most commonly use some variation of a t-test if we have only have 2 groups we are considering, and some variation of an ANOVA test if we have more than 2. We will get into more detail about ANOVA tests in a future session.\n\n\n7.2.2 Paired vs Unpaired\nPaired and unpaired tests refer to whether or not there is a 1:1 correspondence between our different observations. Experiments which involve measuring the same set of biological samples, often as before and after some kind of treatment, are paired. In paired experiments we can look at each observation, see whether it individually changed between groups.\nIn unpaired tests we consider our samples to be independent across groups. This is the case if we have two different groups, such as a control group and a treatment group.\nPerforming a paired or unpaired test can be set as an argument in R’s t.test function, but nonparametric tests have different names, the Mann-Whitney U test for unpaired samples and the Wilcoxon signed-rank test for paired samples in tests with 2 groups, and the Kruskal-Wallis test and Friedman test for more than two groups.\n\n\n7.2.3 Parametric vs Non-Parametric\nSo far, we have only seen parametric tests. These are tests which are based on a statistical distribution, and thus depends on having defined parameters. These tests inherently assume that the collected data follows some distribution, typically a normal distribution as discussed above.\nA nonparametric test makes many fewer assumptions about the distribution of our data. Instead of dealing with values directly, they typically perform their calculations on rank. This makes them especially good at dealing with extreme values and outliers. However, they are typically less powerful than parametric tests; they will be less likely to reject the null hypothesis (return a higher p-value) if the data did follow a normal distribution and you had performed a parametric test on it. Thus, they should only be used if necessary.\nA typical rule of thumb is that around \\(30\\) samples is enough to not have to worry about the underlying distribution of your data. However, they are types of data, such as directly collecting ranking data or ratings, which should be analyzed with nonparametric methods.\n\n\n7.2.4 One-tailed and Two-tailed tests\nAll tests have one-tailed and two-tailed versions. A two-tailed test considers a result significant if it is extreme in either direction; it can be higher or lower than what would be expected under the null hypothesis. A one-tailed test will only consider a single direction, either higher or lower. Usually, the p value for the two-tailed test is twice as large as that for the one-tailed test, which reflects the fact that an extreme value is less surprising since it could have occurred in either direction.\nHow do you choose whether to use a one-tailed versus a two-tailed test? The two-tailed test is always going to be more conservative, so it’s always a good bet to use that one, unless you had a very strong prior reason for using a one-tailed test. This is set through the alternative argument in t.test.\n\n\n7.2.5 Variance\nAnother underlying assumption of many statistical tests is that different groups have the same variance. The t-test will perform a slightly more conservative calculation if equal variance is not assumed (called Welch’s t-test instead of Student’s t-test). This can be set as the var.equal argument of t.test.\nWe often can assume equal variance, but as we will see in a later session, many modern sequencing technologies can produce data with patterns in its variance we will have to adjust for.\n\n\n7.2.6 How Many Variables of Interest?\nAll of the above discussion is for experiments with where we are interested in looking at the relationship between two variables. These, slightly confusingly, are called 2 sample tests, and line up with the classical experimental paradigm of a single dependent and a single independent variable. However, there are other options.\n\nOne Sample: Instead of wanting to compare how a categorical variable (like treatment) affects some outcome variable, we could imagine comparing against some known value. When we considered whether or not a coin was fair, we were not comparing two coins, but instead comparing the output of one coin against a known value.\nMore than two samples: Modern observational studies often, by necessity, need to consider how many variables affect some outcome. These analyses are performed via regression models, multiple linear regression for a quantitative dependent variable and logistic regression for a qualitative dependent variable."
  },
  {
    "objectID": "session1/pSet1.html#problem-1",
    "href": "session1/pSet1.html#problem-1",
    "title": "8  Problem Set 1",
    "section": "8.1 Problem 1",
    "text": "8.1 Problem 1\nR can generate numbers from all known distributions. We now know how to generate random discrete data using the specialized R functions tailored for each type of distribution. We use the functions that start with an r as in rXXXX, where XXXX could be pois, binom, multinom. If we need a theoretical computation of a probability under one of these models, we use the functions dXXXX, such as dbinom, which computes the probabilities of events in the discrete binomial distribution, and dnorm, which computes the probability density function for the continuous normal distribution. When computing tail probabilities such as \\(P(X > a)\\) it is convenient to use the cumulative distribution functions, which are called pXXXX. Find two other discrete distributions that could replace the XXXX above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOther discrete distributions in R:\n\nGeometric distribution: geom\nHypergeometric distribution: hyper\nNegative binomial distribution: nbinom\n\nYou can type in ?Distributions to see a list of available distributions in base R. You can also view this information online here, and a list of distributions included in other packages here."
  },
  {
    "objectID": "session1/pSet1.html#problem-2",
    "href": "session1/pSet1.html#problem-2",
    "title": "8  Problem Set 1",
    "section": "8.2 Problem 2",
    "text": "8.2 Problem 2\nHow would you calculate the probability mass at the value \\(X=2\\) for a binomial \\(B(10, 0.3)\\) with dbinom? Use dbinom to compute the cumulative distribution at the value 2, corresponding to \\(P(X \\leq 2)\\), and check your answer with another R function. Hint: You will probably want to use the sum function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe dbinom function directly gives us the probabilty mass:\n\ndbinom(2, 10, 0.3)\n\n[1] 0.2334744\n\n\nSince the binomial distribution is discrete, we can get the cumulative distribution function by simply summing the mass at 0, 1, and 2. Note that if this were a continuous distribution, we would have to integrate the mass function over the range instead.\nRecall that we can pass a vector into functions like dbinom to get multiple values at once:\n\ndbinom(0:2, 10, 0.3)\n\n[1] 0.02824752 0.12106082 0.23347444\n\n\nWe can then simply sum the result:\n\nsum(dbinom(0:2, 10, 0.3))\n\n[1] 0.3827828\n\n\nWe can now check our answer with the pbinom function which directly gives the cumulative distribution function:\n\npbinom(2, 10, 0.3)\n\n[1] 0.3827828"
  },
  {
    "objectID": "session1/pSet1.html#problem-3",
    "href": "session1/pSet1.html#problem-3",
    "title": "8  Problem Set 1",
    "section": "8.3 Problem 3",
    "text": "8.3 Problem 3\nIn the epitope example (Section 5.1), use a simulation to find the probability of having a maximum of 9 or larger in 100 trials. How many simulations do you need if you would like to prove that “the probability is smaller than 0.000001”?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimulation solution (what was asked for)\nWe can re-examine the results of the simulation we ran during class:\n\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n    1     2     3     4     5     6     7     8 \n    7 23347 60509 14385  1619   122    10     1 \n\n\nHowever, most of the time we don’t even get a single 9! We need to increase the number of trials in order to see more extreme numbers:\n\nmaxes = replicate(10000000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n      1       2       3       4       5       6       7       8       9 \n    792 2346064 6044147 1438199  156941   12872     925      57       3 \n\n\nThis calculation may take awhile to run. When running it I got 6 instances of 9 counts, so we can estimate the probability as: \\(6/10000000 = 6\\times 10^{-7}\\). We can see that the lower-probability of an event we want to estimate, the more simulations we need to run and the more computational power we need.\nWe would need at least a million runs in order to be able to estimate a probability of \\(0.000001\\), as \\(1/0.000001 = 1000000\\).\nHow you would calculate things exactly\nIn the epitope example we were able to calculate the probability of a single assay having a count of at least 7 as:\n\n1 - ppois(6, 0.5)\n\n[1] 1.00238e-06\n\n\nAnd then the probability of seeing a number this extreme at least once among 100 assays as:\n\n1 - ppois(6, 0.5)^100\n\n[1] 0.000100233\n\n\nIn order to calculate the probability of a maximum of 9 or larger, we simply need to alter our complementary event probabilty calculation to 8:\n\n1 - ppois(8, 0.5)^100\n\n[1] 3.43549e-07"
  },
  {
    "objectID": "session1/pSet1.html#problem-4",
    "href": "session1/pSet1.html#problem-4",
    "title": "8  Problem Set 1",
    "section": "8.4 Problem 4",
    "text": "8.4 Problem 4\nFind a paper in your research area which uses a hypothesis test. Cite the paper and note:\n\nThe null hypothesis.\nThe alternative hypothesis.\nWas the test two-tailed or one-tailed?\nWhat types of variables were compared?\nWas the test parametric or non-parametric?\nCan we safely assume equal variance?\nWhat was the sample size?\n\nIf the necessary details to determine any of the above are not in the paper, you can note that instead.\nGiven what you’ve written and the author’s decisions, do you agree with the choice of hypothesis test and the conclusions drawn?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution here obviously varies. In order to determine whether or not a test was used correctly, we need to at least consider: - The validity of the null and alternative hypotheses - Whether or not the assumptions of the test (independent samples, variable type, parametric or non-parametric, etc., uniform variance, etc.) hold or at least probably mostly hold for the experiment. - Whether there is any indication of p-hacking or sources of experimental bias.\n\n\n\n\nThe materials in this lesson have been adapted from: Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session2/session2.html#learning-objectives",
    "href": "session2/session2.html#learning-objectives",
    "title": "Session 2: Functions and Multiple Hypothesis Correction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nConvert and re-level factor data.\nDetermine which hypothesis test is appropriate for common biological analyses.\nUse and create functions in R.\nApply and interpret multiple hypotheses testing corrections.\nImplement hypothesis tests using R."
  },
  {
    "objectID": "session2/packages.html",
    "href": "session2/packages.html",
    "title": "9  Packages and Libraries",
    "section": "",
    "text": "Packages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages and growing.\nThere are a set of standard (or base) packages which are considered part of the R source code and automatically available as part of your R installation. Base packages contain the basic functions that allow R to work, and enable standard statistical and graphical functions on datasets; for example, all of the functions that we have been using so far in our examples.\nThe directories in R where the packages are stored are called the libraries. The terms package and library are sometimes used synonymously and there has been discussion amongst the community to resolve this. It is somewhat counter-intuitive to load a package using the library() function and so you can see how confusion can arise.\nYou can check what libraries are loaded in your current R session by typing into the console:\nsessionInfo() #Print version information about R, the OS and attached or loaded packages\n\n# OR\n\nsearch() #Gives a list of attached packages\nPreviously we have introduced you to functions from the standard base packages. However, the more you work with R, you will come to realize that there is a cornucopia of R packages that offer a wide variety of functionality. To use additional packages will require installation. Many packages can be installed from the CRAN or Bioconductor repositories.\n\n9.0.1 Helpful tips for package installations\n\nPackage names are case sensitive!\nAt any point (especially if you’ve used R/Bioconductor in the past), in the console R may ask you if you want to “update any old packages by asking Update all/some/none? [a/s/n]:”. If you see this, type “a” at the prompt and hit Enter to update any old packages. Updating packages can sometimes take awhile to run. If you are short on time, you can choose “n” and proceed. Without updating, you run the risk of conflicts between your old packages and the ones from your updated R version later down the road.\nIf you see a message in your console along the lines of “binary version available but the source version is later”, followed by a question, “Do you want to install from sources the package which needs compilation? y/n”, type n for no, and hit enter.\n\n\n\n9.0.2 Package installation from CRAN\nCRAN is a repository where the latest downloads of R (and legacy versions) are found in addition to source code for thousands of different user contributed R packages.\n\nPackages for R can be installed from the CRAN package repository using the install.packages function. This function will download the source code from on the CRAN mirrors and install the package (and any dependencies) locally on your computer.\nAn example is given below for the ggplot2 package that will be required for some plots we will create later on. Run this code to install ggplot2.\ninstall.packages(\"ggplot2\")\n\n\n9.0.3 Package installation from Bioconductor\nAlternatively, packages can also be installed from Bioconductor, another repository of packages which provides tools for the analysis and comprehension of high-throughput genomic data. These packages includes (but is not limited to) tools for performing statistical analysis, annotation packages, and accessing public datasets.\n\nThere are many packages that are available in CRAN and Bioconductor, but there are also packages that are specific to one repository. Generally, you can find out this information with a Google search or by trial and error.\nTo install from Bioconductor, you will first need to install BiocManager. This only needs to be done once ever for your R installation.\n# DO NOT RUN THIS!\n\ninstall.packages(\"BiocManager\")\nNow you can use the install() function from the BiocManager package to install a package by providing the name in quotations.\nHere we have the code to install ggplot2, through Bioconductor:\n# DO NOT RUN THIS!\n\nBiocManager::install(\"ggplot2\")\n\nThe code above may not be familiar to you - it is essentially using a new operator, a double colon :: to execute a function from a particular package. This is the syntax: package::function_name().\n\n\n\n9.0.4 Package installation from source\nFinally, R packages can also be installed from source. This is useful when you do not have an internet connection (and have the source files locally), since the other two methods are retrieving the source files from remote sites.\nTo install from source, we use the same install.packages function but we have additional arguments that provide specifications to change from defaults:\n# DO NOT RUN THIS!\n\ninstall.packages(\"~/Downloads/ggplot2_1.0.1.tar.gz\", type=\"source\", repos=NULL)\n\n\n9.0.5 Loading libraries\nOnce you have the package installed, you can load the library into your R session for use. Any of the functions that are specific to that package will be available for you to use by simply calling the function as you would for any of the base functions. Note that quotations are not required here.\nlibrary(ggplot2)\nYou can also check what is loaded in your current environment by using sessionInfo() or search() and you should see your package listed as:\nother attached packages:\n[1] ggplot2_2.0.0\nIn this case there are several other packages that were also loaded along with ggplot2.\nWe only need to install a package once on our computer. However, to use the package, we need to load the library every time we start a new R/RStudio environment. You can think of this as installing a bulb versus turning on the light.\n\nAnalogy and image credit to Dianne Cook of Monash University.\n\n\n9.0.6 Finding functions specific to a package\nThis is your first time using ggplot2, how do you know where to start and what functions are available to you? One way to do this, is by using the Package tab in RStudio. If you click on the tab, you will see listed all packages that you have installed. For those libraries that you have loaded, you will see a blue checkmark in the box next to it. Scroll down to ggplot2 in your list:\n\nIf your library is successfully loaded you will see the box checked, as in the screenshot above. Now, if you click on ggplot2 RStudio will open up the help pages and you can scroll through.\nAn alternative is to find the help manual online, which can be less technical and sometimes easier to follow. For example, this website is much more comprehensive for ggplot2 and is the result of a Google search. Many of the Bioconductor packages also have very helpful vignettes that include comprehensive tutorials with mock data that you can work with.\nIf you can’t find what you are looking for, you can use the rdocumention.org website that search through the help files across all packages available.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/loading_functions.html#read.csv",
    "href": "session2/loading_functions.html#read.csv",
    "title": "10  Reading data into R",
    "section": "10.1 read.csv()",
    "text": "10.1 read.csv()\nYou can check the arguments for the function using the ? to ensure that you are entering all the information appropriately:\n?read.csv\n\nThe first thing you will notice is that you’ve pulled up the documentation for read.table(), this is because that is the parent function and all the other functions are in the same family.\nThe next item on the documentation page is the function Description, which specifies that the output of this set of functions is going to be a data frame - “Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.”\nIn usage, all of the arguments listed for read.table() are the default values for all of the family members unless otherwise specified for a given function. Let’s take a look at 2 examples: 1. The separator - * in the case of read.table() it is sep = \"\" (space or tab) * whereas for read.csv() it is sep = \",\" (a comma). 2. The header - This argument refers to the column headers that may (TRUE) or may not (FALSE) exist in the plain text file you are reading in. * in the case of read.table() it is header = FALSE (by default, it assumes you do not have column names) * whereas for read.csv() it is header = TRUE (by default, it assumes that all your columns have names listed).\nThe take-home from the “Usage” section for read.csv() is that it has one mandatory argument, the path to the file and filename in quotations.\n\n\n10.1.0.1 Note on stringsAsFactors\nNote that the read.table {utils} family of functions has an argument called stringsAsFactors, which by default will take the value of default.stringsAsFactors().\nType out default.stringsAsFactors() in the console to check what the default value is for your current R session. Is it TRUE or FALSE?\nIf default.stringsAsFactors() is set to TRUE, then stringsAsFactors = TRUE. In that case any function in this family of functions will coerce character columns in the data you are reading in to factor columns (i.e. coerce from vector to factor) in the resulting data frame.\nIf you want to maintain the character vector data structure (e.g. for gene names), you will want to make sure that stringsAsFactors = FALSE (or that default.stringsAsFactors() is set to FALSE).\n\n\n10.1.1 List of functions for data inspection\nWe already saw how the functions head() and str() (in the releveling section) can be useful to check the content and the structure of a data.frame. Below is a non-exhaustive list of functions to get a sense of the content/structure of data. The list has been divided into functions that work on all types of objects, some that work only on vectors/factors (1 dimensional objects), and others that work on data frames and matrices (2 dimensional objects).\nWe have some exercises below that will allow you to gain more familiarity with these. You will definitely be using some of them in the next few homework sections.\n\nAll data structures - content display:\n\nstr(): compact display of data contents (similar to what you see in the Global environment)\nclass(): displays the data type for vectors (e.g. character, numeric, etc.) and data structure for dataframes, matrices, lists\nsummary(): detailed display of the contents of a given object, including descriptive statistics, frequencies\nhead(): prints the first 6 entries (elements for 1-D objects, rows for 2-D objects)\ntail(): prints the last 6 entries (elements for 1-D objects, rows for 2-D objects)\n\nVector and factor variables:\n\nlength(): returns the number of elements in a vector or factor\n\nDataframe and matrix variables:\n\ndim(): returns dimensions of the dataset (number_of_rows, number_of_columns) [Note, row numbers will always be displayed before column numbers in R]\nnrow(): returns the number of rows in the dataset\nncol(): returns the number of columns in the dataset\nrownames(): returns the row names in the dataset\ncolnames(): returns the column names in the dataset\n\n\nExercises\n\nRead the tab-delimited project-summary.txt file in the data folder it in to R using read.table() and store it as the variable proj_summary. As you use read.table(), keep in mind that:\n\nall the columns in the input text file have column names\nyou want the first column of the text file to be used as row names (hint: look up the input for the row.names = argument in read.table())\n\nDisplay the contents of proj_summary in your console\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/pvals.html#interpreting-p-values",
    "href": "session2/pvals.html#interpreting-p-values",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.1 Interpreting p values",
    "text": "11.1 Interpreting p values\nLet’s start by checking our understanding of a p value.\n\n\n\n\n\n\nAre these statements correct or incorrect interpretations of p values?\n\nWe can use the quantity \\(1-p\\) to represent the probability that the alternative hypothesis is true.\nA p value can let us know how incompatible an observation is with a specified statistical model.\nA p value tells us how likely we would be to randomly see the observed value with minimal assumptions.\nA p value indicates an important result."
  },
  {
    "objectID": "session2/pvals.html#p-value-hacking",
    "href": "session2/pvals.html#p-value-hacking",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.2 P-value hacking",
    "text": "11.2 P-value hacking\nLet’s go back to the coin tossing example. We did not reject the null hypothesis (that the coin is fair) at a level of 5%—even though we “knew” that it is unfair. After all, probHead was chosen as 0.6. Let’s suppose we now start looking at different test statistics. Perhaps the number of consecutive series of 3 or more heads. Or the number of heads in the first 50 coin flips. And so on. A t some point we will find a test that happens to result in a small p-value, even if just by chance (after all, the probability for the p-value to be less than 0.05 under the null hypothesis—fair coin—is one in twenty).\nThere is a xkcd comic which illustrates this issue in the context of selective reporting. We just did what is called p-value hacking. You see what the problem is: in our zeal to prove our point we tortured the data until some statistic did what we wanted. A related tactic is hypothesis switching or HARKing – hypothesizing after the results are known: we have a dataset, maybe we have invested a lot of time and money into assembling it, so we need results. We come up with lots of different null hypotheses and test statistics, test them, and iterate, until we can report something.\n\n\n\nLet’s try running our binomial test on a fair coin, and see what we get:\n\nnumFlips = 100\nprobHead = 0.5\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nnumHeads <- sum(coinFlips == \"H\")\npval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\npval\n\n[1] 0.4841184\n\n\nThis p value is probably relatively large. But what if we keep on repeating the experiment?\n\n#Let's make a function for performing our experiment\nflip_coin <- function(numFlips, probHead){\n  numFlips = 100\n  probHead = 0.50\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n    replace = TRUE, prob = c(probHead, 1 - probHead))\n  numHeads <- sum(coinFlips == \"H\")\n  pval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\n  return(pval)\n}\n\n#And then run it 10000 times\nparray <- replicate(10000, flip_coin(1000, 0.5), simplify=TRUE)\nhist(parray, breaks=100)\n\n\n\nmin(parray)\n\n[1] 3.216002e-05"
  },
  {
    "objectID": "session2/pvals.html#the-multiple-testing-problem",
    "href": "session2/pvals.html#the-multiple-testing-problem",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.3 The Multiple Testing Problem",
    "text": "11.3 The Multiple Testing Problem\nIn modern biology, we are often conducting hundreds or thousands of statistical tests on high-throughput data. This means that even a low false positive rate can cause there to be a large number of cases where we falsely reject the null hypothesis. Luckily, there are ways we can correct our rejection threshold or p values to limit the type I error."
  },
  {
    "objectID": "session2/mult.html#definitions",
    "href": "session2/mult.html#definitions",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.1 Definitions",
    "text": "12.1 Definitions\nLet’s redefine our error table from earlier, in the framework of multiple hypotheses. Thus, each of the following variables represents a count out of the total number of tests performed.\n\n\n\nTest vs reality\nNull is true\nNull is false\nTotal\n\n\n\n\nRejected\n\\(V\\)\n\\(S\\)\n\\(R\\)\n\n\nNot Rejected\n\\(U\\)\n\\(T\\)\n\\(m - R\\)\n\n\nTotal\n\\(m_0\\)\n\\(m-m_0\\)\n\\(m\\)\n\n\n\n\n\\(m\\): total number of tests (and null hypotheses)\n\\(m_0\\): number of true null hypotheses\n\\(m - m_0\\): number of false null hypotheses\n\\(V\\): number of false positives (a measure of type I error)\n\\(T\\): number of false negatives (a measure of type II error)\n\\(S, U\\): number of true positives and true negatives\n\\(R\\): number of rejections"
  },
  {
    "objectID": "session2/mult.html#family-wise-error-rate",
    "href": "session2/mult.html#family-wise-error-rate",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.2 Family wise error rate",
    "text": "12.2 Family wise error rate\nThe family wise error rate (FWER) is the probability that $V>0$, i.e., that we make one or more false positive errors.\nWe can compute it as the complement of making no false positive errors at all. Recall that \\(\\alpha\\) is our probability threshold for rejecting the null hypothesis.\n\\[\nP(V>0) = 1 - P(V=0) = 1 - (1-\\alpha)^{m_0}\n\\]\nNote that, as \\(m_0\\) approaches \\(\\infty\\), the FWER approaches 1. In other words, with enough tests we are guaranteed to have at least 1 false positive."
  },
  {
    "objectID": "session2/mult.html#bonferroni-method",
    "href": "session2/mult.html#bonferroni-method",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.3 Bonferroni method",
    "text": "12.3 Bonferroni method\nThe Bonferroni method uses the FWER to adjust \\(\\alpha\\) such that we can choose a false positive rate across all tests. In other words, to control the FWER to the level \\(\\alpha_{FWER}\\) a new threshold is chosen, \\(\\alpha = \\alpha_{FWER}/m\\).\nThis means that, for \\(10000\\) tests, to set \\(alpha_{FWER} = 0.05\\) our new p value threshold for individual tests would be \\(5 \\times 10{-6}\\). Often FWER control is too conservative, and would lead to an ineffective use of the time and money that was spent to generate and assemble the data."
  },
  {
    "objectID": "session2/mult.html#false-discovery-rate",
    "href": "session2/mult.html#false-discovery-rate",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.4 False discovery rate",
    "text": "12.4 False discovery rate\nThe false discovery rate takes a more relaxed approach than Bonferroni correction. Instead of trying to have no or a fixed total rate of false positives, what if we allowed a small proportion of our null hypothesis rejections to be false positives?\nIt uses the total number of null hypotheses rejected to inform what is an acceptable number of false positive errors to let through. It makes the claim that, for instance, making \\(4\\) type I errors out of \\(10\\) rejected null hypotheses is a worse error than making \\(20\\) type I errors out of \\(100\\) rejected null hypotheses.\nTo see an example, we will load up the RNA-Seq dataset airway, which contains gene expression measurements (gene-level counts) of four primary human airway smooth muscle cell lines with and without treatment with dexamethasone, a synthetic glucocorticoid.\nConceptually, the tested null hypothesis is similar to that of the t-test, although the details are slightly more involved since we are dealing with count data.\n\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\nlibrary(\"tidyverse\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\n# This next line filters out NA p values from the dataset\nawde = as.data.frame(results(aw)) |> dplyr::filter(!is.na(pvalue))\n\nIn this dataset, we have performed a statistical test for each of \\(33,469\\) measured genes. We can look at a histogram of the p values:\n\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)\n\n\n\n\nLet’s say we reject the null hypothesis for all p values less than \\(\\alpha\\). We can see how many null hypotheses we reject:\n\nalpha <- 0.025\n\n# Recall that TRUE and FALSE are stored as 0 and 1, so we can sum to get a count\nsum(awde$pvalue <= alpha)\n\n[1] 4772\n\n\nAnd we can estimate \\(V\\), how many false positives we have:\n\nalpha * nrow(awde)\n\n[1] 836.725\n\n\nWe can then estimate the fraction of false rejections as:\n\n(alpha * nrow(awde))/sum(awde$pvalue <= alpha)\n\n[1] 0.1753405\n\n\nFormally, the false discovery rate (FDR) is defined as: \\[\nFDR = E\\left[\\frac{V}{max(R,1)}\\right]\n\\] Which is the average proportion of rejections that are false rejections."
  },
  {
    "objectID": "session2/mult.html#the-benjamini-hochberg-algorithm-for-controlling-the-fdr",
    "href": "session2/mult.html#the-benjamini-hochberg-algorithm-for-controlling-the-fdr",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.5 The Benjamini-Hochberg algorithm for controlling the FDR",
    "text": "12.5 The Benjamini-Hochberg algorithm for controlling the FDR\nThe Benjamini-Hochberg algorithm controls for a chosen FDR threshold via the following steps:\n\nFirst, order the p values in increasing order, \\(p_{(1)}...p_{(m)}\\)\nThen for some choice of the target FDR, \\(\\varphi\\), find the largest value of \\(k\\) that satisfies \\(p_{(k)} < \\varphi k/m\\)\nReject hypotheses \\(1\\) through \\(k\\)\n\nWe can see how this procedure works when applied to our RNA-Seq p value distribution:\n\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")\n\n\n\n\nWe find the rightmost point where our p-values and the expected null false discoveries intersect, then reject all tests to the left."
  },
  {
    "objectID": "session2/mult.html#multiple-hypothesis-correction-in-r",
    "href": "session2/mult.html#multiple-hypothesis-correction-in-r",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.6 Multiple Hypothesis Correction in R",
    "text": "12.6 Multiple Hypothesis Correction in R\nWe can use Bonferroni correction or the Benjamini-Hochberg algorithm using the function p.adjust.\n\np.adjust(awde$pvalue, method=\"bonferroni\")\np.adjust(awde$pvalue, method=\"BH\")"
  },
  {
    "objectID": "session2/making_functions.html#functions-and-their-arguments",
    "href": "session2/making_functions.html#functions-and-their-arguments",
    "title": "13  Functions",
    "section": "13.1 Functions and their arguments",
    "text": "13.1 Functions and their arguments\n\n13.1.1 What are functions?\nA key feature of R is functions. Functions are “self contained” modules of code that accomplish a specific task. Functions usually take in some sort of data structure (value, vector, dataframe etc.), process it, and return a result.\nThe general usage for a function is the name of the function followed by parentheses:\nfunction_name(input)\nThe input(s) are called arguments, which can include:\n\nthe physical object (any data structure) on which the function carries out a task\nspecifications that alter the way the function operates (e.g. options)\n\nNot all functions take arguments, for example:\ngetwd()\nHowever, most functions can take several arguments. If you don’t specify a required argument when calling the function, you will either receive an error or the function will fall back on using a default.\nThe defaults represent standard values that the author of the function specified as being “good enough in standard cases”. An example would be what symbol to use in a plot. However, if you want something specific, simply change the argument yourself with a value of your choice.\n\n\n13.1.2 Basic functions\nWe have already used a few examples of basic functions in the previous lessons i.e getwd(), c(), and factor(). These functions are available as part of R’s built in capabilities, and we will explore a few more of these base functions below.\nLet’s revisit a function that we have used previously to combine data c() into vectors. The arguments it takes is a collection of numbers, characters or strings (separated by a comma). The c() function performs the task of combining the numbers or characters into a single vector. You can also use the function to add elements to an existing vector:\n\nglengths <- c(4.6, 3000, 50000)\nglengths <- c(glengths, 90) # adding at the end \nglengths <- c(30, glengths) # adding at the beginning\n\nWhat happens here is that we take the original vector glengths (containing three elements), and we are adding another item to either end. We can do this over and over again to build a vector or a dataset.\nSince R is used for statistical computing, many of the base functions involve mathematical operations. One example would be the function sqrt(). The input/argument must be a number, and the output is the square root of that number. Let’s try finding the square root of 81:\n\nsqrt(81)\n\n[1] 9\n\n\nNow what would happen if we called the function (e.g. ran the function), on a vector of values instead of a single value?\n\nsqrt(glengths)\n\n[1]   5.477226   2.144761  54.772256 223.606798   9.486833\n\n\nIn this case the task was performed on each individual value of the vector glengths and the respective results were displayed.\nLet’s try another function, this time using one that we can change some of the options (arguments that change the behavior of the function), for example round:\n\nround(3.14159)\n\n[1] 3\n\n\nWe can see that we get 3. That’s because the default is to round to the nearest whole number. What if we want a different number of significant digits? Let’s first learn how to find available arguments for a function.\n\n\n13.1.3 Seeking help on arguments for functions\nThe best way of finding out this information is to use the ? followed by the name of the function. Doing this will open up the help manual in the bottom right panel of RStudio that will provide a description of the function, usage, arguments, details, and examples:\n?round\nAlternatively, if you are familiar with the function but just need to remind yourself of the names of the arguments, you can use:\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nEven more useful is the example() function. This will allow you to run the examples section from the Online Help to see exactly how it works when executing the commands. Let’s try that for round():\n\nexample(\"round\")\n\n\nround> round(.5 + -2:4) # IEEE / IEC rounding: -2  0  0  2  2  4  4\n[1] -2  0  0  2  2  4  4\n\nround> ## (this is *good* behaviour -- do *NOT* report it as bug !)\nround> \nround> ( x1 <- seq(-2, 4, by = .5) )\n [1] -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0\n\nround> round(x1) #-- IEEE / IEC rounding !\n [1] -2 -2 -1  0  0  0  1  2  2  2  3  4  4\n\nround> x1[trunc(x1) != floor(x1)]\n[1] -1.5 -0.5\n\nround> x1[round(x1) != floor(x1 + .5)]\n[1] -1.5  0.5  2.5\n\nround> (non.int <- ceiling(x1) != floor(x1))\n [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[13] FALSE\n\nround> x2 <- pi * 100^(-1:3)\n\nround> round(x2, 3)\n[1]       0.031       3.142     314.159   31415.927 3141592.654\n\nround> signif(x2, 3)\n[1] 3.14e-02 3.14e+00 3.14e+02 3.14e+04 3.14e+06\n\n\nIn our example, we can change the number of digits returned by adding an argument. We can type digits=2 or however many we may want:\n\nround(3.14159, digits=2)\n\n[1] 3.14\n\n\n\nNOTE: If you provide the arguments in the exact same order as they are defined (in the help manual) you don’t have to name them:\nround(3.14159, 2)\nHowever, it’s usually not recommended practice because it involves a lot of memorization. In addition, it makes your code difficult to read for your future self and others, especially if your code includes functions that are not commonly used. (It’s however OK to not include the names of the arguments for basic functions like mean, min, etc…). Another advantage of naming arguments, is that the order doesn’t matter. This is useful when a function has many arguments.\n\nExercise\n\n\n\n\n\n\nBasic\n\n\n\n\nLet’s use base R function to calculate mean value of the glengths vector. You might need to search online to find what function can perform this task.\nCreate a new vector test <- c(1, NA, 2, 3, NA, 4). Use the same base R function from exercise 1 (with addition of proper argument), and calculate mean value of the test vector. The output should be 2.5. > NOTE: In R, missing values are represented by the symbol NA (not available). It’s a way to make sure that users know they have missing data, and make a conscious decision on how to deal with it. There are ways to ignore NA during statistical calculation, or to remove NA from the vector. If you want more information related to missing data or NA you can go to this page (please note that there are many advanced concepts on that page that have not been covered in class).\nAnother commonly used base function is sort(). Use this function to sort the glengths vector in descending order.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Setup\nglengths <- c(4.6, 3000, 50000)\nglengths <- c(glengths, 90) # adding at the end \nglengths <- c(30, glengths) # adding at the beginning\n\n# Basic \n# 1\nmean(glengths)\n\n[1] 10624.92\n\n# 2\ntest <- c(1, NA, 2, 3, NA, 4)\nmean(test, na.rm=TRUE)\n\n[1] 2.5\n\n# 3\nsort(glengths, decreasing = TRUE)\n\n[1] 50000.0  3000.0    90.0    30.0     4.6\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\n\nUse rnorm and the matrix functions to create a random square matrix with \\(6\\) rows/columns.\nCalculate the mean of each row in the matrix, so you should have 6 means total.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# We need to sample a length 36 vector, then coerce it into a matrix\nmy_matrix <- matrix(rnorm(36), nrow=6)\n\n# There's a built-in function called rowMeans! It's always good to look things up. \nrowMeans(my_matrix)\n\n[1] -0.1813913 -0.5053032  0.3654681 -0.7635631  0.1275227 -0.6549711\n\n# We could also use apply to call mean on each row of the matrix\napply(my_matrix, 1, mean)\n\n[1] -0.1813913 -0.5053032  0.3654681 -0.7635631  0.1275227 -0.6549711\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nCreate vector c_data <- c(1, NA, 2, 3, NA, 4, 4, 3, 2, NA, NA, 2, 4, 2, 3, 4, 4, 2, 1, NA, 1, 1, 1). Fill in the NA values with the mean of all non-missing values.\nRe-create the vector with its NAs. Instead of filling in the missing data with the mean, estimate the parameter of a Poisson distribution from the data and sample from it to fill in the missing data.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# 1\nc_data <- c(1, NA, 2, 3, NA, 4, 4, 3, 2, NA, NA, 2, 4, 2, 3, 4, 4, 2, 1, NA, 1, 1, 1)\nc_data[is.na(c_data)] <- mean(c_data, na.rm = TRUE)\n\n# 2\nc_data <- c(1, NA, 2, 3, NA, 4, 4, 3, 2, NA, NA, 2, 4, 2, 3, 4, 4, 2, 1, NA, 1, 1, 1)\n\n# We need this to calculate how many numbers we need to sample\nnum_na <- sum(is.na(c_data)) \n# A poisson distribution is paramaterized by it's mean. \n# so we just need the mean of the data to model\nnew_vals <- rpois(num_na, mean(c_data, na.rm = TRUE))\n# And finally, we can index the data to set the sampled values equal to it\nc_data[is.na(c_data)] <- new_vals\n\n\n\n\n\n\n\n13.1.4 User-defined Functions\nOne of the great strengths of R is the user’s ability to add functions. Sometimes there is a small task (or series of tasks) you need done and you find yourself having to repeat it multiple times. In these types of situations, it can be helpful to create your own custom function. The structure of a function is given below:\nname_of_function <- function(argument1, argument2) {\n    statements or code that does something\n    return(something)\n}\n\nFirst you give your function a name.\nThen you assign value to it, where the value is the function.\n\nWhen defining the function you will want to provide the list of arguments required (inputs and/or options to modify behaviour of the function), and wrapped between curly brackets place the tasks that are being executed on/using those arguments. The argument(s) can be any type of object (like a scalar, a matrix, a dataframe, a vector, a logical, etc), and it’s not necessary to define what it is in any way.\nFinally, you can “return” the value of the object from the function, meaning pass the value of it into the global environment. The important idea behind functions is that objects that are created within the function are local to the environment of the function – they don’t exist outside of the function.\nLet’s try creating a simple example function. This function will take in a numeric value as input, and return the squared value.\n\nsquare_it <- function(x) {\n    square <- x * x\n    return(square)\n}\n\nOnce you run the code, you should see a function named square_it in the Environment panel (located at the top right of Rstudio interface). Now, we can use this function as any other base R functions. We type out the name of the function, and inside the parentheses we provide a numeric value x:\n\nsquare_it(5)\n\n[1] 25\n\n\nPretty simple, right? In this case, we only had one line of code that was run, but in theory you could have many lines of code to get obtain the final results that you want to “return” to the user.\n\n13.1.4.1 Do I always have to return() something at the end of the function?\nIn the example above, we created a new variable called square inside the function, and then return the value of square. If you don’t use return(), by default R will return the value of the last line of code inside that function. That is to say, the following function will also work.\n\nsquare_it <- function(x) {\n   x * x\n}\n\nHowever, we recommend always using return at the end of a function as the best practice.\n\nWe have only scratched the surface here when it comes to creating functions! We will revisit this in later lessons, but if interested you can also find more detailed information on this R-bloggers site, which is where we adapted this example from.\nExercise\n\n\n\n\n\n\nBasic\n\n\n\n\nLet’s create a function temp_conv(), which converts the temperature in Fahrenheit (input) to the temperature in Kelvin (output).\n\nWe could perform a two-step calculation: first convert from Fahrenheit to Celsius, and then convert from Celsius to Kelvin.\nThe formula for these two calculations are as follows: temp_c = (temp_f - 32) * 5 / 9; temp_k = temp_c + 273.15. To test your function,\nif your input is 70, the result of temp_conv(70) should be 294.2611.\n\nNow we want to round the temperature in Kelvin (output of temp_conv()) to a single decimal place. Use the round() function with the newly-created temp_conv() function to achieve this in one line of code. If your input is 70, the output should now be 294.3.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Basic\n\n# 1\ntemp_conv <- function(temp_f) {\n  temp_c = (temp_f - 32) * 5 / 9\n  temp_k = temp_c + 273.15\n  return (temp_k)\n}\n\n# 2\nround(temp_conv(70), digits = 1)\n\n[1] 294.3\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nThe Fibonacci sequence is \\(0, 1, 1, 2, 3, 5, 8, ...\\) where the first two terms are 0 and 1, and for all other terms \\(n^{th}\\) term is the sum of the \\((n-1)^{th}\\) and \\((n-2)^{th}\\) terms. Note that for n=0 you should return 0 and for n=1 you should return 1 as the first 2 terms.\n\nWrite a function fibonacci which takes in a single integer argument n and returns the \\(n^{th}\\) term in the Fibonacci sequence.\nHave your function stop with an appropriate message if the argument n is not an integer. Stop allows you to create your own errors in R. This StackOverflow thread contains useful information on how to tell if something is or is not an integer in R.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Advanced\nfibonacci <- function(n){\n  \n  # These next 3 lines are part 2\n  if((n %% 1)!=0){\n    stop(\"Must provide an integer to fibonacci\")\n  }\n  fibs <- c(0,1)\n  for (i in 2:n){\n    fibs <- c(fibs, fibs[i-1]+fibs[i])\n  }\n  return(fibs[n+1])\n}\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRe-write your fibonacci function so that it calculates the Fibonacci sequence recursively, meaning that it calls itself. Your function should contain no loops or iterative code.\nYou will need to define two base cases, where the function does not call itself.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Challenge\nfibonacci2 <- function(n){\n  if((n %% 1)!=0){\n    stop(\"Must provide an integer to fibonacci\")\n  }\n  # We call these two if statement the 'base cases' of the recursion\n  if (n==0){\n    return(0)\n  }\n  if (n==1){\n    return(1)\n  }\n  # And this is the recursive case, where the function calls itself\n  return(fibonacci2(n-1)+fibonacci2(n-2))\n}\n\nRecursion isn’t relevant to most data analysis, as it is often significantly slower than a non-recursive solution in most programming languages.\nHowever, setting up a solution as recursive sometimes allows us to perform an algorithmic strategy called dynamic programming and is fundamental to most sequence alignment algorithms.\n\n\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/session2problems.html",
    "href": "session2/session2problems.html",
    "title": "14  Practice Exercises",
    "section": "",
    "text": "Basic\n\n\n\nIn a spreadsheet editor like excel or Google sheets, open the file ../data/messy_temperature_data.csv.\n\nWhat problems will arise when we load this data into R? If you’re unsure, try it out and take a look at the data. Are the columns the types you expected? Does the data appear correct?\nInside your spreadsheet editor of choice, fix the problems with the data. Save it under a new file name in your data folder (so that the original data file is not overwritten).\nLoad the dataset into R.\nWhat are the dimensions of the dataset? How rows and columns does it have?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nTry loading the dataset ../data/corrupted_data.txt. Take a look at the the gene symbols. Some of the gene symbols appear to be dates! This is actually a common problem in biology.\nTry installing the HGCNhelper package and using it to correct the date-converted gene symbols.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nAs opposed to manually fixing the problems with the dataset from the basic exercise, try to fix the dataset problems using R.\n\n\n\nWorking with distributions\n\n\n\n\n\n\n\nBasic\n\n\n\nGenerate 100 instances of a Poisson(3) random variable.\n\nWhat is the mean?\nWhat is the variance as computed by the R function var?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Basic\npVars <- rpois(100,3)\nmean(pVars)\n\n[1] 2.96\n\nvar(pVars)\n\n[1] 3.170101\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nConduct a binomial test for the following scenario: out of 1 million reads, 19 reads are mapped to a gene of interest, with the probability for mapping a read to that gene being \\(10^{-5}\\).\n\nAre these more or less reads than we would expect to be mapped to that gene?\nIs the finding statistically significant?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Advanced\n# Let's check our intuition\ntable(rbinom(100000, n=1e6, p=1e-6))\n\n\n     0      1      2      3      4 \n905672  89698   4474    151      5 \n\n# Let's run the test\nbinom.test(x = 19, n = 1e6, p = 1e-6)\n\n\n    Exact binomial test\n\ndata:  19 and 1e+06\nnumber of successes = 19, number of trials = 1e+06, p-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 1e-06\n95 percent confidence interval:\n 1.143928e-05 2.967070e-05\nsample estimates:\nprobability of success \n               1.9e-05 \n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a function, bh_correction, which takes in a vector of p-values and a target FDR, performs the Benjamini-Hochberg procedure, and returns a vector of p-values which should be rejected at that FDR.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Challenge\n\nbh_correction <- function(pvals, phi){\n  pvals <- sort(pvals)\n  m <- length(pvals)\n  k <- 1\n  test_val <- phi/m\n  while((test_val>pvals[k]) && (k<m)){\n    k <- k+1\n    test_val <- (phi*k)/m\n  }\n  return(pvals[1:k])\n}\n\n# Let's test the solution\nx <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\npvals <- 2*pnorm(sort(-abs(x)))\nbh_correction(pvals,0.05)\n\n [1] 3.958492e-06 1.693070e-05 3.156727e-05 3.269199e-05 5.556655e-05\n [6] 7.146096e-05 7.457843e-05 1.429892e-04 1.653987e-04 3.381682e-04\n[11] 4.589747e-04 7.340335e-04 7.566873e-04 1.881698e-03 2.172266e-03\n[16] 2.812296e-03 5.873386e-03 8.222307e-03 1.322639e-02 1.444331e-02\n[21] 2.165735e-02"
  },
  {
    "objectID": "session2/pSet2.html#problem-1",
    "href": "session2/pSet2.html#problem-1",
    "title": "15  Problem Set 2",
    "section": "15.1 Problem 1",
    "text": "15.1 Problem 1\nWrite a function to compute the probability of having a maximum as big as m when looking across n Poisson variables with rate lambda. Give these arguments default values in your function declaration.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmaxPois <- function(m = 8, n = 100, lambda = 0.5){\n  1 - ppois(m-1, lambda)^n\n}\nmaxPois()\n\n[1] 6.219672e-06"
  },
  {
    "objectID": "session2/pSet2.html#problem-2",
    "href": "session2/pSet2.html#problem-2",
    "title": "15  Problem Set 2",
    "section": "15.2 Problem 2",
    "text": "15.2 Problem 2\nLet’s answer a question about C. elegans genome nucleotide frequency: Is the mitochondrial sequence of C. elegans consistent with a model of equally likely nucleotides?\nSetup: This is our opportunity to use Bioconductor for the first time. Since Bioconductor’s package management is more tightly controlled than CRAN’s, we need to use a special install function (from the BiocManager package) to install Bioconductor packages.\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"Biostrings\", \"BSgenome.Celegans.UCSC.ce2\"))\n\nAfter that, we can load the genome sequence package as we load any other R packages.\n\nlibrary(\"BSgenome.Celegans.UCSC.ce2\",quietly = TRUE )\n\nWarning: package 'BSgenome' was built under R version 4.2.2\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n    table, tapply, union, unique, unsplit, which.max, which.min\n\n\nWarning: package 'S4Vectors' was built under R version 4.2.2\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following object is masked from 'package:grDevices':\n\n    windows\n\n\nWarning: package 'GenomeInfoDb' was built under R version 4.2.2\n\n\nWarning: package 'GenomicRanges' was built under R version 4.2.2\n\n\n\nAttaching package: 'Biostrings'\n\n\nThe following object is masked from 'package:base':\n\n    strsplit\n\nCelegans\n\nWorm genome:\n# organism: Caenorhabditis elegans (Worm)\n# genome: ce2\n# provider: UCSC\n# release date: Mar. 2004\n# 7 sequences:\n#   chrI   chrII  chrIII chrIV  chrV   chrX   chrM                              \n# (use 'seqnames()' to see all the sequence names, use the '$' or '[[' operator\n# to access a given sequence)\n\nseqnames(Celegans)\n\n[1] \"chrI\"   \"chrII\"  \"chrIII\" \"chrIV\"  \"chrV\"   \"chrX\"   \"chrM\"  \n\nCelegans$chrM\n\n13794-letter DNAString object\nseq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\n\nclass(Celegans$chrM)\n\n[1] \"DNAString\"\nattr(,\"package\")\n[1] \"Biostrings\"\n\n\nWe can take advantage of the Biostrings library to get base counts:\n\nlibrary(\"Biostrings\", quietly = TRUE)\nlfM = letterFrequency(Celegans$chrM, letters=c(\"A\", \"C\", \"G\", \"T\"))\nlfM\n\n   A    C    G    T \n4335 1225 2055 6179 \n\n\nTest whether the C. elegans data is consistent with the uniform model (all nucleotide frequencies the same) using a simulation. For the purposes of this simulation, we can assume that all base pairs are independent from each other. Your solution should compute a simulated p-value based on \\(10,000\\) simulations.\nHint: The multinomial distribution is similar to the binomial distribution but can model experiments with more than \\(2\\) outcomes. For instance suppose we have 8 characters of four different, equally likely types:\n\npvec = rep(1/4, 4)\nt(rmultinom(1, prob = pvec, size = 8))\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    1    2    0\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know that, for equal frequencies, we would expect each nucleotide to have an equal count.\nThere are a few ways we could imagine explaining how different a set of counts is from a multinomial output. One way is to define a single test statistic which is the sum of the square difference in expected counts and real counts, scaled by the number of expected counts. This function calculates this sum based on the observed (o) and expected (e) counts.\n\nbases_stat = function(o, e) {\n  sum((o-e)^2 / e)\n}\nobs = bases_stat(o = lfM, e = length(Celegans$chrM) / 4)\nobs\n\n[1] 4386.634\n\n\nThis is essentially the average percent difference in our counts from the expected counts, squared so that we do not need to worry about positive versus negative differences.\n\nB = 10000\nn = length(Celegans$chrM)\nexpected = rep(n / 4, 4)\noenull = replicate(B, bases_stat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))\nobserved <- bases_stat(lfM, expected)\nmax(oenull)\n\n[1] 25.84863\n\nsim_p <- sum(oenull > observed)/B\nsim_p\n\n[1] 0"
  },
  {
    "objectID": "session2/pSet2.html#problem-3",
    "href": "session2/pSet2.html#problem-3",
    "title": "15  Problem Set 2",
    "section": "15.3 Problem 3",
    "text": "15.3 Problem 3\nInstead of testing across the entire mitochondria, let’s now see if we can find certain nucleotides being enriched locally. To do this, split up the mitochondrial sequence into 100 base pair chunks, and perform your test from problem \\(3\\) on each chunk. Perform a multiple hypothesis correction at an FDR of \\(0.01\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst we need to split the chromosome into 100bp chunks. We can use the substring and seq functions to do this.\n\nchunks <- substring(as.character(Celegans$chrM), seq(1, n, 100), seq(100, n, 100))\n\n#We get an empty string as the last chunk, remove it\nchunks <- chunks[-length(chunks)]\n\nNow we define a function with the test we performed above.\n\nuniform_test <- function(seq_int){\n  B <- 10000\n  seq_int <- DNAString(seq_int)\n  n <- length(seq_int)\n  #We need to remake the chunk into a biostrings DNAString object\n  lfM <- letterFrequency(seq_int, letters=c(\"A\", \"C\", \"G\", \"T\"))\n  expected <- rep(n / 4, 4)\n  oenull <- replicate(B, bases_stat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))\n  observed <- bases_stat(lfM, expected)\n  sim_p <- sum(oenull > observed)/B\n  return(sim_p)\n}\n\nFinally we apply the test to all the chunks and correct the simulation-derived p values.\n\nresult <- sapply(chunks, uniform_test)\nresult <- p.adjust(result, method = \"fdr\")\nhead(sort(result, decreasing = TRUE))\n\nCTTTTAACAGCTTTTACTAAAAGAGCACAATTTCCATTTAGATCTTGGTTACCCAAAGCTATAAGAGCCCCCACACCGGTGAGGTCTTTGGTTCATAGTA \n                                                                                         0.184200000 \nAATTTTAGATGCTTGTTTTGTAGATATAGGTTGTGGGACTAGGTGAACAGTCTACCCACCTTTAAGAACAATGGGGCACCCTGGAAGTAGAGTAGATTTA \n                                                                                         0.069608088 \nTTTTATTGTTCAAAGAATCGCTTTTATTACTCTATATGAGCGTCATTTATTGGGAAGAAGACAAAATCGTCTAGGGCCCACCAAGGTTACATTTATGGGA \n                                                                                         0.036330370 \nGCTTTATATTAAAGCTGGCTTCTGCCCTATGATATTTAAATGGCAGTCTTAGCGTGAGGACATTAAGGTAGCAAAATAATTTGTGCTTTTATTGAGTTCC \n                                                                                         0.016562687 \nTAATTATAGTAATTGCTGAACTTAACCGGGCGCCATTTGATTTTTCTGAAGGTGAAAGGGAGTTAGTTAGAGGATTTAATGTGGAGTTTGCCAGAGTAGC \n                                                                                         0.010506767 \nAAGAGCAGGAGTAAAGTTGTATTTAAACTGAAAAGATATTGGCAGACATTCTAAATTATCTTTGGAGGCTGAGTAGTAACTGAGAACCCTCATTAACTAC \n                                                                                         0.009548485 \n\n\n\n\n\n\nThe materials in this lesson have been adapted from: Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session3/session3.html#learning-objectives",
    "href": "session3/session3.html#learning-objectives",
    "title": "Session 3: Data Wrangling",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSubset vectors based on logical conditions.\nSubset dataframes by columns and rows.\nCreate, modify, and subset lists.\nDefine normalization and identify its uses.\nUse the %in% operator and match function to select corresponding data between different objects.\nUse the match function to reorder corresponding data between different objects.\nExplore alternatives to base subsetting and matching methods available in the Tidyverse package suite."
  },
  {
    "objectID": "session3/count_data1.html#terminology",
    "href": "session3/count_data1.html#terminology",
    "title": "16  Count Data",
    "section": "16.1 Terminology",
    "text": "16.1 Terminology\nLet’s define some terminology related to count data.\n\nA sequencing library is the collection of DNA molecules used as input for the sequencing machine. Note that library size can either mean the total number of reads that were sequenced in the run or the total number of mapped reads.\nFragments are the molecules being sequenced. Since the currently most widely used technology1 can only deal with molecules of length around 300–1000 nucleotides, these are obtained by fragmenting the (generally longer) DNA or cDNA molecules of interest.\nA read is the sequence obtained from a fragment. With the current technology, the read covers not the whole fragment, but only one or both ends of it, and the read length on either side is up to around 150 nucleotides.\n\nWe can load in an example of some count data from the data package pasilla.\n\nlibrary(pasilla)\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))\n\nHow would we check the dimension of counts and preview its contents?"
  },
  {
    "objectID": "session3/count_data1.html#challenges-with-count-data",
    "href": "session3/count_data1.html#challenges-with-count-data",
    "title": "16  Count Data",
    "section": "16.2 Challenges with count data",
    "text": "16.2 Challenges with count data\nWhat are the challenges that we need to overcome with such count data?\n\nThe data have a large dynamic range, starting from zero up to millions. The variance, and more generally, the distribution shape of the data in different parts of the dynamic range are very different. We need to take this phenomenon, called heteroskedasticity, into account.\nThe data are non-negative integers, and their distribution is not symmetric – thus normal or log-normal distribution models may be a poor fit.\nWe need to understand the systematic sampling biases and adjust for them. This is often called normalization, but has a different meaning from other types of normalization. Examples are the total sequencing depth of an experiment (even if the true abundance of a gene in two libraries is the same, we expect different numbers of reads for it depending on the total number of reads sequenced), or differing sampling probabilities (even if the true abundance of two genes within a biological sample is the same, we expect different numbers of reads for them if their biophysical properties differ, such as length, GC content, secondary structure, binding partners)."
  },
  {
    "objectID": "session3/count_data1.html#modeling-count-data",
    "href": "session3/count_data1.html#modeling-count-data",
    "title": "16  Count Data",
    "section": "16.3 Modeling count data",
    "text": "16.3 Modeling count data\nConsider a sequencing library that contains \\(n_1\\) fragments corresponding to gene 1, \\(n_2\\) fragments for gene 2, and so on, with a total library size of \\(n = n_1 + n_2 + ...\\). We submit the library to sequencing and determine the identity of \\(r\\) randomly sampled fragments.\nWe can consider the probability that a given read maps to the \\(i^{th}\\) gene is \\(p_i = n_i \\ n\\), and that this is pretty much independent of the outcomes for all the other reads. So we can model the number of reads for gene by a Poisson distribution, where the rate of the Poisson process is the product of \\(p_i\\), the initial proportion of fragments for the \\(i^{th}\\) gene, times \\(r\\), that is: \\(\\lambda_i = rp_i\\).\nIn practice, we are usually not interested in modeling the read counts within a single library, but in comparing the counts between libraries. That is, we want to know whether any differences that we see between different biological conditions – say, the same cell line with and without drug treatment – are larger than expected “by chance”, i.e., larger than what we may expect even between biological replicates. Empirically, it turns out that replicate experiments vary more than what the Poisson distribution predicts.\nIntuitively, what happens is that \\(p_i\\) and therefore \\(\\lambda_i\\) also vary even between biological replicates; perhaps the temperature at which the cells grew was slightly different, or the amount of drug added varied by a few percent, or the incubation time was slightly longer. To account for that, we need to add another layer of modeling on top. It turns out that the gamma-Poisson (a.k.a. negative binomial) distribution suits our modeling needs. Instead of a single \\(\\lambda\\) which represents both mean and variance, this distribution has two parameters. In principle, these can be different for each gene."
  },
  {
    "objectID": "session3/count_data1.html#normalization",
    "href": "session3/count_data1.html#normalization",
    "title": "16  Count Data",
    "section": "16.4 Normalization",
    "text": "16.4 Normalization\nOften, there are systematic biases that have affected the data generation and are worth taking into account. The term normalization is commonly used for that aspect of the analysis, even though it is misleading: it has nothing to do with the normal distribution; nor does it involve a data transformation. Rather, what we aim to do is identify the nature and magnitude of systematic biases, and take them into account in our model-based analysis of the data.\nThe most important systematic bias stems from variations in the total number of reads in each sample. If we have more reads for one library than in another, then we might assume that, everything else being equal, the counts are proportional to each other. This is true to a point. However, DESeq2 uses a slightly more advanced method of normalizing total number of reads by ignoring genes that appear to be truly up- or down- regulated in some samples, thus only considering ‘control’ genes to calculate a factor for total read size in each sample. We can compare the simple total read count versus DESeq2’s size estimation in the Pasilla data:\n\nlibrary(\"tibble\")\n\nWarning: package 'tibble' was built under R version 4.2.2\n\nlibrary(\"ggplot2\")\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(\"DESeq2\")\nggplot(tibble(\n  `size factor` = estimateSizeFactorsForMatrix(counts),\n  `sum` = colSums(counts)), aes(x = `size factor`, y = `sum`)) +\n  geom_point()\n\n\n\n\nNormalization is often used to account for known biases, such as batch effects accross different samples in many types of analyses. The most classic example of normalization, and thus its name, would be to transform a dataset such that its mean is 0 and its variance is 1, thus matching a normal distribution."
  },
  {
    "objectID": "session3/count_data1.html#log-transformations",
    "href": "session3/count_data1.html#log-transformations",
    "title": "16  Count Data",
    "section": "16.5 Log transformations",
    "text": "16.5 Log transformations\nFor testing for differential expression we operate on raw counts and use discrete distributions. For other downstream analyses – e.g., for visualization or clustering – it might however be useful to work with transformed versions of the count data.\nMaybe the most obvious choice of transformation is the logarithm. However, since count values for a gene can become zero, some advocate the use of pseudocounts, i.e., transformations of the form\n\\[\ny = log_2(n+n_0)\n\\] where \\(n\\) represents the count values and \\(n_0\\) is a somehow chosen positive constant (often just 1)."
  },
  {
    "objectID": "session3/count_data1.html#classes-in-r",
    "href": "session3/count_data1.html#classes-in-r",
    "title": "16  Count Data",
    "section": "16.6 Classes in R",
    "text": "16.6 Classes in R\nLet’s return to the pasilla data. These data are from an experiment on Drosophila melanogaster cell cultures that investigated the effect of RNAi knock-down of the splicing factor pasilla on the cells’ transcriptome. There were two experimental conditions, termed untreated and treated in the header of the count table that we loaded. They correspond to negative control and to siRNA against pasilla. The experimental metadata of the 7 samples in this dataset are provided in a spreadsheet-like table, which we load.\n\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\n\nRows: 7 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): file, condition, type, total number of reads\ndbl (2): number of lanes, exon counts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npasillaSampleAnno\n\n# A tibble: 7 × 6\n  file         condition type        `number of lanes` total number of…¹ exon …²\n  <chr>        <chr>     <chr>                   <dbl> <chr>               <dbl>\n1 treated1fb   treated   single-read                 5 35158667           1.57e7\n2 treated2fb   treated   paired-end                  2 12242535 (x2)      1.56e7\n3 treated3fb   treated   paired-end                  2 12443664 (x2)      1.27e7\n4 untreated1fb untreated single-read                 2 17812866           1.49e7\n5 untreated2fb untreated single-read                 6 34284521           2.08e7\n6 untreated3fb untreated paired-end                  2 10542625 (x2)      1.03e7\n7 untreated4fb untreated paired-end                  2 12214974 (x2)      1.17e7\n# … with abbreviated variable names ¹​`total number of reads`, ²​`exon counts`\n\n\nAs we see here, the overall dataset was produced in two batches, the first one consisting of three sequencing libraries that were subjected to single read sequencing, the second batch consisting of four libraries for which paired end sequencing was used. As so often, we need to do some data wrangling: we replace the hyphens in the type column by underscores, as arithmetic operators in factor levels are discouraged, and convert the type and condition columns into factors, explicitly specifying our prefered order of the levels.\n\nlibrary(\"dplyr\")\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:AnnotationDbi':\n\n    select\n\n\nThe following objects are masked from 'package:GenomicRanges':\n\n    intersect, setdiff, union\n\n\nThe following object is masked from 'package:GenomeInfoDb':\n\n    intersect\n\n\nThe following objects are masked from 'package:IRanges':\n\n    collapse, desc, intersect, setdiff, slice, union\n\n\nThe following objects are masked from 'package:S4Vectors':\n\n    first, intersect, rename, setdiff, setequal, union\n\n\nThe following object is masked from 'package:matrixStats':\n\n    count\n\n\nThe following object is masked from 'package:Biobase':\n\n    combine\n\n\nThe following objects are masked from 'package:BiocGenerics':\n\n    combine, intersect, setdiff, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))\n\nwith(pasillaSampleAnno,\n       table(condition, type))\n\n           type\ncondition   single paired\n  untreated      2      2\n  treated        1      2\n\n\nDESeq2 uses a specialized data container, called DESeqDataSet to store the datasets it works with. Such use of specialized containers – or, in R terminology, classes – is a common principle of the Bioconductor project, as it helps users to keep together related data. While this way of doing things requires users to invest a little more time upfront to understand the classes, compared to just using basic R data types like matrix and dataframe, it helps avoiding bugs due to loss of synchronization between related parts of the data. It also enables the abstraction and encapsulation of common operations that could be quite wordy if always expressed in basic terms. DESeqDataSet is an extension of the class SummarizedExperiment in Bioconductor. The SummarizedExperiment class is also used by many other packages, so learning to work with it will enable you to use quite a range of tools.\nWe use the constructor function DESeqDataSetFromMatrix to create a DESeqDataSet from the count data matrix counts and the sample annotation dataframe pasillaSampleAnno.\n\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\nclass(pasilla)\n\n[1] \"DESeqDataSet\"\nattr(,\"package\")\n[1] \"DESeq2\"\n\n\nThe SummarizedExperiment class – and therefore DESeqDataSet – also contains facilities for storing annotation of the rows of the count matrix. For now, we are content with the gene identifiers from the row names of the counts table.\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session3/wrangling.html#selecting-data-using-indices-and-sequences",
    "href": "session3/wrangling.html#selecting-data-using-indices-and-sequences",
    "title": "17  Data Wrangling",
    "section": "17.1 Selecting data using indices and sequences",
    "text": "17.1 Selecting data using indices and sequences\nWhen analyzing data, we often want to partition the data so that we are only working with selected columns or rows. A data frame or data matrix is simply a collection of vectors combined together. So let’s begin with vectors and how to access different elements, and then extend those concepts to dataframes.\n\n17.1.1 Vectors\n\n17.1.1.1 Selecting using indices\nIf we want to extract one or several values from a vector, we must provide one or several indices using square brackets [ ] syntax. The index represents the element number within a vector (or the compartment number, if you think of the bucket analogy). R indices start at 1. Programming languages like Fortran, MATLAB, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that’s simpler for computers to do.\nLet’s start by creating a vector called age:\n\nage <- c(15, 22, 45, 52, 73, 81)\n\n\n\n\nvector indices\n\n\nSuppose we only wanted the fifth value of this vector, we would use the following syntax:\n\nage[5]\n\n[1] 73\n\n\nIf we wanted all values except the fifth value of this vector, we would use the following:\n\nage[-5]\n\n[1] 15 22 45 52 81\n\n\nIf we wanted to select more than one element we would still use the square bracket syntax, but rather than using a single value we would pass in a vector of several index values:\n\nage[c(3,5,6)]   ## nested\n\n[1] 45 73 81\n\n# OR\n\n## create a vector first then select\nidx <- c(3,5,6) # create vector of the elements of interest\nage[idx]\n\n[1] 45 73 81\n\n\nTo select a sequence of continuous values from a vector, we would use : which is a special function that creates numeric vectors of integer in increasing or decreasing order. Let’s select the first four values from age:\n\nage[1:4]\n\n[1] 15 22 45 52\n\n\nAlternatively, if you wanted the reverse could try 4:1 for instance, and see what is returned.\n\n\n17.1.1.2 Selecting using indices with logical operators\nWe can also use indices with logical operators. Logical operators include greater than (>), less than (<), and equal to (==). A full list of logical operators in R is displayed below:\n\n\n\nOperator\nDescription\n\n\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&\nand\n\n\n|\nor\n\n\n\nWe can use logical expressions to determine whether a particular condition is true or false. For example, let’s use our age vector:\n\nage\n\n[1] 15 22 45 52 73 81\n\n\nIf we wanted to know if each element in our age vector is greater than 50, we could write the following expression:\n\nage > 50\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nReturned is a vector of logical values the same length as age with TRUE and FALSE values indicating whether each element in the vector is greater than 50.\nWe can use these logical vectors to select only the elements in a vector with TRUE values at the same position or index as in the logical vector.\nSelect all values in the age vector over 50 or age less than 18:\n\nage > 50 | age < 18\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n\nage\n\n[1] 15 22 45 52 73 81\n\nage[age > 50 | age < 18]\n\n[1] 15 52 73 81\n\n\n\n17.1.1.2.1 Indexing with logical operators using the which() function\nWhile logical expressions will return a vector of TRUE and FALSE values of the same length, we could use the which() function to output the indices where the values are TRUE. Indexing with either method generates the same results, and personal preference determines which method you choose to use. For example:\n\nwhich(age > 50 | age < 18)\n\n[1] 1 4 5 6\n\nage[which(age > 50 | age < 18)]\n\n[1] 15 52 73 81\n\n\nNotice that we get the same results regardless of whether or not we use the which(). Also note that while which() works the same as the logical expressions for indexing, it can be used for multiple other operations, where it is not interchangeable with logical expressions.\n\n\n\n\n17.1.2 Dataframes\nDataframes (and matrices) have 2 dimensions (rows and columns), so if we want to select some specific data from it we need to specify the “coordinates” we want from it. We use the same square bracket notation but rather than providing a single index, there are two indices required. Within the square bracket, row numbers come first followed by column numbers (and the two are separated by a comma). Let’s explore the metadata dataframe, shown below are the first six samples:\n\n\n\nmetadata\n\n\nLet’s say we wanted to extract the wild type (Wt) value that is present in the first row and the first column. To extract it, just like with vectors, we give the name of the data frame that we want to extract from, followed by the square brackets. Now inside the square brackets we give the coordinates or indices for the rows in which the value(s) are present, followed by a comma, then the coordinates or indices for the columns in which the value(s) are present. We know the wild type value is in the first row if we count from the top, so we put a one, then a comma. The wild type value is also in the first column, counting from left to right, so we put a one in the columns space too.\n\nmetadata <- read.csv(file=\"../data/mouse_exp_design.csv\")\n\n# Extract value 'Wt'\nmetadata[1, 1]\n\n[1] \"Wt\"\n\n\nNow let’s extract the value 1 from the first row and third column.\n\n# Extract value '1'\nmetadata[1, 3] \n\n[1] 1\n\n\nNow if you only wanted to select based on rows, you would provide the index for the rows and leave the columns index blank. The key here is to include the comma, to let R know that you are accessing a 2-dimensional data structure:\n\n# Extract third row\nmetadata[3, ] \n\n        genotype celltype replicate\nsample3       Wt    typeA         3\n\n\nWhat kind of data structure does the output appear to be? We see that it is two-dimensional with row names and column names, so we can surmise that it’s likely a data frame.\nIf you were selecting specific columns from the data frame - the rows are left blank:\n\n# Extract third column\nmetadata[ , 3]   \n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\n\nWhat kind of data structure does this output appear to be? It looks different from the data frame, and we really just see a series of values output, indicating a vector data structure. This happens be default if just selecting a single column from a data frame. R will drop to the simplest data structure possible. Since a single column in a data frame is really just a vector, R will output a vector data structure as the simplest data structure. Oftentimes we would like to keep our single column as a data frame. To do this, there is an argument we can add when subsetting called drop, meaning do we want to drop down to the simplest data structure. By default it is TRUE, but we can change it’s value to FALSE in order to keep the output as a data frame.\n\n# Extract third column as a data frame\nmetadata[ , 3, drop = FALSE] \n\n         replicate\nsample1          1\nsample2          2\nsample3          3\nsample4          1\nsample5          2\nsample6          3\nsample7          1\nsample8          2\nsample9          3\nsample10         1\nsample11         2\nsample12         3\n\n\nJust like with vectors, you can select multiple rows and columns at a time. Within the square brackets, you need to provide a vector of the desired values.\nWe can extract consecutive rows or columns using the colon (:) to create the vector of indices to extract.\n\n# Dataframe containing first two columns\nmetadata[ , 1:2] \n\n         genotype celltype\nsample1        Wt    typeA\nsample2        Wt    typeA\nsample3        Wt    typeA\nsample4        KO    typeA\nsample5        KO    typeA\nsample6        KO    typeA\nsample7        Wt    typeB\nsample8        Wt    typeB\nsample9        Wt    typeB\nsample10       KO    typeB\nsample11       KO    typeB\nsample12       KO    typeB\n\n\nAlternatively, we can use the combine function (c()) to extract any number of rows or columns. Let’s extract the first, third, and sixth rows.\n\n# Data frame containing first, third and sixth rows\nmetadata[c(1,3,6), ] \n\n        genotype celltype replicate\nsample1       Wt    typeA         1\nsample3       Wt    typeA         3\nsample6       KO    typeA         3\n\n\nFor larger datasets, it can be tricky to remember the column number that corresponds to a particular variable. (Is celltype in column 1 or 2? oh, right… they are in column 1). In some cases, the column/row number for values can change if the script you are using adds or removes columns/rows. It’s, therefore, often better to use column/row names to refer to extract particular values, and it makes your code easier to read and your intentions clearer.\n\n# Extract the celltype column for the first three samples\nmetadata[c(\"sample1\", \"sample2\", \"sample3\") , \"celltype\"] \n\n[1] \"typeA\" \"typeA\" \"typeA\"\n\n\nIt’s important to type the names of the columns/rows in the exact way that they are typed in the data frame; for instance if I had spelled celltype with a capital C, it would not have worked.\nIf you need to remind yourself of the column/row names, the following functions are helpful:\n\n# Check column names of metadata data frame\ncolnames(metadata)\n\n[1] \"genotype\"  \"celltype\"  \"replicate\"\n\n# Check row names of metadata data frame\nrownames(metadata)\n\n [1] \"sample1\"  \"sample2\"  \"sample3\"  \"sample4\"  \"sample5\"  \"sample6\" \n [7] \"sample7\"  \"sample8\"  \"sample9\"  \"sample10\" \"sample11\" \"sample12\"\n\n\nIf only a single column is to be extracted from a data frame, there is a useful shortcut available. If you type the name of the data frame, then the $, you have the option to choose which column to extract. For instance, let’s extract the entire genotype column from our dataset:\n\n# Extract the genotype column\nmetadata$genotype \n\n [1] \"Wt\" \"Wt\" \"Wt\" \"KO\" \"KO\" \"KO\" \"Wt\" \"Wt\" \"Wt\" \"KO\" \"KO\" \"KO\"\n\n\nThe output will always be a vector, and if desired, you can continue to treat it as a vector. For example, if we wanted the genotype information for the first five samples in metadata, we can use the square brackets ([]) with the indices for the values from the vector to extract:\n\n# Extract the first five values/elements of the genotype column\nmetadata$genotype[1:5]\n\n[1] \"Wt\" \"Wt\" \"Wt\" \"KO\" \"KO\"\n\n\nUnfortunately, there is no equivalent $ syntax to select a row by name.\n\n17.1.2.1 Selecting using indices with logical operators\nWith data frames, similar to vectors, we can use logical expressions to extract the rows or columns in the data frame with specific values. First, we need to determine the indices in a rows or columns where a logical expression is TRUE, then we can extract those rows or columns from the data frame.\nFor example, if we want to return only those rows of the data frame with the celltype column having a value of typeA, we would perform two steps:\n\nIdentify which rows in the celltype column have a value of typeA.\nUse those TRUE values to extract those rows from the data frame.\n\nTo do this we would extract the column of interest as a vector, with the first value corresponding to the first row, the second value corresponding to the second row, so on and so forth. We use that vector in the logical expression. Here we are looking for values to be equal to typeA, so our logical expression would be:\n\nmetadata$celltype == \"typeA\"\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThis will output TRUE and FALSE values for the values in the vector. The first six values are TRUE, while the last six are FALSE. This means the first six rows of our metadata have a vale of typeA while the last six do not. We can save these values to a variable, which we can call whatever we would like; let’s call it logical_idx.\n\nlogical_idx <- metadata$celltype == \"typeA\"\n\nNow we can use those TRUE and FALSE values to extract the rows that correspond to the TRUE values from the metadata data frame. We will extract as we normally would a data frame with metadata[ , ], and we need to make sure we put the logical_idx in the row’s space, since those TRUE and FALSE values correspond to the ROWS for which the expression is TRUE/FALSE. We will leave the column’s space blank to return all columns.\n\nmetadata[logical_idx, ]\n\n        genotype celltype replicate\nsample1       Wt    typeA         1\nsample2       Wt    typeA         2\nsample3       Wt    typeA         3\nsample4       KO    typeA         1\nsample5       KO    typeA         2\nsample6       KO    typeA         3\n\n\n\n17.1.2.1.1 Selecting indices with logical operators using the which() function\nAs you might have guessed, we can also use the which() function to return the indices for which the logical expression is TRUE. For example, we can find the indices where the celltype is typeA within the metadata dataframe:\n\nwhich(metadata$celltype == \"typeA\")\n\n[1] 1 2 3 4 5 6\n\n\nThis returns the values one through six, indicating that the first 6 values or rows are true, or equal to typeA. We can save our indices for which rows the logical expression is true to a variable we’ll call idx, but, again, you could call it anything you want.\n\nidx <- which(metadata$celltype == \"typeA\")\n\nThen, we can use these indices to indicate the rows that we would like to return by extracting that data as we have previously, giving the idx as the rows that we would like to extract, while returning all columns:\n\nmetadata[idx, ]\n\n        genotype celltype replicate\nsample1       Wt    typeA         1\nsample2       Wt    typeA         2\nsample3       Wt    typeA         3\nsample4       KO    typeA         1\nsample5       KO    typeA         2\nsample6       KO    typeA         3\n\n\nLet’s try another subsetting. Extract the rows of the metadata data frame for only the replicates 2 and 3. First, let’s create the logical expression for the column of interest (replicate):\n\nwhich(metadata$replicate > 1)\n\n[1]  2  3  5  6  8  9 11 12\n\n\nThis should return the indices for the rows in the replicate column within metadata that have a value of 2 or 3. Now, we can save those indices to a variable and use that variable to extract those corresponding rows from the metadata table.\n\nidx <- which(metadata$replicate > 1)\n    \nmetadata[idx, ]\n\n         genotype celltype replicate\nsample2        Wt    typeA         2\nsample3        Wt    typeA         3\nsample5        KO    typeA         2\nsample6        KO    typeA         3\nsample8        Wt    typeB         2\nsample9        Wt    typeB         3\nsample11       KO    typeB         2\nsample12       KO    typeB         3\n\n\nAlternatively, instead of doing this in two steps, we could use nesting to perform in a single step:\n\nmetadata[which(metadata$replicate > 1), ]\n\n         genotype celltype replicate\nsample2        Wt    typeA         2\nsample3        Wt    typeA         3\nsample5        KO    typeA         2\nsample6        KO    typeA         3\nsample8        Wt    typeB         2\nsample9        Wt    typeB         3\nsample11       KO    typeB         2\nsample12       KO    typeB         3\n\n\nEither way works, so use the method that is most intuitive for you.\nSo far we haven’t stored as variables any of the extractions/subsettings that we have performed. Let’s save this output to a variable called sub_meta:\n\nsub_meta <- metadata[which(metadata$replicate > 1), ]\n\n\nExercises\n\n\n\n\n\n\nBasic\n\n\n\nVectors\n\nCreate a vector called alphabets with the following letters, C, D, X, L, F.\nUse the associated indices along with [ ] to do the following:\n\n\nonly display C, D and F\ndisplay all except X\ndisplay the letters in the opposite order (F, L, X, D, C)\n\nDataframes\n\nReturn a dataframe with only the genotype and replicate column values for sample2 and sample8.\nReturn the fourth and ninth values of the replicate column.\nExtract the replicate column as a data frame.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Vectors \n#1\nv <- c(\"C\",\"D\",\"X\",\"L\",\"F\")\n\n#2\nv[c(1,2,5)]\n\n[1] \"C\" \"D\" \"F\"\n\nv[-3]\n\n[1] \"C\" \"D\" \"L\" \"F\"\n\nv[5:1]\n\n[1] \"F\" \"L\" \"X\" \"D\" \"C\"\n\n#Dataframes\nmetadata[c(2,8),c(1,3)]\n\n        genotype replicate\nsample2       Wt         2\nsample8       Wt         2\n\nmetadata$replicate[c(4,9)]\n\n[1] 1 3\n\nmetadata[, 3, drop=FALSE]\n\n         replicate\nsample1          1\nsample2          2\nsample3          3\nsample4          1\nsample5          2\nsample6          3\nsample7          1\nsample8          2\nsample9          3\nsample10         1\nsample11         2\nsample12         3\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nYou find out that there may be a problem with your data. The facility which processed your data contacted you to let you know that they discovered a potentially faulty reagent. They are concerned about all analyses which took place within a week (before or after) of January 9th.\n\nThey provide the processing dates for all of your samples. They let you know that, starting on January 12th, they processed 1 sample per day in ascending order (you’re not sure why they did things that way, you’re definitely not working with these people again). Add a date column to the metadata dataframe with this information.\n\nHint: You can create a date object in R as the number of days from an origin date: as.Date(2, origin = \"1992-01-01\") becomes \"1970-01-03\". Internally, dates in R are stored as the number of days since January 1, 1970. Which is the case for most programming languages.\n\nAdd another column to metadata called contaminated and have it indicate whether or not each sample was within the possible contamination range.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndvec <- as.Date(0:11, origin = \"2023-01-12\")\nmetadata$date <- dvec\nmetadata$contaminated <- metadata$date < as.Date(7, origin = \"2023-01-9\")\n\n\n\n\n\nNOTE: There are easier methods for subsetting dataframes using logical expressions, including the filter() and the subset() functions. These functions will return the rows of the dataframe for which the logical expression is TRUE, allowing us to subset the data in a single step. We will explore the filter() function in more detail in a later lesson.\n\n\n\n\n\n17.1.3 Lists\nSelecting components from a list requires a slightly different notation, even though in theory a list is a vector (that contains multiple data structures). To select a specific component of a list, you need to use double bracket notation [[]]. Let’s use the list1 that we created previously, and index the second component.\nIf you need to recreate list1, run the following code:\n\nspecies <- c(\"ecoli\", \"human\", \"corn\")\nexpression <- factor(c(\"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\", \"high\"))\nglengths <- c(4.6, 3000, 50000)\ndf <- data.frame(species, glengths)\nlist1 <- list(species, df, expression)\n\n\nlist1[[2]]\n\n  species glengths\n1   ecoli      4.6\n2   human   3000.0\n3    corn  50000.0\n\n\nUsing the double bracket notation is useful for accessing the individual components whilst preserving the original data structure. When creating this list we know we had originally stored a dataframe in the second component. With the class function we can check if that is what we retrieve:\n\ncomp2 <- list1[[2]]\nclass(comp2)\n\n[1] \"data.frame\"\n\n\nYou can also reference what is inside the component by adding an additional bracket. For example, in the first component we have a vector stored.\n\nlist1[[1]]\n\n[1] \"ecoli\" \"human\" \"corn\" \n\n\nNow, if we wanted to reference the first element of that vector we would use:\n\nlist1[[1]][1]\n\n[1] \"ecoli\"\n\n\nYou can also do the same for dataframes and matrices, although with larger datasets it is not advisable. Instead, it is better to save the contents of a list component to a variable (as we did above) and further manipulate it. Also, it is important to note that when selecting components we can only access one at a time. To access multiple components of a list, see the note below.\n\nNOTE: Using the single bracket notation also works with lists. The difference is the class of the information that is retrieved. Using single bracket notation i.e. list1[1] will return the contents in a list form and not the original data structure. The benefit of this notation is that it allows indexing by vectors so you can access multiple components of the list at once.\n\n\n17.1.4 An R package for data wrangling\nThe methods presented above are using base R functions for data wrangling. Later we will explore the Tidyverse suite of packages, specifically designed to make data wrangling easier.\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session3/matching_reordering.html#logical-operators-for-identifying-matching-elements",
    "href": "session3/matching_reordering.html#logical-operators-for-identifying-matching-elements",
    "title": "18  Matching and Reordering Data in R",
    "section": "18.1 Logical operators for identifying matching elements",
    "text": "18.1 Logical operators for identifying matching elements\nOftentimes, we encounter different analysis tools that require multiple input datasets. It is not uncommon for these inputs to need to have the same row names, column names, or unique identifiers in the same order to perform the analysis. Therefore, knowing how to reorder datasets and determine whether the data matches is an important skill.\nIn our use case, we will be working with genomic data. We have gene expression data generated by RNA-seq; in addition, we have a metadata file corresponding to the RNA-seq samples. The metadata contains information about the samples present in the gene expression file, such as which sample group each sample belongs to and any batch or experimental variables present in the data.\nLet’s read in some gene expression data (RPKM matrix):\n\nrpkm_data <- read.csv(\"../data/counts.rpkm\")\nmetadata <- read.csv(file=\"../data/mouse_exp_design.csv\")\n\n\nNOTE: If the data file name ends with txt instead of csv, you can read in the data using the code: rpkm_data <- read.csv(\"../data/counts.rpkm.txt\").\n\nTake a look at the first few lines of the data matrix to see what’s in there.\n\nhead(rpkm_data)\n\n                     sample2    sample5  sample7   sample8   sample9   sample4\nENSMUSG00000000001 19.265000 23.7222000 2.611610 5.8495400 6.5126300 24.076700\nENSMUSG00000000003  0.000000  0.0000000 0.000000 0.0000000 0.0000000  0.000000\nENSMUSG00000000028  1.032290  0.8269540 1.134410 0.6987540 0.9251170  0.827891\nENSMUSG00000000031  0.000000  0.0000000 0.000000 0.0298449 0.0597726  0.000000\nENSMUSG00000000037  0.056033  0.0473238 0.000000 0.0685938 0.0494147  0.180883\nENSMUSG00000000049  0.258134  1.0730200 0.252342 0.2970320 0.2082800  2.191720\n                      sample6   sample12   sample3   sample11  sample10\nENSMUSG00000000001 20.8198000 26.9158000 20.889500 24.0465000 24.198100\nENSMUSG00000000003  0.0000000  0.0000000  0.000000  0.0000000  0.000000\nENSMUSG00000000028  1.1686300  0.6735630  0.892183  0.9753270  1.045920\nENSMUSG00000000031  0.0511932  0.0204382  0.000000  0.0000000  0.000000\nENSMUSG00000000037  0.1438840  0.0662324  0.146196  0.0206405  0.017004\nENSMUSG00000000049  1.6853800  0.1161970  0.421286  0.0634322  0.369550\n                      sample1\nENSMUSG00000000001 19.7848000\nENSMUSG00000000003  0.0000000\nENSMUSG00000000028  0.9377920\nENSMUSG00000000031  0.0359631\nENSMUSG00000000037  0.1514170\nENSMUSG00000000049  0.2567330\n\n\nIt looks as if the sample names (header) in our data matrix are similar to the row names of our metadata file, but it’s hard to tell since they are not in the same order. We can do a quick check of the number of columns in the count data and the rows in the metadata and at least see if the numbers match up.\n\nncol(rpkm_data)\n\n[1] 12\n\nnrow(metadata)\n\n[1] 12\n\n\nWhat we want to know is, do we have data for every sample that we have metadata?"
  },
  {
    "objectID": "session3/matching_reordering.html#the-in-operator",
    "href": "session3/matching_reordering.html#the-in-operator",
    "title": "18  Matching and Reordering Data in R",
    "section": "18.2 The %in% operator",
    "text": "18.2 The %in% operator\nAlthough lacking in documentation, this operator is well-used and convenient once you get the hang of it. The operator is used with the following syntax:\nvector1 %in% vector2\nIt will take each element from vector1 as input, one at a time, and evaluate if the element is present in vector2. The two vectors do not have to be the same size. This operation will return a vector containing logical values to indicate whether or not there is a match. The new vector will be of the same length as vector1. Take a look at the example below:\n\nA <- c(1,3,5,7,9,11)   # odd numbers\nB <- c(2,4,6,8,10,12)  # even numbers\n\n# test to see if each of the elements of A is in B  \nA %in% B\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSince vector A contains only odd numbers and vector B contains only even numbers, the operation returns a logical vector containing six FALSE, suggesting that no element in vector A is present in vector B. Let’s change a couple of numbers inside vector B to match vector A:\n\nA <- c(1,3,5,7,9,11)   # odd numbers\nB <- c(2,4,6,8,1,5)  # add some odd numbers in \n\n\n# test to see if each of the elements of A is in B\nA %in% B\n\n[1]  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\nThe returned logical vector denotes which elements in A are also in B - the first and third elements, which are 1 and 5.\nWe saw previously that we could use the output from a logical expression to subset data by returning only the values corresponding to TRUE. Therefore, we can use the output logical vector to subset our data, and return only those elements in A, which are also in B by returning only the TRUE values:\n\n\n\nmatching1\n\n\n\nintersection <- A %in% B\nintersection\n\n[1]  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\n\n\n\nmatching2\n\n\n\nA[intersection]\n\n[1] 1 5\n\n\n\n\n\nmatching3\n\n\nIn these previous examples, the vectors were so small that it’s easy to check every logical value by eye; but this is not practical when we work with large datasets (e.g. a vector with 1000 logical values). Instead, we can use any function. Given a logical vector, this function will tell you whether at least one value is TRUE. It provides us a quick way to assess if any of the values contained in vector A are also in vector B:\n\nany(A %in% B)\n\n[1] TRUE\n\n\nThe all function is also useful. Given a logical vector, it will tell you whether all values are TRUE. If there is at least one FALSE value, the all function will return a FALSE. We can use this function to assess whether all elements from vector A are contained in vector B.\n\nall(A %in% B)\n\n[1] FALSE\n\n\nSuppose we had two vectors containing same values. How can we check if those values are in the same order in each vector? In this case, we can use == operator to compare each element of the same position from two vectors. The operator returns a logical vector indicating TRUE/FALSE at each position. Then we can use all() function to check if all values in the returned vector are TRUE. If all values are TRUE, we know that these two vectors are the same. Unlike %in% operator, == operator requires that two vectors are of equal length.\n\nA <- c(10,20,30,40,50)\nB <- c(50,40,30,20,10)  # same numbers but backwards \n\n# test to see if each element of A is in B\nA %in% B\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n# test to see if each element of A is in the same position in B\nA == B\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n# use all() to check if they are a perfect match\nall(A == B)\n\n[1] FALSE\n\n\nLet’s try this on our genomic data, and see whether we have metadata information for all samples in our expression data. We’ll start by creating two vectors: one is the rownames of the metadata, and one is the colnames of the RPKM data. These are base functions in R which allow you to extract the row and column names as a vector:\n\nx <- rownames(metadata)\ny <- colnames(rpkm_data)\n\nNow check to see that all of x are in y:\n\nall(x %in% y)\n\n[1] TRUE\n\n\nNote that we can use nested functions in place of x and y and still get the same result:\n\nall(rownames(metadata) %in% colnames(rpkm_data))\n\n[1] TRUE\n\n\nWe know that all samples are present, but are they in the same order?\n\nx == y\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\nall(x == y)\n\n[1] FALSE\n\n\nExercise\n\n\n\n\n\n\nBasic\n\n\n\nWe have a list of 6 marker genes that we are very interested in. Our goal is to extract count data for these genes using the %in% operator from the rpkm_data data frame, instead of scrolling through rpkm_data and finding them manually.\nFirst, let’s create a vector called important_genes with the Ensembl IDs of the 6 genes we are interested in:\n\nimportant_genes <- c(\"ENSMUSG00000083700\", \"ENSMUSG00000080990\", \"ENSMUSG00000065619\", \"ENSMUSG00000047945\", \"ENSMUSG00000081010\", \"ENSMUSG00000030970\")\n\n\nUse the %in% operator to determine if all of these genes are present in the row names of the rpkm_data dataframe.\nExtract the rows from rpkm_data that correspond to these 6 genes using [] and the %in% operator. Double check the row names to ensure that you are extracting the correct rows.\nExtract the rows from rpkm_data that correspond to these 6 genes using [], but without using the %in% operator.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#1\nimportant_genes %in% rownames(rpkm_data)\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n#2\nidx <- rownames(rpkm_data) %in% important_genes\nans <- rpkm_data[idx, ]\nidx2 <- which(rownames(rpkm_data) %in% important_genes)\nans2 <- rpkm_data[idx2, ]\n\n#3.\nans3 <- rpkm_data[important_genes, ]\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nUsing important_genes as defined above, check whether or not the genes which in the rpkm_data dataframe are in the same order as important_genes. Return a vector indicating, for each important gene in important_genes, whether or not its order rank is the same as it’s order rank in rpkm_data, i.e. whether or not the second gene in important_genes is also the second important gene to appear in rpkm_data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#This is actually very simple to do, given the basic solutions\nrownames(ans2) == rownames(ans3)\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nYou are already upset with your collaborator for giving you data which uses Ensembl IDs as identifiers (we will convert these IDs soon). They then write down 2 genes of interest for you to look for in the dataset before leaving on vacation.\nWhen you look at the gene list a few days later, you realize you cannot make out some of their handwriting. You decipher what you can, but realize there are some digits you simply cannot interpret.\n\ncollaborator_genes <- c(\"ENSMUSG00000081**0\", \"ENSMUSG00000030*7*\")\n\nFind all genes in rpkm_data which match these two identifiers, where * could be replaced with any single 0-9 digit.\nHint: You’ll probably want to use something like grep, which can pattern match based on regular expressions. You can make sure you have the right regular expression regular here\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nregexes <- c(\"ENSMUSG00000081\\\\d\\\\d0\", \"ENSMUSG00000030\\\\d7\\\\d\")\nrpkm_data[grep(regexes[1], rownames(rpkm_data)),]\n\n                      sample2     sample5      sample7   sample8      sample9\nENSMUSG00000081000 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081010 0.22227500 3.49415e-01  1.90397e-01 0.1671660  2.21353e-01\nENSMUSG00000081020 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081030 0.00000000 0.00000e+00  0.00000e+00 0.1223340  0.00000e+00\nENSMUSG00000081050 0.02785810 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081060 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081070 0.09740580 1.44535e-01  1.25448e-01 0.1353490  1.99690e-01\nENSMUSG00000081080 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081100 0.27463500 6.61332e-01  0.00000e+00 0.2855090  2.19711e+00\nENSMUSG00000081110 0.39801600 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081120 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081130 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081150 0.10121900 2.01987e-02  1.31797e-01 0.0673127  4.44992e-02\nENSMUSG00000081160 0.00000000 0.00000e+00  0.00000e+00 0.0000000  3.32879e-01\nENSMUSG00000081170 0.00000000 4.00769e-02  5.13139e-02 0.0000000  5.67826e-02\nENSMUSG00000081180 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081200 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081210 0.00000000 3.44678e-02  0.00000e+00 0.0588781  2.46227e-02\nENSMUSG00000081220 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081230 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081240 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081250 0.05572640 4.34706e-02  4.36641e-01 0.1093740  1.12035e-01\nENSMUSG00000081260 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081270 0.06923800 5.74667e-02  0.00000e+00 0.1305830  2.13832e-02\nENSMUSG00000081280 0.00000000 0.00000e+00  0.00000e+00 0.0676693  0.00000e+00\nENSMUSG00000081290 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081300 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081310 0.00000000 0.00000e+00  5.18181e-02 0.0000000  0.00000e+00\nENSMUSG00000081320 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081330 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081340 0.64417200 4.34211e-01  2.37312e-01 0.4893080  2.15859e-01\nENSMUSG00000081350 0.03330220 1.01136e-01  1.83130e-01 0.0340187  1.46664e-02\nENSMUSG00000081360 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081370 0.05727410 9.36444e-02  2.18960e-02 0.0343115  0.00000e+00\nENSMUSG00000081390 0.00907964 1.78338e-02  4.30100e-02 0.0000000  1.35086e-02\nENSMUSG00000081400 0.02334000 5.37279e-02  0.00000e+00 0.0679511  9.87008e-02\nENSMUSG00000081410 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081420 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081440 0.05252400 8.26902e-02  1.62518e-01 0.1913190  1.05592e-01\nENSMUSG00000081450 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081460 0.00000000 4.06980e-02  0.00000e+00 0.0000000  2.99414e-02\nENSMUSG00000081470 0.02638720 1.23921e-01  3.54163e-01 0.1049310  1.14508e-01\nENSMUSG00000081480 0.04461430 0.00000e+00  0.00000e+00 0.0000000  4.89494e-02\nENSMUSG00000081490 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081500 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081510 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081520 0.00000000 1.23610e-01  1.06965e+00 0.0000000  9.01922e-02\nENSMUSG00000081530 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081540 0.00000000 0.00000e+00  0.00000e+00 0.0000000  5.77606e-02\nENSMUSG00000081550 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081560 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081570 0.00000000 0.00000e+00  0.00000e+00 0.0000000  3.66591e-02\nENSMUSG00000081580 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081590 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081600 0.10577100 2.66121e-02  6.73608e-01 0.0681934  1.15221e-01\nENSMUSG00000081610 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081620 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081630 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081640 0.00000000 0.00000e+00  3.95136e-02 0.1259820  2.56990e-01\nENSMUSG00000081650 0.00000000 1.42372e-39 1.53320e-233 0.1438880 6.09986e-254\nENSMUSG00000081660 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081670 0.00368657 0.00000e+00  0.00000e+00 0.0000000  2.91256e-02\nENSMUSG00000081680 0.00000000 4.29817e-02  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081690 0.00000000 0.00000e+00  0.00000e+00 0.1035380  0.00000e+00\nENSMUSG00000081700 0.52714800 6.40574e-01  1.76096e+00 3.9369200  2.36134e+00\nENSMUSG00000081720 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081730 0.02585980 8.62299e-02  1.05176e-01 0.0000000  0.00000e+00\nENSMUSG00000081740 0.11143400 1.28128e-01  2.14718e-01 0.1081290  1.87253e-01\nENSMUSG00000081750 0.00000000 1.13206e-01  1.37036e-01 0.0995484  8.50347e-02\nENSMUSG00000081770 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081800 0.00000000 0.00000e+00  0.00000e+00 0.5228580  9.02322e-01\nENSMUSG00000081810 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081820 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081830 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081840 0.00000000 0.00000e+00  0.00000e+00 0.0000000  8.33125e-02\nENSMUSG00000081850 0.00000000 0.00000e+00  0.00000e+00 0.0213764  0.00000e+00\nENSMUSG00000081860 0.00000000 6.89421e-02  0.00000e+00 0.0000000  1.01849e-01\nENSMUSG00000081870 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081880 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081890 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081900 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081910 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081920 0.00000000 6.97755e-02  6.88611e-02 0.1178060  2.64442e-02\nENSMUSG00000081930 0.00000000 0.00000e+00  0.00000e+00 0.0000000  2.28777e-02\nENSMUSG00000081940 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081950 0.00000000 0.00000e+00  0.00000e+00 0.0000000  1.98463e-01\nENSMUSG00000081960 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081970 0.51591900 9.45115e-02  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081980 0.00000000 0.00000e+00  0.00000e+00 0.0000000  0.00000e+00\nENSMUSG00000081990 0.14530100 1.73048e-01  0.00000e+00 0.0000000  0.00000e+00\n                     sample4   sample6     sample12   sample3    sample11\nENSMUSG00000081000 0.0000000 0.0000000  4.30796e-02 0.0000000 0.00000e+00\nENSMUSG00000081010 0.4196660 0.2482440  5.94672e-01 0.2143470 4.15823e-01\nENSMUSG00000081020 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081030 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081050 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081060 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081070 0.1972550 0.1439340  8.82678e-02 0.0000000 8.55702e-02\nENSMUSG00000081080 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081100 0.5043170 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081110 0.0000000 0.0000000  3.10864e-01 0.0000000 0.00000e+00\nENSMUSG00000081120 0.0000000 0.0000000  0.00000e+00 0.0408195 0.00000e+00\nENSMUSG00000081130 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081150 0.0000000 0.0146228  0.00000e+00 0.0162030 0.00000e+00\nENSMUSG00000081160 0.0000000 0.0000000  0.00000e+00 0.0000000 4.24105e-01\nENSMUSG00000081170 0.0300449 0.0845798  2.56416e-02 0.0597709 0.00000e+00\nENSMUSG00000081180 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081200 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081210 0.0259691 0.0243380  2.22047e-02 0.0000000 0.00000e+00\nENSMUSG00000081220 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081230 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081240 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081250 0.0506275 0.1104960  1.44034e-02 0.0357261 5.89878e-02\nENSMUSG00000081260 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081270 0.0112396 0.0103406  2.29162e-01 0.0950715 2.90225e-02\nENSMUSG00000081280 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081290 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081300 0.0000000 0.0545545  2.40617e-02 0.0295514 0.00000e+00\nENSMUSG00000081310 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081320 0.0000000 0.0000000  0.00000e+00 0.0818319 0.00000e+00\nENSMUSG00000081330 0.0000000 0.0950554  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081340 0.2247430 0.1051720  3.99464e-01 1.8677500 4.09300e-01\nENSMUSG00000081350 0.1089630 0.0726467  7.86517e-02 0.0792945 3.53316e-02\nENSMUSG00000081360 0.0000000 0.0314025  0.00000e+00 0.0374285 0.00000e+00\nENSMUSG00000081370 0.0421598 0.0689326  5.23947e-02 0.0770847 8.38816e-02\nENSMUSG00000081390 0.0071164 0.0195862  6.78611e-03 0.0361030 3.80935e-02\nENSMUSG00000081400 0.0618234 0.0580507  0.00000e+00 0.0674159 0.00000e+00\nENSMUSG00000081410 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081420 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081440 0.1273620 0.1475540  5.49808e-02 0.0837032 3.73026e-02\nENSMUSG00000081450 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081460 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081470 0.0239154 0.0898210  1.40554e-01 0.1519990 5.52225e-02\nENSMUSG00000081480 0.0103467 0.0000000  3.48421e-02 0.0107177 1.41788e-01\nENSMUSG00000081490 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081500 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081510 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081520 0.0000000 0.0000000  0.00000e+00 0.1038290 0.00000e+00\nENSMUSG00000081530 0.0000000 0.0000000  1.61702e-01 0.0000000 0.00000e+00\nENSMUSG00000081540 0.0000000 0.0000000  5.14198e-02 0.0000000 0.00000e+00\nENSMUSG00000081550 0.0000000 0.0102009  0.00000e+00 0.0000000 4.18039e-02\nENSMUSG00000081560 0.0000000 0.0000000  3.10240e-03 0.0000000 0.00000e+00\nENSMUSG00000081570 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081580 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081590 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081600 0.0605787 0.1134160  6.73165e-02 0.0397673 4.51580e-02\nENSMUSG00000081610 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081620 0.0571851 0.0541486  2.19254e-06 0.0579367 0.00000e+00\nENSMUSG00000081630 0.0000000 0.0000000  0.00000e+00 0.0144917 0.00000e+00\nENSMUSG00000081640 0.0000000 0.0211881  1.70749e-01 0.0000000 1.01114e-01\nENSMUSG00000081650 0.0448670 0.0000000 2.41125e-244 0.0950681 2.09208e-77\nENSMUSG00000081660 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081670 0.0119842 0.0000000  2.68140e-03 0.0000000 7.58623e-03\nENSMUSG00000081680 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081690 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081700 0.4849380 0.0000000  4.20815e-01 0.0000000 0.00000e+00\nENSMUSG00000081720 0.2939240 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081730 0.0000000 0.0426079  1.15971e-01 0.1004120 0.00000e+00\nENSMUSG00000081740 0.0976122 0.2768850  8.49159e-02 0.0000000 0.00000e+00\nENSMUSG00000081750 0.0881494 0.0000000  0.00000e+00 0.0000000 9.62319e-02\nENSMUSG00000081770 0.0123680 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081800 0.0000000 0.4461050  3.79284e-01 0.0000000 0.00000e+00\nENSMUSG00000081810 0.0000000 0.0547575  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081820 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081830 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081840 0.0000000 0.0821355  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081850 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081860 0.0531812 0.1531220  0.00000e+00 0.0519515 0.00000e+00\nENSMUSG00000081870 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081880 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081890 0.0000000 0.0000000  0.00000e+00 0.0783868 0.00000e+00\nENSMUSG00000081900 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081910 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081920 0.0548575 0.0766305  1.00302e-01 0.0000000 3.44230e-02\nENSMUSG00000081930 0.0000000 0.0222406  0.00000e+00 0.0229968 0.00000e+00\nENSMUSG00000081940 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081950 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081960 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081970 0.0563853 0.0176324  1.10769e-01 0.3353480 2.21302e-02\nENSMUSG00000081980 0.0000000 0.0000000  0.00000e+00 0.0000000 0.00000e+00\nENSMUSG00000081990 0.0000000 0.0000000  1.18007e-01 0.0000000 3.13326e-01\n                       sample10    sample1\nENSMUSG00000081000  0.00000e+00 0.00000000\nENSMUSG00000081010  4.52537e-01 0.23584800\nENSMUSG00000081020  0.00000e+00 0.00000000\nENSMUSG00000081030  0.00000e+00 0.00000000\nENSMUSG00000081050  0.00000e+00 0.00000000\nENSMUSG00000081060  0.00000e+00 0.00000000\nENSMUSG00000081070  4.73858e-02 0.01427330\nENSMUSG00000081080  0.00000e+00 0.00000000\nENSMUSG00000081100  0.00000e+00 0.00000000\nENSMUSG00000081110  0.00000e+00 0.00000000\nENSMUSG00000081120  0.00000e+00 0.00000000\nENSMUSG00000081130  0.00000e+00 0.00000000\nENSMUSG00000081150  0.00000e+00 0.04398270\nENSMUSG00000081160  0.00000e+00 0.00000000\nENSMUSG00000081170  0.00000e+00 0.02736590\nENSMUSG00000081180  0.00000e+00 0.00000000\nENSMUSG00000081200  0.00000e+00 0.00000000\nENSMUSG00000081210  0.00000e+00 0.00000000\nENSMUSG00000081220  0.00000e+00 0.00000000\nENSMUSG00000081230  0.00000e+00 0.00000000\nENSMUSG00000081240  0.00000e+00 0.00000000\nENSMUSG00000081250  0.00000e+00 0.06468100\nENSMUSG00000081260  0.00000e+00 0.09822360\nENSMUSG00000081270  2.43866e-01 0.22114700\nENSMUSG00000081280  0.00000e+00 0.00000000\nENSMUSG00000081290  0.00000e+00 0.00000000\nENSMUSG00000081300  0.00000e+00 0.00000000\nENSMUSG00000081310  0.00000e+00 0.00000000\nENSMUSG00000081320  0.00000e+00 0.07414980\nENSMUSG00000081330  0.00000e+00 0.03489590\nENSMUSG00000081340  1.87861e-01 0.22680200\nENSMUSG00000081350  7.22104e-02 0.02882380\nENSMUSG00000081360  1.13712e-01 0.00000000\nENSMUSG00000081370  4.95519e-02 0.05991200\nENSMUSG00000081390  1.34264e-02 0.00816612\nENSMUSG00000081400  0.00000e+00 0.06122950\nENSMUSG00000081410  0.00000e+00 0.00000000\nENSMUSG00000081420  0.00000e+00 0.00000000\nENSMUSG00000081440  1.27270e-01 0.06093470\nENSMUSG00000081450  0.00000e+00 0.00000000\nENSMUSG00000081460  0.00000e+00 0.02948170\nENSMUSG00000081470  1.90757e-01 0.18401900\nENSMUSG00000081480  0.00000e+00 0.01939520\nENSMUSG00000081490  0.00000e+00 0.03172180\nENSMUSG00000081500  0.00000e+00 0.00000000\nENSMUSG00000081510  0.00000e+00 0.00000000\nENSMUSG00000081520  0.00000e+00 0.00000000\nENSMUSG00000081530  0.00000e+00 0.00000000\nENSMUSG00000081540  0.00000e+00 0.00000000\nENSMUSG00000081550  3.91927e-02 0.01189620\nENSMUSG00000081560  0.00000e+00 0.00000000\nENSMUSG00000081570  0.00000e+00 0.03759050\nENSMUSG00000081580  0.00000e+00 0.00000000\nENSMUSG00000081590  0.00000e+00 0.00000000\nENSMUSG00000081600  6.09340e-02 0.10902500\nENSMUSG00000081610  0.00000e+00 0.00000000\nENSMUSG00000081620  0.00000e+00 0.00000000\nENSMUSG00000081630  0.00000e+00 0.00000000\nENSMUSG00000081640  0.00000e+00 0.02051630\nENSMUSG00000081650 2.02100e-208 0.00000000\nENSMUSG00000081660  0.00000e+00 0.00000000\nENSMUSG00000081670  0.00000e+00 0.00989694\nENSMUSG00000081680  0.00000e+00 0.00000000\nENSMUSG00000081690  1.63996e-01 0.00000000\nENSMUSG00000081700  0.00000e+00 0.45534200\nENSMUSG00000081720  0.00000e+00 0.00000000\nENSMUSG00000081730  1.49843e-01 0.06830410\nENSMUSG00000081740  0.00000e+00 0.00000000\nENSMUSG00000081750  0.00000e+00 0.07743630\nENSMUSG00000081770  0.00000e+00 0.00000000\nENSMUSG00000081800  1.42942e+00 0.00000000\nENSMUSG00000081810  0.00000e+00 0.05703180\nENSMUSG00000081820  0.00000e+00 0.00000000\nENSMUSG00000081830  0.00000e+00 0.00000000\nENSMUSG00000081840  0.00000e+00 0.08190040\nENSMUSG00000081850  0.00000e+00 0.00000000\nENSMUSG00000081860  0.00000e+00 0.00000000\nENSMUSG00000081870  0.00000e+00 0.00000000\nENSMUSG00000081880  0.00000e+00 0.00000000\nENSMUSG00000081890  0.00000e+00 0.00000000\nENSMUSG00000081900  0.00000e+00 0.00000000\nENSMUSG00000081910  0.00000e+00 0.00000000\nENSMUSG00000081920  4.82955e-02 0.11722600\nENSMUSG00000081930  0.00000e+00 0.00000000\nENSMUSG00000081940  0.00000e+00 0.00000000\nENSMUSG00000081950  3.46450e-01 0.00000000\nENSMUSG00000081960  0.00000e+00 0.00000000\nENSMUSG00000081970  6.22136e-02 0.26596500\nENSMUSG00000081980  0.00000e+00 0.00000000\nENSMUSG00000081990  0.00000e+00 0.00000000\n\nrpkm_data[grep(regexes[2], rownames(rpkm_data)),]\n\n                       sample2     sample5     sample7     sample8     sample9\nENSMUSG00000030074  0.20637600   0.0133865   0.0612572   0.1150200   0.0697535\nENSMUSG00000030075  2.33666000   1.3492500   1.8535600   1.1424600   1.2362400\nENSMUSG00000030077 16.14110000   2.7689800   2.3481000   2.6011400   2.6115000\nENSMUSG00000030079  3.60851000   4.2917600   3.2846100   6.2027900   5.6977500\nENSMUSG00000030170  0.05989460   0.0600771   0.2459500   0.3276650   0.3892750\nENSMUSG00000030172 35.25170000  20.5494000  11.8559000  13.0022000  12.0354000\nENSMUSG00000030173  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030177  0.00000000   0.0000000   0.6003390   1.0381100   1.0106200\nENSMUSG00000030178  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030270  0.66494200   0.0661059   0.1617390   0.2180730   0.2286060\nENSMUSG00000030271  2.04301000   3.4457800   2.5931400   2.4352500   2.8924600\nENSMUSG00000030272  5.08121000   5.5455900   4.9117200   7.0002900   9.8384400\nENSMUSG00000030275 71.76190000  53.7067000  48.9662000  42.4483000  38.2939000\nENSMUSG00000030276  1.97518000   3.1597000   1.7151700   2.1701800   2.8689800\nENSMUSG00000030278  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030279 10.20820000   9.0127400   5.3734900   5.8671100   5.9447100\nENSMUSG00000030373  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030374  6.58867000   5.7330300   3.9467100   3.8056400   3.9936900\nENSMUSG00000030376  0.87396800   0.0621262   0.2192290   0.2963300   0.6559060\nENSMUSG00000030378  0.00000000   0.0000000   0.0000000   0.0153495   0.0000000\nENSMUSG00000030470  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030471  5.88901000  16.2952000   3.5564400   5.5312900   5.8230100\nENSMUSG00000030472  0.00836035   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030474  0.00990759   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030577  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030579  0.05260400   0.0631277   0.0823306   0.0000000   0.0000000\nENSMUSG00000030670  5.10446000   0.3958210   0.1130280   0.0567055   0.1616280\nENSMUSG00000030671 14.77660000   1.6977000   1.6044300   0.7720800   1.4736500\nENSMUSG00000030672  0.54324600   1.9351900   1.5820900   0.8342890   1.9166100\nENSMUSG00000030674  0.00000000   0.0000000   0.4764780   0.1636520   0.1831090\nENSMUSG00000030677  0.68018300   0.9177370   0.3962670   0.6364730   0.8227140\nENSMUSG00000030678 12.61320000  16.9219000  11.0297000  15.0193000  14.9331000\nENSMUSG00000030770 44.97410000   5.1622500  22.5371000  18.8658000  21.6958000\nENSMUSG00000030771  0.08677560   0.0000000   0.1551500   0.1050040   0.1950270\nENSMUSG00000030772 54.27900000  75.0917000   5.1320900   4.5507700   4.8783100\nENSMUSG00000030774 30.79580000   8.4707100  69.2368000  67.6231000  67.7527000\nENSMUSG00000030775  0.00000000   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030779 40.34600000  15.8712000  11.1304000  12.6770000  16.5309000\nENSMUSG00000030870 24.84810000  18.4889000  17.9845000  18.1982000  19.0447000\nENSMUSG00000030871  0.64068300   0.8441390   0.5102840   0.5436110   0.5804900\nENSMUSG00000030872 21.53450000  37.8914000   3.2362800   5.2779200   4.7577500\nENSMUSG00000030873  0.00000000   0.0000000   0.0132390   0.0000000   0.0000000\nENSMUSG00000030876  3.30157000   3.2615900   3.9052900   8.4561100  10.3881000\nENSMUSG00000030877  0.03246930   0.0708122   0.1728210   0.1353880   0.0580233\nENSMUSG00000030878 14.05730000  11.3728000  20.6072000  21.3325000  20.8103000\nENSMUSG00000030879 84.20300000 149.3960000 151.5250000 191.7790000 184.0930000\nENSMUSG00000030970  2.22118000   0.5378520   2.2438100   2.5994000   3.5939700\nENSMUSG00000030972  0.00000000   0.0000000   0.0000000   0.0000000   0.0832298\nENSMUSG00000030976  0.03803630   0.0000000   0.0000000   0.0000000   0.0000000\nENSMUSG00000030978  1.83094000   0.9441210   1.6368600   1.5149200   1.4087400\nENSMUSG00000030979  7.42428000  14.6282000   6.0847000   7.2492800   6.6150800\n                       sample4     sample6   sample12     sample3    sample11\nENSMUSG00000030074 0.00000e+00   0.0291724  0.1229840  0.17460700  0.07651200\nENSMUSG00000030075 1.03547e+00   0.8918970  7.6240800  2.09576000  7.10890000\nENSMUSG00000030077 3.55031e+00   3.3894900 12.3911000 17.40950000 19.41070000\nENSMUSG00000030079 4.70178e+00   4.6240400  5.6084100  4.23242000  5.28554000\nENSMUSG00000030170 1.02297e-01   0.0348348  0.0942575  0.04705470  0.06297680\nENSMUSG00000030172 2.10776e+01  23.1949000 45.9152000 44.83060000 39.73400000\nENSMUSG00000030173 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030177 5.21551e-01   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030178 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030270 1.70996e-01   0.1071400  8.5634100  1.20026000  6.71953000\nENSMUSG00000030271 3.29241e+00   3.3445900  2.4525200  1.16827000  3.48406000\nENSMUSG00000030272 5.62289e+00   6.0813100  7.0790400  4.84389000  6.44008000\nENSMUSG00000030275 5.67920e+01  47.9664000 64.7983000 84.23070000 65.81580000\nENSMUSG00000030276 1.80709e+00   2.6832000  3.8991300  2.78648000  2.67536000\nENSMUSG00000030278 0.00000e+00   0.0000000  0.2909060  0.00000000  0.00000000\nENSMUSG00000030279 1.09897e+01   9.4774500 10.7257000 11.58750000  9.57691000\nENSMUSG00000030373 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030374 5.83355e+00   7.3753300  7.1402200  6.37080000  6.64417000\nENSMUSG00000030376 5.25474e-02   0.1234100  1.1495200  0.81869300  1.81936000\nENSMUSG00000030378 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030470 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030471 1.79915e+01  17.4328000  8.9368500  6.93130000 10.34000000\nENSMUSG00000030472 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030474 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030577 0.00000e+00   0.0000000  0.0000000  0.00817220  0.00000000\nENSMUSG00000030579 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030670 4.61151e-01   0.5641390  0.3491230  5.45081000  0.26870800\nENSMUSG00000030671 1.91161e+00   2.3628600  5.0098600 13.99050000  3.67025000\nENSMUSG00000030672 2.93484e+00   1.3046600  2.7187300  1.25106000  1.59852000\nENSMUSG00000030674 1.75575e-02   0.0000000  0.3643380  0.05641660  0.26297700\nENSMUSG00000030677 5.69520e-01   0.7113590  2.3767800  0.42698800  1.78656000\nENSMUSG00000030678 1.57272e+01  20.3632000 17.7778000 12.58830000 17.52280000\nENSMUSG00000030770 4.06875e+00   3.1445500 17.4887000 41.82530000 11.58450000\nENSMUSG00000030771 9.47579e-03   0.0089190  0.0287954  0.07294370  0.04995530\nENSMUSG00000030772 7.53729e+01  83.1674000 11.1374000 66.36510000  8.19865000\nENSMUSG00000030774 6.68499e+00   7.9819700 25.8756000 30.53650000 20.09400000\nENSMUSG00000030775 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00000000\nENSMUSG00000030779 1.73307e+01  14.5697000 53.9570000 40.71080000 50.02630000\nENSMUSG00000030870 1.88126e+01  17.8138000 20.1388000 27.41960000 22.22650000\nENSMUSG00000030871 6.30509e-01   0.7676120  0.8915580  0.76229600  0.87410300\nENSMUSG00000030872 3.94705e+01  37.3549000 20.5128000 25.73480000 17.62530000\nENSMUSG00000030873 0.00000e+00   0.0000000  0.0000000  0.00000000  0.00833236\nENSMUSG00000030876 3.33462e+00   3.7663300  3.9197200  3.15634000  2.97219000\nENSMUSG00000030877 2.62000e-02   0.0229823  0.0000000  0.12591100  0.28712500\nENSMUSG00000030878 1.00931e+01  11.6458000 16.1902000 13.68670000 12.38450000\nENSMUSG00000030879 1.96557e+02 182.6510000 99.4870000 60.55510000 96.11420000\nENSMUSG00000030970 1.75380e-01   0.4354840  0.9641690  2.15149000  0.96352300\nENSMUSG00000030972 0.00000e+00   0.0000000  0.0000000  0.00987532  0.00000000\nENSMUSG00000030976 0.00000e+00   0.0000000  0.0289630  0.01840510  0.00000000\nENSMUSG00000030978 1.28609e+00   1.5986900  2.9124400  1.54334000  2.76794000\nENSMUSG00000030979 1.37998e+01  14.1101000  9.2110300  8.12846000  7.49274000\n                      sample10     sample1\nENSMUSG00000030074   0.0000000  0.10624200\nENSMUSG00000030075   6.9174900  2.35778000\nENSMUSG00000030077  18.1342000 21.22800000\nENSMUSG00000030079   4.6592600  3.90293000\nENSMUSG00000030170   0.0712690  0.07750520\nENSMUSG00000030172  40.5971000 42.42630000\nENSMUSG00000030173   0.0000000  0.00000000\nENSMUSG00000030177   0.0000000  0.00000000\nENSMUSG00000030178   0.0000000  0.00000000\nENSMUSG00000030270   6.3930500  1.02800000\nENSMUSG00000030271   3.6347100  1.76838000\nENSMUSG00000030272   5.9450300  5.13694000\nENSMUSG00000030275  67.9997000 79.90400000\nENSMUSG00000030276   3.5075900  2.64830000\nENSMUSG00000030278   0.0000000  0.00000000\nENSMUSG00000030279   9.8767300 12.08380000\nENSMUSG00000030373   0.0000000  0.00000000\nENSMUSG00000030374   5.4757700  4.60863000\nENSMUSG00000030376   1.6399200  1.08756000\nENSMUSG00000030378   0.0000000  0.00000000\nENSMUSG00000030470   0.0000000  0.00000000\nENSMUSG00000030471  10.4466000  6.32738000\nENSMUSG00000030472   0.0000000  0.00726246\nENSMUSG00000030474   0.0000000  0.00000000\nENSMUSG00000030577   0.0000000  0.00000000\nENSMUSG00000030579   0.0758163  0.00000000\nENSMUSG00000030670   0.2198540  6.28952000\nENSMUSG00000030671   4.5651500 13.80330000\nENSMUSG00000030672   1.5528100  0.46902400\nENSMUSG00000030674   0.3290180  0.05997590\nENSMUSG00000030677   1.9111300  0.65789100\nENSMUSG00000030678  17.9688000 14.17210000\nENSMUSG00000030770  13.7546000 46.20650000\nENSMUSG00000030771   0.1825680  0.07760820\nENSMUSG00000030772  13.2551000 72.54180000\nENSMUSG00000030774  20.5969000 33.81660000\nENSMUSG00000030775   0.0000000  0.00000000\nENSMUSG00000030779  47.4036000 48.75910000\nENSMUSG00000030870  21.7020000 26.15860000\nENSMUSG00000030871   0.7805900  0.73538600\nENSMUSG00000030872  16.7761000 23.47660000\nENSMUSG00000030873   0.0000000  0.00000000\nENSMUSG00000030876   3.4585600  3.29226000\nENSMUSG00000030877   0.0000000  0.05516930\nENSMUSG00000030878  14.5093000 16.59370000\nENSMUSG00000030879 102.6180000 81.87120000\nENSMUSG00000030970   1.0145200  2.97142000\nENSMUSG00000030972   0.0148159  0.00447713\nENSMUSG00000030976   0.0000000  0.00000000\nENSMUSG00000030978   2.7196800  2.09766000\nENSMUSG00000030979   7.2321400  8.89284000\n\n#Another way yo do things using do.call, rbind, and lapply \nget_fuzzygene <- function(x){rpkm_data[grep(x, rownames(rpkm_data)),]}\nans4 <- do.call(rbind, lapply(regexes,get_fuzzygene))"
  },
  {
    "objectID": "session3/matching_reordering.html#reordering-data-using-match",
    "href": "session3/matching_reordering.html#reordering-data-using-match",
    "title": "18  Matching and Reordering Data in R",
    "section": "18.3 Reordering data using match",
    "text": "18.3 Reordering data using match\nWe can use the match() function to match the values in two vectors. We’ll be using it to evaluate which values are present in both vectors, and how to reorder the elements to make the values match.\nmatch() takes 2 arguments. The first argument is a vector of values in the order you want, while the second argument is the vector of values to be reordered such that it will match the first:\n\na vector of values in the order you want\na vector of values to be reordered\n\nThe function returns the position of the matches (indices) with respect to the second vector, which can be used to re-order it so that it matches the order in the first vector. Let’s use match() on the first and second vectors we created.\n\nfirst <- c(\"A\",\"B\",\"C\",\"D\",\"E\")\nsecond <- c(\"B\",\"D\",\"E\",\"A\",\"C\")  # same letters but different order\nmatch(first,second)\n\n[1] 4 1 5 2 3\n\n\nThe output is the indices for how to reorder the second vector to match the first. These indices match the indices that we derived manually before.\nNow, we can just use the indices to reorder the elements of the second vector to be in the same positions as the matching elements in the first vector:\n\n# Saving indices for how to reorder `second` to match `first`\nreorder_idx <- match(first,second) \n\nThen, we can use those indices to reorder the second vector similar to how we ordered with the manually derived indices.\n\n# Reordering the second vector to match the order of the first vector\nsecond[reorder_idx]\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\nIf the output looks good, we can save the reordered vector to a new variable.\n\n# Reordering and saving the output to a variable\nsecond_reordered <- second[reorder_idx]  \n\n\n\n\nmatching7\n\n\nNow that we know how match() works, let’s change vector second so that only a subset are retained:\n\nfirst <- c(\"A\",\"B\",\"C\",\"D\",\"E\")\nsecond <- c(\"D\",\"B\",\"A\")  # remove values\n\n\n\n\nmatching5\n\n\nAnd try to match() again:\n\nmatch(first,second)\n\n[1]  3  2 NA  1 NA\n\n\nWe see that the match() function takes every element in the first vector and finds the position of that element in the second vector, and if that element is not present, will return a missing value of NA. The value NA represents missing data for any data type within R. In this case, we can see that the match() function output represents the value at position 3 as first, which is A, then position 2 is next, which is B, the value coming next is supposed to be C, but it is not present in the second vector, so NA is returned, so on and so forth.\n\nNOTE: For values that don’t match by default return an NA value. You can specify what values you would have it assigned using nomatch argument. Also, if there is more than one matching value found only the first is reported.\n\nIf we rearrange second using these indices, then we should see that all the values present in both vectors are in the same positions and NAs are present for any missing values.\n\nsecond[match(first, second)]\n\n[1] \"A\" \"B\" NA  \"D\" NA \n\n\n\n18.3.1 Reordering genomic data using match() function\nWhile the input to the match() function is always going to be to vectors, often we need to use these vectors to reorder the rows or columns of a data frame to match the rows or columns of another dataframe. Let’s explore how to do this with our use case featuring RNA-seq data. To perform differential gene expression analysis, we have a data frame with the expression data or counts for every sample and another data frame with the information about to which condition each sample belongs. For the tools doing the analysis, the samples in the counts data, which are the column names, need to be the same and in the same order as the samples in the metadata data frame, which are the rownames.\nWe can take a look at these samples in each dataset by using the rownames() and colnames() functions.\n\n# Check row names of the metadata\nrownames(metadata)\n\n [1] \"sample1\"  \"sample2\"  \"sample3\"  \"sample4\"  \"sample5\"  \"sample6\" \n [7] \"sample7\"  \"sample8\"  \"sample9\"  \"sample10\" \"sample11\" \"sample12\"\n\n# Check the column names of the counts data\ncolnames(rpkm_data)\n\n [1] \"sample2\"  \"sample5\"  \"sample7\"  \"sample8\"  \"sample9\"  \"sample4\" \n [7] \"sample6\"  \"sample12\" \"sample3\"  \"sample11\" \"sample10\" \"sample1\" \n\n\nWe see the row names of the metadata are in a nice order starting at sample1 and ending at sample12, while the column names of the counts data look to be the same samples, but are randomly ordered. Therefore, we want to reorder the columns of the counts data to match the order of the row names of the metadata. To do so, we will use the match() function to match the row names of our metadata with the column names of our counts data, so these will be the arguments for match.\nTo do so, we will use the match function to match the row names of our metadata with the column names of our counts data, so these will be the arguments for match().\nWithin the match() function, the rownames of the metadata is the vector in the order that we want, so this will be the first argument, while the column names of the count or rpkm data is the vector to be reordered. We will save these indices for how to reorder the column names of the count data such that it matches the rownames of the metadata to a variable called genomic idx.\n\ngenomic_idx <- match(rownames(metadata), colnames(rpkm_data))\ngenomic_idx\n\n [1] 12  1  9  6  2  7  3  4  5 11 10  8\n\n\nThe genomic_idx represents how to re-order the column names in our counts data to be identical to the row names in metadata.\nNow we can create a new counts data frame in which the columns are re-ordered based on the match() indices. Remember that to reorder the rows or columns in a data frame we give the name of the data frame followed by square brackets, and then the indices for how to reorder the rows or columns.\nOur genomic_idx represents how we would need to reorder the columns of our count data such that the column names would be in the same order as the row names of our metadata. Therefore, we need to add our genomic_idx to the columns position. We are going to save the output of the reordering to a new data frame called rpkm_ordered.\n\n# Reorder the counts data frame to have the sample names in the same order as the metadata data frame\nrpkm_ordered  <- rpkm_data[ , genomic_idx]\n\nCheck and see what happened by clicking on the rpkm_ordered in the Environment window or using the View() function.\n\n# View the reordered counts\nView(rpkm_ordered)\n\nWe can see the sample names are now in a nice order from sample 1 to 12, just like the metadata. One thing to note is that you would never want to rearrange just the column names without the rest of the column because that would dissociate the sample name from it’s values.\nYou can also verify that column names of this new data matrix matches the metadata row names by using the all function:\n\nall(rownames(metadata) == colnames(rpkm_ordered))\n\n[1] TRUE\n\n\nNow that our samples are ordered the same in our metadata and counts data, if these were raw counts (not RPKM) we could proceed to perform differential expression analysis with this dataset.\n\nExercises: Adding data from biomaRt\nLet’s convert these ensembl ID’s into gene symbols. There are a number of ways to do this in R, but we will be using the biomaRt package. BiomaRt lets us easily map a variety of biological identifiers and choose a data source or ‘mart’. We can see a list of available dataset.\n\nlibrary(biomaRt, quietly = TRUE)\nlistEnsembl()\n\n        biomart                version\n1         genes      Ensembl Genes 109\n2 mouse_strains      Mouse strains 109\n3          snps  Ensembl Variation 109\n4    regulation Ensembl Regulation 109\n\n# For a reproducible analysis, it's good to always specify versions of databases\nensembl = useEnsembl(biomart=\"ensembl\",version=109)\nlistDatasets(ensembl)[100:110,]\n\n                      dataset                                   description\n100    mmmarmota_gene_ensembl               Alpine marmot genes (marMar2.1)\n101   mmonoceros_gene_ensembl                 Narwhal genes (NGI_Narwhal_1)\n102 mmoschiferus_gene_ensembl Siberian musk deer genes (MosMos_v2_BIUU_UCD)\n103     mmulatta_gene_ensembl                       Macaque genes (Mmul_10)\n104     mmurdjan_gene_ensembl       Pinecone soldierfish genes (fMyrMur1.1)\n105     mmurinus_gene_ensembl                  Mouse Lemur genes (Mmur_3.0)\n106    mmusculus_gene_ensembl                          Mouse genes (GRCm39)\n107  mnemestrina_gene_ensembl           Pig-tailed macaque genes (Mnem_1.0)\n108 mochrogaster_gene_ensembl                Prairie vole genes (MicOch1.0)\n109      mpahari_gene_ensembl           Shrew mouse genes (PAHARI_EIJ_v1.1)\n110       mpfuro_gene_ensembl                   Ferret genes (MusPutFur1.0)\n               version\n100          marMar2.1\n101      NGI_Narwhal_1\n102 MosMos_v2_BIUU_UCD\n103            Mmul_10\n104         fMyrMur1.1\n105           Mmur_3.0\n106             GRCm39\n107           Mnem_1.0\n108          MicOch1.0\n109    PAHARI_EIJ_v1.1\n110       MusPutFur1.0\n\n\nWe want to convert ensembl gene ID’s into MGI gene symbols. We can use the getBM function to get a dataframe of our mapped identifiers.\n\nensembl = useEnsembl(biomart=\"ensembl\", dataset=\"mmusculus_gene_ensembl\")\ngene_map <- getBM(filters= \"ensembl_gene_id\", attributes= c(\"ensembl_gene_id\",\"mgi_symbol\"), values = rownames(rpkm_data), mart=ensembl)\n\n\n\n\n\n\n\nBasic\n\n\n\n\nTry to replace the current rownames in rpkm_data with their mapped gene symbol. You may need to add a new column with the data instead.\nUse the match() function to subset the metadata data frame so that the row names of the metadata data frame match the column names of the `rpkm_data`` data frame.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#1\nind <- match(rownames(rpkm_data), gene_map$ensembl_gene_id)\n# rownames(rpkm_data) <- gene_map$mgi_symbol[ind] #oh no! duplicate rownames\nrpkm_data$gene <- gene_map$mgi_symbol[ind]\n\n#2\nidx <- match(colnames(rpkm_data), rownames(metadata))\nmetadata[idx, ]\n\n         genotype celltype replicate\nsample2        Wt    typeA         2\nsample5        KO    typeA         2\nsample7        Wt    typeB         1\nsample8        Wt    typeB         2\nsample9        Wt    typeB         3\nsample4        KO    typeA         1\nsample6        KO    typeA         3\nsample12       KO    typeB         3\nsample3        Wt    typeA         3\nsample11       KO    typeB         2\nsample10       KO    typeB         1\nsample1        Wt    typeA         1\nNA           <NA>     <NA>        NA\n\n\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nWe can use the listAttributes() and listFilters() functions to see what other information we can get using getBM. Choose another piece of data to add to rpkm_data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolutions will vary, but generally are similar to the basic and challenge solutions in how they get data and add it to the dataframe.\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nUse getBM to find all genes on chromosomes 2, 6, or 9. Create another dataframe only containing these genes.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Get the chomosome annotations\nchrom_map <- getBM(filters= \"ensembl_gene_id\", attributes= c(\"ensembl_gene_id\",\"chromosome_name\"), values = rownames(rpkm_data), mart=ensembl)\n\n#not strictly needed, but let's make chromosome a factor\nchrom_map$chromosome_name <- factor(chrom_map$chromosome_name)\n\n#We could either first map to rpkm_data and then filter, or filter chrom_map and then map to rpkm_data. Let's do the latter. \nchrom_map_269 <- chrom_map[chrom_map$chromosome_name %in% c(\"2\",\"6\",\"9\"),]\nrpkm_chrom29 <- rpkm_data[rownames(rpkm_data) %in% chrom_map_269$ensembl_gene_id,]\n\n\n\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session3/tidyverse.html",
    "href": "session3/tidyverse.html",
    "title": "19  Tidyverse",
    "section": "",
    "text": "20 Data Wrangling with Tidyverse\nThe Tidyverse suite of integrated packages are designed to work together to make common data science operations more user friendly. The packages have functions for data wrangling, tidying, reading/writing, parsing, and visualizing, among others. There is a freely available book, R for Data Science, with detailed descriptions and practical examples of the tools available and how they work together. We will explore the basic syntax for working with these packages, as well as, specific functions for data wrangling with the ‘dplyr’ package and data visualization with the ‘ggplot2’ package."
  },
  {
    "objectID": "session3/tidyverse.html#tidyverse-basics",
    "href": "session3/tidyverse.html#tidyverse-basics",
    "title": "19  Tidyverse",
    "section": "20.1 Tidyverse basics",
    "text": "20.1 Tidyverse basics\nThe Tidyverse suite of packages introduces users to a set of data structures, functions and operators to make working with data more intuitive, but is slightly different from the way we do things in base R. Two important new concepts we will focus on are pipes and tibbles.\nBefore we get started with pipes or tibbles, let’s load the library:\n\nlibrary(tidyverse)\n\n\n20.1.1 Pipes\nStringing together commands in R can be quite daunting. Also, trying to understand code that has many nested functions can be confusing.\nTo make R code more human readable, the Tidyverse tools use the pipe, %>%, which was acquired from the magrittr package and is now part of the dplyr package that is installed automatically with Tidyverse. The pipe allows the output of a previous command to be used as input to another command instead of using nested functions.\n\nNOTE: Shortcut to write the pipe is shift + command + M\n\nAn example of using the pipe to run multiple commands:\n\n## A single command\nsqrt(83)\n\n[1] 9.110434\n\n## Base R method of running more than one command\nround(sqrt(83), digits = 2)\n\n[1] 9.11\n\n## Running more than one command with piping\nsqrt(83) %>% round(digits = 2)\n\n[1] 9.11\n\n\nThe pipe represents a much easier way of writing and deciphering R code, and so we will be taking advantage of it, when possible, as we work through the remaining lesson.\n\n\n20.1.2 Tibbles\nA core component of the tidyverse is the tibble. Tibbles are a modern rework of the standard data.frame, with some internal improvements to make code more reliable. They are data frames, but do not follow all of the same rules. For example, tibbles can have numbers/symbols for column names, which is not normally allowed in base R.\nImportant: tidyverse is very opininated about row names. These packages insist that all column data (e.g. data.frame) be treated equally, and that special designation of a column as rownames should be deprecated. Tibble provides simple utility functions to handle rownames: rownames_to_column() and column_to_rownames().\nTibbles can be created directly using the tibble() function or data frames can be converted into tibbles using as_tibble(name_of_df).\n\nNOTE: The function as_tibble() will ignore row names, so if a column representing the row names is needed, then the function rownames_to_column(name_of_df) should be run prior to turning the data.frame into a tibble. Also, as_tibble() will not coerce character vectors to factors by default."
  },
  {
    "objectID": "session3/tidyverse.html#experimental-data",
    "href": "session3/tidyverse.html#experimental-data",
    "title": "19  Tidyverse",
    "section": "20.2 Experimental data",
    "text": "20.2 Experimental data\nWe’re going to explore the Tidyverse suite of tools to wrangle our data to prepare it for visualization. Make sure you have the file called gprofiler_results_Mov10oe.tsv.\nThe dataset:\n\nRepresents the functional analysis results, including the biological processes, functions, pathways, or conditions that are over-represented in a given list of genes.\nOur gene list was generated by differential gene expression analysis and the genes represent differences between control mice and mice over-expressing a gene involved in RNA splicing.\n\nThe functional analysis that we will focus on involves gene ontology (GO) terms, which:\n\ndescribe the roles of genes and gene products\norganized into three controlled vocabularies/ontologies (domains):\n\nbiological processes (BP)\ncellular components (CC)\nmolecular functions (MF)"
  },
  {
    "objectID": "session3/tidyverse.html#analysis-goal-and-workflow",
    "href": "session3/tidyverse.html#analysis-goal-and-workflow",
    "title": "19  Tidyverse",
    "section": "20.3 Analysis goal and workflow",
    "text": "20.3 Analysis goal and workflow\nGoal: Visually compare the most significant biological processes (BP) based on the number of associated differentially expressed genes (gene ratios) and significance values by creating the following plot:\n\n\n\ndotplot6\n\n\nTo wrangle our data in preparation for the plotting, we are going to use the Tidyverse suite of tools to wrangle and visualize our data through several steps:\n\nRead in the functional analysis results\nExtract only the GO biological processes (BP) of interest\nSelect only the columns needed for visualization\nOrder by significance (p-adjusted values)\nRename columns to be more intuitive\nCreate additional metrics for plotting (e.g. gene ratios)\nPlot results"
  },
  {
    "objectID": "session3/tidyverse.html#instructions",
    "href": "session3/tidyverse.html#instructions",
    "title": "19  Tidyverse",
    "section": "20.4 Instructions",
    "text": "20.4 Instructions\nFind a partner (or a group of 3 if needed). Choose one person to go through the following steps using Tidyverse, and the other using base R. It is recommended that the person with more experience attempt the steps in base R."
  },
  {
    "objectID": "session3/tidyverse.html#tidyverse-tools",
    "href": "session3/tidyverse.html#tidyverse-tools",
    "title": "19  Tidyverse",
    "section": "20.5 Tidyverse tools",
    "text": "20.5 Tidyverse tools\nWhile all of the tools in the Tidyverse suite are deserving of being explored in more depth, we are going to investigate more deeply the reading (readr), wrangling (dplyr), and plotting (ggplot2) tools."
  },
  {
    "objectID": "session3/tidyverse.html#read-in-the-functional-analysis-results",
    "href": "session3/tidyverse.html#read-in-the-functional-analysis-results",
    "title": "19  Tidyverse",
    "section": "20.6 1. Read in the functional analysis results",
    "text": "20.6 1. Read in the functional analysis results\n\nTidyverseBase R\n\n\nWhile the base R packages have perfectly fine methods for reading in data, the readr and readxl Tidyverse packages offer additional methods for reading in data. Let’s read in our tab-delimited functional analysis results gprofiler_results_Mov10oe.tsv using read_delim(). Name the dataframe functional_GO_results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Read in the functional analysis results\nfunctional_GO_results <- read_delim(file = \"../data/gprofiler_results_Mov10oe.tsv\", delim = \"\\t\" )\n\nRows: 3644 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): term.id, domain, term.name, intersection\ndbl (9): query.number, p.value, term.size, query.size, overlap.size, recall,...\nlgl (1): significant\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Take a look at the results\nhead(functional_GO_results)\n\n# A tibble: 6 × 14\n  query.…¹ signi…² p.value term.…³ query…⁴ overl…⁵ recall preci…⁶ term.id domain\n     <dbl> <lgl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl> <chr>   <chr> \n1        1 TRUE    0.00434     111    5850      52  0.009   0.468 GO:003… BP    \n2        1 TRUE    0.0033      110    5850      52  0.009   0.473 GO:003… BP    \n3        1 TRUE    0.0297       39    5850      21  0.004   0.538 GO:003… BP    \n4        1 TRUE    0.0193       70    5850      34  0.006   0.486 GO:003… BP    \n5        1 TRUE    0.0148       26    5850      16  0.003   0.615 GO:001… BP    \n6        1 TRUE    0.0187       22    5850      14  0.002   0.636 GO:008… BP    \n# … with 4 more variables: subgraph.number <dbl>, term.name <chr>,\n#   relative.depth <dbl>, intersection <chr>, and abbreviated variable names\n#   ¹​query.number, ²​significant, ³​term.size, ⁴​query.size, ⁵​overlap.size,\n#   ⁶​precision\n\n\n\n\n\n\n\nUse one of the base R read.X functions to read in the tab delimited file gprofiler_results_Mov10oe.tsv. Name the dataframe functional_GO_results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Read in the functional analysis results\nfunctional_GO_results <- read.delim(file = \"../data/gprofiler_results_Mov10oe.tsv\", sep = \"\\t\" )\n\n# Take a look at the results\nhead(functional_GO_results)\n\n\n\n\n\n\n\nDouble check the data types and format of your dataframe. Do the methods yield the same result? Convert anything you think should be a factor into a factor.\nNOTE: A large number of tidyverse functions will work with both tibbles and dataframes, and the data structure of the output will be identical to the input. However, there are some functions that will return a tibble (without row names), whether or not a tibble or dataframe is provided."
  },
  {
    "objectID": "session3/tidyverse.html#extract-only-the-go-biological-processes-bp-of-interest",
    "href": "session3/tidyverse.html#extract-only-the-go-biological-processes-bp-of-interest",
    "title": "19  Tidyverse",
    "section": "20.7 2. Extract only the GO biological processes (BP) of interest",
    "text": "20.7 2. Extract only the GO biological processes (BP) of interest\nNow that we have our data, we will need to wrangle it into a format ready for plotting. To extract the biological processes of interest, we only want those rows where the domain is equal to BP.\n\nTidyverseBase R\n\n\nFor all of our data wrangling steps we will be using tools from the dplyr package, which is a swiss-army knife for data wrangling of data frames.\nTo extract the biological processes of interest, we only want those rows where the domain is equal to BP, which we can do using the filter() function.\nTo filter rows of a data frame/tibble based on values in different columns, we give a logical expression as input to the filter() function to return those rows for which the expression is TRUE.\nPerform an additional filtering step to only keep those rows where the relative.depth is greater than 4.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Return only GO biological processes\nbp_oe <- functional_GO_results %>%\n  filter(domain == \"BP\")       %>% \n  filter(relative.depth > 4)\n\n\n\n\n\n\nUse a conditional expression and indexing ([]) to extract the rows where the domain is equal to BP.\nPerform an additional indexing step to only keep those rows where the relative.depth is greater than 4.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Return only GO biological processes\nidx <- functional_GO_results$domain == \"BP\"\nbp_oe2 <- functional_GO_results[idx,]\nbp_oe <- subset(bp_oe, relative.depth > 4)\n\n\n\n\n\n\n\nNow we have returned only those rows with a domain of BP. How have the dimensions of our results changed?"
  },
  {
    "objectID": "session3/tidyverse.html#select-only-the-columns-needed-for-visualization",
    "href": "session3/tidyverse.html#select-only-the-columns-needed-for-visualization",
    "title": "19  Tidyverse",
    "section": "20.8 3. Select only the columns needed for visualization",
    "text": "20.8 3. Select only the columns needed for visualization\nFor visualization purposes, we are only interested in the columns related to the GO terms, the significance of the terms, and information about the number of genes associated with the terms.\n\nTidyverseBase R\n\n\nTo extract columns from a data frame/tibble we can use the select() function. In contrast to base R, we do not need to put the column names in quotes for selection.\nSelect the columns term.id, term.name, p.value, query.size, term.size, overlap.size, intersection.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Selecting columns to keep\nbp_oe <- bp_oe %>%\n  select(term.id, term.name, p.value, query.size, term.size, overlap.size, intersection)\n\n\n\n\n\n\nIndex the columnsterm.id, term.name, p.value, query.size, term.size, overlap.size, intersection.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbp_oe <- bp_oe[, c(\"term.id\", \"term.name\", \"p.value\", \"query.size\", \"term.size\", \"overlap.size\", \"intersection\")]\n\n\n\n\n\n\n\nBoth indexing and the select() function also allows for negative selection. However, select allows for negative selection using column names, while in base R we can only do so with indexes. Note that we need to put the column names inside of the combine (c()) function with a - preceding it for this functionality.\nTo use column names in base R, we have to use %in%:\n# Selecting columns to keep\nidx <- !(colnames(functional_GO_results) %in% c(\"query.number\", \"significant\", \"recall\", \"precision\", \"subgraph.number\", \"relative.depth\", \"domain\"))"
  },
  {
    "objectID": "session3/tidyverse.html#order-go-processes-by-significance-adjusted-p-values",
    "href": "session3/tidyverse.html#order-go-processes-by-significance-adjusted-p-values",
    "title": "19  Tidyverse",
    "section": "20.9 4. Order GO processes by significance (adjusted p-values)",
    "text": "20.9 4. Order GO processes by significance (adjusted p-values)\nNow that we have only the rows and columns of interest, let’s arrange these by significance, which is denoted by the adjusted p-value.\n\nTidyverseBase R\n\n\nSort the rows by adjusted p-value with the arrange() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Order by adjusted p-value ascending\nbp_oe <- bp_oe %>%\n  arrange(p.value)\n\n\n\n\n\n\nSort the rows by adjusted p-value with the order() function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Order by adjusted p-value ascending\nidx <- order(bp_oe$p.value)\nbp_oe <- bp_oe[idx,]\n\n\n\n\n\n\n\nNOTE: If you wanted to arrange in descending order, then you could have run the following instead:\n# Order by adjusted p-value descending\nfunctional_GO_results <- functional_GO_results %>%\narrange(desc(p.value))\nNOTE: Ordering variables in ggplot2 is a bit different. This post introduces a few ways of ordering variables in a plot."
  },
  {
    "objectID": "session3/tidyverse.html#rename-columns-to-be-more-intuitive",
    "href": "session3/tidyverse.html#rename-columns-to-be-more-intuitive",
    "title": "19  Tidyverse",
    "section": "20.10 5. Rename columns to be more intuitive",
    "text": "20.10 5. Rename columns to be more intuitive\nWhile not necessary for our visualization, renaming columns more intuitively can help with our understanding of the data. Let’s rename the term.id and term.name columns.\n\nTidyverseBase R\n\n\nRename term.id and term.name to GO_id and GO_term using the rename function. Note that you may need to call rename as dplyr::rename, since rename is a common function name in other packages.\nThe syntax is new_name = old_name.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Provide better names for columns\nbp_oe <- bp_oe %>% \n  dplyr::rename(GO_id = term.id, \n                GO_term = term.name)\n\n\n\n\n\n\nRename term.id and term.name to GO_id and GO_term using colnames and indexing.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Provide better names for columns\ncolnames(bp_oe)[colnames(bp_oe) == \"term.id\"] <- \"GO_id\"\ncolnames(bp_oe)[colnames(bp_oe) == \"term.name\"] <- \"GO_term\""
  },
  {
    "objectID": "session3/tidyverse.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "href": "session3/tidyverse.html#create-additional-metrics-for-plotting-e.g.-gene-ratios",
    "title": "19  Tidyverse",
    "section": "20.11 6. Create additional metrics for plotting (e.g. gene ratios)",
    "text": "20.11 6. Create additional metrics for plotting (e.g. gene ratios)\nFinally, before we plot our data, we need to create a couple of additional metrics. Let’s generate gene ratios to reflect the number of DE genes associated with each GO process relative to the total number of DE genes.\nThis is calculated as gene_ratio = overlap.size / query.size.\n\nTidyverseBase R\n\n\nThe mutate() function enables you to create a new column from an existing column.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbp_oe <- bp_oe %>%\n  mutate(gene_ratio = overlap.size / query.size)\n\n\n\n\n\n\nCreate a new column in the dataframe using the $ syntax or cbind.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Create gene ratio column based on other columns in dataset\nbp_oe <- cbind(bp_oe, gene_ratio = bp_oe$overlap.size / bp_oe$query.size)\n\n\n\n\n\n\n\nThe mutate() function enables you to create a new column from an existing column."
  },
  {
    "objectID": "session3/tidyverse.html#compare-code",
    "href": "session3/tidyverse.html#compare-code",
    "title": "19  Tidyverse",
    "section": "20.12 Compare code",
    "text": "20.12 Compare code\nTake a look at your code verses your partner’s code. Which method do you think results in cleaner, more readable code? Which steps were easier in base R, and which in Tidyverse?"
  },
  {
    "objectID": "session3/tidyverse.html#making-the-plot",
    "href": "session3/tidyverse.html#making-the-plot",
    "title": "19  Tidyverse",
    "section": "20.13 Making the Plot",
    "text": "20.13 Making the Plot\nLet’s start by making a scatterplot of the top 30 terms:\n\nbp_plot <- bp_oe[1:30, ]\nggplot(bp_plot) +\n  geom_point(aes(x = overlap.size, y = p.value))\n\n\n\n\nHowever, instead of a scatterplot with numeric values on both axes, we would like to create a dotplot for visualizing the top 30 functional categories in our dataset, and how prevalent they are. Basically, we want a dotplot for visualizing functional analysis data, which plots the gene ratio values on the x-axis and the GO terms on the y-axis.\nLet’s see what happens when we add a non-numeric value to the y-axis and change the x-axis to the “gene_ratio” column:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term))\n\n\n\n\nNow that we have the required aesthetics, let’s add some extras like color to the plot. Let’s say we wanted to quickly visualize significance of the GO terms in the plot, we can color the points on the plot based on p-values, by specifying the column header.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = p.value))\n\n\n\n\nYou will notice that there are a default set of colors that will be used so we do not have to specify which colors to use. Also, the legend has been conveniently plotted for us!\nAlternatively, we could color number of DE genes associated with each term (overlap.size).\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = overlap.size))\n\n\n\n\nMoving forward, we are going to stick with coloring the dots based on the p.value column. Let’s explore some of the other arguments that can be specified in the geom layer.\nTo modify the size of the data points we can use the size argument. * If we add size inside aes() we could assign a numeric column to it and the size of the data points would change according to that column. * However, if we add size inside the geom_point() but outside aes() we can’t assign a column to it, instead we have to give it a numeric value. This use of size will uniformly change the size of all the data points.\n\nNote: This is true for several arguments, including color, shape etc. E.g. we can change all shapes to square by adding this argument to be outside the aes() function; if we put the argument inside the aes() function we could change the shape according to a (categorical) variable in our data frame or tibble.\n\nWe have decided that we want to change the size of all the data point to a uniform size instead of typing it to a numeric column in the input tibble. Add in the size argument by specifying a number for the size of the data point:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2)\n\n\n\n\n\nNote: The size of the points is personal preference, and you may need to play around with the parameter to decide which size is best. That seems a bit too small, so we can try out a slightly larger size.\n\nAs we do that, let’s see how we can change the shape of the data point. Different shapes are available, as detailed in the RStudio ggplot2 cheatsheet. Let’s explore this parameter by changing all of the points to squares:\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2, \n             shape = \"square\")\n\n\n\n\nNow we can start updating the plot to suit our preferences for how we want the data displayed. The labels on the x- and y-axis are also quite small and not very descriptive. To change their size and labeling, we need to add additional theme layers. The ggplot2 theme() system handles modification of non-data plot elements such as:\n\nAxis label aesthetics\nPlot background\nFacet label backround\nLegend appearance\n\nThere are built-in themes that we can use (i.e. theme_bw()) that mostly change the background/foreground colours, by adding it as additional layer. Alternatively, we can adjust specific elements of the current default theme by adding a theme() layer and passing in arguments for the things we wish to change. Or we can use both, a built-in theme layer and a custom theme layer!\nLet’s add a built-in theme layer theme_bw() first.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, , color = p.value), \n             size = 2) +\n  theme_bw()\n\n\n\n\nDo the axis labels or the tick labels get any larger by changing themes?\nNot in this case. But we can add arguments using theme() to change it ourselves. Since we are adding this layer on top (i.e later in sequence), any features we change will override what is set in the theme_bw(). Here we’ll increase the size of the axes labels to be 1.15 times the default size and the x-axis tick labels to be 1.15 times the default.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = p.value), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15)))\n\n\n\n\n\nNote #1: When modifying the size of text we often use the rel() function to specify the size we want relative to the default. We can also provide a numeric value as we did with the data point size, but it can be cumbersome if you don’t know what the default font size is to begin with.\nNote #2: You can use the example(\"geom_point\") function here to explore a multitude of different aesthetics and layers that can be added to your plot. As you scroll through the different plots, take note of how the code is modified. You can use this with any of the different geom layers available in ggplot2 to learn how you can easily modify your plots!\nNote #3: RStudio provides this very useful cheatsheet for plotting using ggplot2. Different example plots are provided and the associated code (i.e which geom or theme to use in the appropriate situation.)\n\n\n20.13.1 Customizing data point colors\nThe plot is looking better, but it is hard to distinguish differences in significance based on the colors used. There are cheatsheets available for specifying the base R colors by name or hexadecimal code. We could specify other colors available or use pre-created color palettes from an external R package.\nTo make additional color palettes available for plotting, we can load the RColorBrewer library, which contains color palettes designed specifically for the different types of data being compared.\n\n# Load the RColorBrewer library\nlibrary(RColorBrewer)\n\n# Check the available color palettes\ndisplay.brewer.all()\n\n\n\n\nThe output is separated into three sections based on the suggested palettes for sequential, qualitative, and diverging data.\n\nSequential palettes (top): For sequential data, with lighter colors for low values and darker colors for high values.\nQualitative palettes (middle): For categorical data, where the color does not denote differences in magnitude or value.\nDiverging palettes (bottom): For data with emphasis on mid-range values and extremes.\n\nSince our adjusted p-values are sequential, we will choose from these palettes. Let’s go with the “Yellow, orange, red” palette. We can choose how many colors from the palette to include, which may take some trial and error. We can test the colors included in a palette by using the display.brewer.pal() function, and changing if desired:\n\n# Testing the palette with six colors\ndisplay.brewer.pal(6, \"YlOrRd\")\n\n\n\n\nThe yellow might be a bit too light, and we might not need so many different colors. Let’s test with three different colors:\n\n# Testing the palette with three colors\ndisplay.brewer.pal(3, \"YlOrRd\")\n\n\n\n# Define a palette\nmypalette <- brewer.pal(3, \"YlOrRd\")\n\n# how are the colors represented in the mypalette vector?\nmypalette\n\n[1] \"#FFEDA0\" \"#FEB24C\" \"#F03B20\"\n\n\nThose colors look okay, so let’s test them in our plot. We can add a color scale layer, and most often one of the following two scales will work:\n\nscale_color_manual(): for categorical data or quantiles\nscale_color_gradient() family: for continuous data.\n\nBy default, scale_color_gradient() creates a two color gradient from low to high. Since we plan to use more colors, we will use the more flexible scale_color_gradientn() function. To make the legend a bit cleaner, we will also perform a -log10 transform on the p-values (higher values means more significant).\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = -log10(p.value)), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15))) +\n  xlab(\"Gene ratios\") +\n  ylab(\"Top 30 significant GO terms\") +\n  ggtitle(\"Dotplot of top 30 significant GO terms\") +\n  theme(plot.title = element_text(hjust=0.5, \n    face = \"bold\")) +\n  scale_color_gradientn(colors = mypalette)\n\n\n\n\nThis looks good, but we want to add better name for the legend and we want to make sure the legend title is centered and bold. To do this, we can add a name argument to scale_color_gradientn() and a new theme layer for the legend title.\n\nggplot(bp_plot) +\n  geom_point(aes(x = gene_ratio, y = GO_term, color = -log10(p.value)), \n             size = 2) +\n  theme_bw() +\n  theme(axis.text.x = element_text(size=rel(1.15)),\n        axis.title = element_text(size=rel(1.15))) +\n  xlab(\"Gene ratios\") +\n  ylab(\"Top 30 significant GO terms\") +\n  ggtitle(\"Dotplot of top 30 significant GO terms\") +\n  theme(plot.title = element_text(hjust=0.5, \n    face = \"bold\")) +\n  scale_color_gradientn(name = \"Significance \\n (-log10(padj))\", colors = mypalette) +\n  theme(legend.title = element_text(size=rel(1.15),\n    hjust=0.5, \n    face=\"bold\"))"
  },
  {
    "objectID": "session3/tidyverse.html#additional-resources",
    "href": "session3/tidyverse.html#additional-resources",
    "title": "19  Tidyverse",
    "section": "20.14 Additional resources",
    "text": "20.14 Additional resources\n\nR for Data Science\nteach the tidyverse\ntidy style guide\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session3/pSet3.html#instructions",
    "href": "session3/pSet3.html#instructions",
    "title": "20  Problem Set 3",
    "section": "20.1 Instructions",
    "text": "20.1 Instructions\nIn this problem set, you will be going through an analysis resolve a potential label swap in phosphoproteomic mass spec data.\nIt is recommended to create a Quarto notebook for your report. You can create a new notebook in RSTudio by going to file->new file->quarto document. Set the default output to be a PDF. As an example, the entire workbook is a quarto document. More information can be found here. However, if you are finding it difficult to render your document, feel free to instead submit a script and separate writeup document."
  },
  {
    "objectID": "session3/pSet3.html#data-description",
    "href": "session3/pSet3.html#data-description",
    "title": "20  Problem Set 3",
    "section": "20.2 Data Description",
    "text": "20.2 Data Description\nWe are collaborating with a lab that is studying phosphorylation changes during covid infection. This dataset consists of phosphoproteomic TMT mass spec data from 2 10-plexes. We took samples at 0, 5, and 60 minutes post-infection. We also wanted to explore the specific role of 2 genes thought to be used in covid infection, RAB7A and NPC1. To do this, we included cell lines with each of these genes knocked out.\nWe wanted to have 2 replicates for each condition we were looking at, so we have a total of 3X2X3 or 18 different samples we want to measure. We decide to replicate wild type at 0 minutes in each 10plex for our total 20 wells accross the 2 10-plexes.\nOur collaborator has alerted us that there may have been a label swap in the dataset. We need to see if we can find two samples which seem to have been swapped, and correct the error if we feel confident that we know what swap took place.\nNote: This data has been adapted with permission from an unpublished study. The biological context of the original data has been changed, and all gene names were shuffled."
  },
  {
    "objectID": "session3/pSet3.html#loading-data",
    "href": "session3/pSet3.html#loading-data",
    "title": "20  Problem Set 3",
    "section": "20.3 Loading Data",
    "text": "20.3 Loading Data\nLoad in the data phospho_exp2_safe.csv and phospho_exp2_safe.csv.\nThere are two variables of interest, the time, 0, 5, or 60 minutes post-infection, and the genotype, WT, NPC1 knockout and RAB7A knockout.\nUnfortunately, all of this data is embedded in the column names of the dataset.\nCreate a metadata_plex# dataframes to contain this data instead. You can try to do this programatically from the column names, or you can type out the data manually.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First we load the data normally\n\nplex2_data <- read.csv(\"../data/phospho_exp2_safe.csv\")\nplex3_data <- read.csv(\"../data/phospho_exp3_safe.csv\")\n\n#We'll use the stringr library to split up the column names\nlibrary(stringr, quietly = TRUE)\n\nWarning: package 'stringr' was built under R version 4.2.2\n\nmake_metadata <- function(in_names){\n  split_names <- str_split_fixed(in_names, \"_\", 3)\n  metadata <- data.frame(split_names)\n  #paste0 lets us concatenate strings\n  rownames(metadata) <- paste0('sample', rownames(metadata))\n  colnames(metadata) <- c(\"condition\",\"time\",\"replicate\")\n  metadata$condition <- factor(metadata$condition)\n  metadata$time <- factor(metadata$time, levels = c(\"0Min\",\"5Min\",\"60Min\"))\n  return(metadata)\n}\n\nmetadata_plex2 <- make_metadata(colnames(plex2_data[,6:15]))\nmetadata_plex3 <- make_metadata(colnames(plex3_data[,6:15]))\n\ncolnames(plex2_data)[6:15] <- rownames(metadata_plex2)\ncolnames(plex3_data)[6:15] <- rownames(metadata_plex3)"
  },
  {
    "objectID": "session3/pSet3.html#pca",
    "href": "session3/pSet3.html#pca",
    "title": "20  Problem Set 3",
    "section": "20.4 PCA",
    "text": "20.4 PCA\nAs an initial quality check, let’s run PCA on our data. We can use prcomp to run pca, and autoplot to plot the result. Let’s try making 2 pca plots, 1 for each 10plex. We can set the color equal to the genotype and the shape of the points equal to the time.\nYou can call prcomp and autoplot like this:\nlibrary(ggfortify)\n#PCA Plots\npca_res2 <- prcomp(plex2_data, scale = FALSE)\nautoplot(pca_res2, data=metadata_plex2, colour = 'condition', shape='time', size=3)\nHint: prcomp might be expecting data in a wide format as opposed to a long format, meaning that we need to make each peptide a column and each row a sample. We can use the t() function and convert the result to a dataframe to get our data into this format.\nNote: You may need to set the scale parameter to FALSE to avoid an error in prcomp.\nWe should look at how our replicates are clustered. Does everything look good in both 10-plexes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to transpose the numeric parts of the data in order to run PCA on it.\n\nlibrary(ggfortify)\n\nWarning: package 'ggfortify' was built under R version 4.2.2\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\npca_res2 <- prcomp(t(plex2_data[,6:15]), scale = FALSE)\nautoplot(pca_res2, data=metadata_plex2, colour = 'condition', shape='time', size=3)\n\n\n\npca_res3 <- prcomp(t(plex3_data[,6:15]), scale = FALSE)\nautoplot(pca_res3, data=metadata_plex3, colour = 'condition', shape='time', size=3)\n\n\n\n\nAt first glance, both plots look messy. However, when interpreting a PCA plot is important to note how much variance is explained by each principle component. On both of these, the 1st PC explains over 80% of the variance, while the second less than 10%. Thus, we care much more (as in 8 times more) about the X axis than the Y axis.\nIn both plots, we see much stronger time point clustering than condition clustering, given how muchn more important the horizontal axis is. However, in plex3 there is one 60 minute point with the 0 minute points, and vice versa."
  },
  {
    "objectID": "session3/pSet3.html#heatmaps",
    "href": "session3/pSet3.html#heatmaps",
    "title": "20  Problem Set 3",
    "section": "20.5 Heatmaps",
    "text": "20.5 Heatmaps\nLet’s explore this more by looking at some heatmaps of our data. We can use the heatmap function to plot a heatmap of the correlation between each of the samples in each 10plex.\nBelow is how to calculate the correlation and call the heatmap function. You can try to use the RowSideColors argument or change the column names to improve the visualization.\nheatmap(x=cor(plex2_data))\nHint: heatmap only accepts numeric columns.\nIs there anything unexpected in how the samples have clustered here?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nheatmap(x=cor(plex2_data[,6:15]))\n\n\n\nheatmap(x=cor(plex3_data[,6:15]))\n\n\n\n\nAt first glance our heatmaps look alright, but that is because they have been clustered automatically by the heatmap function. We can tell heatmap not to cluster to see things better or use the RowSideColors argument.\nNot clustering:\n\nheatmap(x=cor(plex2_data[,6:15]), Rowv=NA, Colv=NA)\n\n\n\nheatmap(x=cor(plex3_data[,6:15]), Rowv=NA, Colv=NA)\n\n\n\n\nUsing RowSideColors:\n\nlibrary(RColorBrewer)\ncolSide2 <- brewer.pal(3, \"Set1\")[metadata_plex2$time]\nheatmap(x=cor(plex2_data[,6:15]), RowSideColors = colSide2)\n\n\n\ncolSide3 <- brewer.pal(3, \"Set1\")[metadata_plex3$time]\nheatmap(x=cor(plex3_data[,6:15]), RowSideColors = colSide3)\n\n\n\n\nIn both versions we clearly see a single sample in plex 3 looking to be out of place."
  },
  {
    "objectID": "session3/pSet3.html#resolving-the-issue",
    "href": "session3/pSet3.html#resolving-the-issue",
    "title": "20  Problem Set 3",
    "section": "20.6 Resolving the issue",
    "text": "20.6 Resolving the issue\nDecide what to do about the potential label swap and explain your reasoning. You could declare there to be too much uncertainty and report to your collaborator that they will have to redo the experiment, decide there is no label swap, or correct a label swap and continue the analysis.\nDo you feel confident enough to continue the analysis, or is there too much uncertainty to use this data? What other factors might influence your decision?\nIf there is additional analysis you want to perform or calculations you want to make to support your answer, feel free to do so. If you are unsure how to perform that analysis or it would be outside the scope of a problem set, instead describe what you would do and how you would use the results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are a number of ‘correct’ answers here. It makes sense to redo the experiment, gain more biological context/knowledge, try to construct a statistical test, or other directions.\nIn reality, we ultimately determined that there was a label swap between samples 3 and 5 in 10plex 3. This was based on the irregular time series clustering, and that it appeared that time series clustering was significantly stronger than condition clustering. For the real experiment we also had a third 10plex with a slightly different design and corresponding proteomic measurements for all 3 10plexes which we could confirm the clustering patterns with.\nHowever, we also were okay correcting this swap and moving forward because this was for an exploratory analysis. We ultimately were using this data to generate hypothesis and perform targeted experiments based on what seemed unusual. Thus, it wasn’t the end of the world if a label swap slipped through, since we were not drawing any concrete conclusions from the data. If this had been a final experiment testing a specific hypothesis, we would have redone it."
  },
  {
    "objectID": "session4/session4.html#learning-objectives",
    "href": "session4/session4.html#learning-objectives",
    "title": "Session 4: Data visualization in R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCreate and export histograms, boxplots, line plots, and scatter plots using ggplot2.\nApply basic linear models and ANOVA tests.\nExplore analysis-specific common plots such as volcano plots, heatmaps, clustergrams, networks, and set enrichment visualizations.\nDefine R data structures.\nUse SummarizedExperiment objects."
  },
  {
    "objectID": "session4/session4.html#directions-for-feb.-24",
    "href": "session4/session4.html#directions-for-feb.-24",
    "title": "Session 4: Data visualization in R",
    "section": "Directions for Feb. 24",
    "text": "Directions for Feb. 24\nFor today please:\n\nGo through the final exercise, Adding data from biomaRt , in the Matching and Reordering Data in R lesson from Session 3.\nGo through the session 4 chapters in the order they appear. Complete the ggplot2 exercises.\nIf there is time, look through the Tidyverse lesson from session 3. Code to plot the enrichment results has been added.\nBegin problem set 4."
  },
  {
    "objectID": "session4/linear_models.html#returning-to-count-data",
    "href": "session4/linear_models.html#returning-to-count-data",
    "title": "21  Linear Models",
    "section": "21.1 Returning to count data",
    "text": "21.1 Returning to count data\n\nlibrary(tidyverse)\nlibrary(pasilla)\n\n\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\n\nRows: 7 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): file, condition, type, total number of reads\ndbl (2): number of lanes, exon counts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\n\nLet’s assume that in addition to the siRNA knockdown of the pasilla gene, we also want to test the effect of a certain drug. We could then envisage an experiment in which the experimenter treats the cells either with negative control, with the siRNA against pasilla, with the drug, or with both. To analyse this experiment, we can use the notation:\n\\[\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_2\n\\]\nThis equation can be parsed as follows. The left hand side, \\(y\\) , is the experimental measurement of interest. In our case, this is the suitably transformed expression level of a gene. Since in an RNA-Seq experiment there are lots of genes, we’ll have as many copies of Equation the above equation, one for each. The coefficient \\(\\beta_0\\) is the base level of the measurement in the negative control; often it is called the intercept.\nThe design factors \\(x_1\\) and \\(x_2\\) and are binary indicator variables, sometimes called dummy variables: \\(x_1\\) takes the value 1 if the siRNA was transfected and 0 if not, and similarly, \\(x_2\\) indicates whether the drug was administered. In the experiment where only the siRNA is used, \\(x_1 = 1\\) and \\(x_2 = 0\\), and the third and fourth terms of the equation vanish. Then, the equation simplifies to \\(y = \\beta+0 + \\beta_1\\). This means that \\(\\beta_1\\) represents the difference between treatment and control.\nWe can succinctly encode the design of the experiment in the design matrix. For instance, for the combinatorial experiment described above, the design matrix is\n\n\n\nx_0\nx_1\nx_2\n\n\n\n\n1\n0\n0\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n1\n1\n1\n\n\n\nMany R packges such as limma and edgeR use the design matrix to represent experimental design.\nThe columns of the design matrix correspond to the experimental factors, and its rows represent the different experimental conditions, four in our case since we are including an interaction effect.\nHowever, for the pasilla data we’re not done yet. While the above equation would function if our data was perfect, in reality we have small differences between our replicates and other sources of variation in our data. We need to slightly extend the equation,\n\\[\ny = x_{j0}\\beta_0 + x_{j1}\\beta_1 + x_{j2}\\beta_2 + x_{j1}x_{j2}\\beta_2 + \\epsilon_j\n\\]\nWe have added the index \\(j\\) and a new term \\(\\epsilon_j\\). The index \\(j\\) now explicitly counts over our individual replicate experiments; for instance, if for each of the four conditions we perform three replicates, then \\(j\\) counts from 1 to 12. The design matrix has now 12 rows, and \\(x_{jk}\\) is the value of the matrix in its \\(j\\)th row and \\(k\\)th column. The additional terms \\(\\epsilon_j\\), which we call the residuals, are there to absorb differences between replicates. Under the assumptions of our experimental design, we require the residuals to be small. For instance, we can minimize the sum of the square of all the residuals, which is called least sum of squares fitting. The R function lm performs least squares."
  },
  {
    "objectID": "session4/linear_models.html#defining-linear-models",
    "href": "session4/linear_models.html#defining-linear-models",
    "title": "21  Linear Models",
    "section": "21.2 Defining linear models",
    "text": "21.2 Defining linear models\nThe above is an example of a linear model. A linear model is a model for a continuous outcome Y of the form \\[Y = \\beta_0 + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\dots + \\beta_{p}X_{p} + \\epsilon\\] The covariates X can be:\n\na continuous variable (age, weight, temperature, etc.)\nDummy variables coding a categorical covariate (more later)\n\nThe \\(\\beta\\)’s are unknown parameters to be estimated.\nThe error term \\(\\epsilon\\) is assumed to be normally distributed with a variance that is constant across the range of the data.\nModels with all categorical covariates are referred to as ANOVA models and models with continuous covariates are referred to as linear regression models. These are all linear models, and R doesn’t distinguish between them.\nWe have already seen the t-test, but it can also be viewed as an application of the general linear model. In this case, the model would look like this:\n\\[\n{y} = {\\beta_1}*x_1 + {\\beta_0}\n\\] Many of the statistical tests we have seen can be represented as special cases of linear models."
  },
  {
    "objectID": "session4/linear_models.html#linear-models-in-r",
    "href": "session4/linear_models.html#linear-models-in-r",
    "title": "21  Linear Models",
    "section": "21.3 Linear models in R",
    "text": "21.3 Linear models in R\nR uses the function lm to fit linear models.\nRead in ’lm_example_data.csv`:\n\ndat <- read.csv(\"https://raw.githubusercontent.com/ucdavis-bioinformatics-training/2018-September-Bioinformatics-Prerequisites/master/friday/lm_example_data.csv\")\nhead(dat)\n\n  sample expression  batch treatment  time temperature\n1      1  1.2139625 Batch1         A time1    11.76575\n2      2  1.4796581 Batch1         A time2    12.16330\n3      3  1.0878287 Batch1         A time1    10.54195\n4      4  1.4438585 Batch1         A time2    10.07642\n5      5  0.6371621 Batch1         A time1    12.03721\n6      6  2.1226740 Batch1         B time2    13.49573\n\nstr(dat)\n\n'data.frame':   25 obs. of  6 variables:\n $ sample     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ expression : num  1.214 1.48 1.088 1.444 0.637 ...\n $ batch      : chr  \"Batch1\" \"Batch1\" \"Batch1\" \"Batch1\" ...\n $ treatment  : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ time       : chr  \"time1\" \"time2\" \"time1\" \"time2\" ...\n $ temperature: num  11.8 12.2 10.5 10.1 12 ...\n\n\nFit a linear model using expression as the outcome and treatment as a categorical covariate:\n\noneway.model <- lm(expression ~ treatment, data = dat)\n\nIn R model syntax, the outcome is on the left side, with covariates (separated by +) following the ~\n\noneway.model\n\n\nCall:\nlm(formula = expression ~ treatment, data = dat)\n\nCoefficients:\n(Intercept)   treatmentB   treatmentC   treatmentD   treatmentE  \n     1.1725       0.4455       0.9028       2.5537       7.4140  \n\nclass(oneway.model)\n\n[1] \"lm\"\n\n\nWe can look at the design matrix:\n\nX <- model.matrix(~treatment, data = dat)\nX\n\n   (Intercept) treatmentB treatmentC treatmentD treatmentE\n1            1          0          0          0          0\n2            1          0          0          0          0\n3            1          0          0          0          0\n4            1          0          0          0          0\n5            1          0          0          0          0\n6            1          1          0          0          0\n7            1          1          0          0          0\n8            1          1          0          0          0\n9            1          1          0          0          0\n10           1          1          0          0          0\n11           1          0          1          0          0\n12           1          0          1          0          0\n13           1          0          1          0          0\n14           1          0          1          0          0\n15           1          0          1          0          0\n16           1          0          0          1          0\n17           1          0          0          1          0\n18           1          0          0          1          0\n19           1          0          0          1          0\n20           1          0          0          1          0\n21           1          0          0          0          1\n22           1          0          0          0          1\n23           1          0          0          0          1\n24           1          0          0          0          1\n25           1          0          0          0          1\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$treatment\n[1] \"contr.treatment\"\n\n\nNote that this is a one-way ANOVA model.\nsummary() applied to an lm object will give p-values and other relevant information:\n\nsummary(oneway.model)\n\n\nCall:\nlm(formula = expression ~ treatment, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.5353  0.1790  0.7725  3.6114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.1725     0.7783   1.506    0.148    \ntreatmentB    0.4455     1.1007   0.405    0.690    \ntreatmentC    0.9028     1.1007   0.820    0.422    \ntreatmentD    2.5537     1.1007   2.320    0.031 *  \ntreatmentE    7.4140     1.1007   6.735 1.49e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 20 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7033 \nF-statistic: 15.22 on 4 and 20 DF,  p-value: 7.275e-06\n\n\nIn the output:\n\n“Coefficients” refer to the \\(\\beta\\)’s\n“Estimate” is the estimate of each coefficient\n“Std. Error” is the standard error of the estimate\n“t value” is the coefficient divided by its standard error\n“Pr(>|t|)” is the p-value for the coefficient\nThe residual standard error is the estimate of the variance of \\(\\epsilon\\)\nDegrees of freedom is the sample size minus # of coefficients estimated\nR-squared is (roughly) the proportion of variance in the outcome explained by the model\nThe F-statistic compares the fit of the model as a whole to the null model (with no covariates)\n\ncoef() gives you model coefficients:\n\ncoef(oneway.model)\n\n(Intercept)  treatmentB  treatmentC  treatmentD  treatmentE \n  1.1724940   0.4455249   0.9027755   2.5536669   7.4139642 \n\n\nWhat do the model coefficients mean?\nBy default, R uses reference group coding or “treatment contrasts”. For categorical covariates, the first level alphabetically (or first factor level) is treated as the reference group. The reference group doesn’t get its own coefficient, it is represented by the intercept. Coefficients for other groups are the difference from the reference:\nFor our simple design:\n\n(Intercept) is the mean of expression for treatment = A\ntreatmentB is the mean of expression for treatment = B minus the mean for treatment = A\ntreatmentC is the mean of expression for treatment = C minus the mean for treatment = A\netc.\n\n\n# Get means in each treatment\ntreatmentmeans <- tapply(dat$expression, dat$treatment, mean)\ntreatmentmeans[\"A\"] \n\n       A \n1.172494 \n\n# Difference in means gives you the \"treatmentB\" coefficient from oneway.model\ntreatmentmeans[\"B\"] - treatmentmeans[\"A\"] \n\n        B \n0.4455249 \n\n\nWhat if you don’t want reference group coding? Another option is to fit a model without an intercept:\n\nno.intercept.model <- lm(expression ~ 0 + treatment, data = dat) # '0' means 'no intercept' here\nsummary(no.intercept.model)\n\n\nCall:\nlm(formula = expression ~ 0 + treatment, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.5353  0.1790  0.7725  3.6114 \n\nCoefficients:\n           Estimate Std. Error t value Pr(>|t|)    \ntreatmentA   1.1725     0.7783   1.506 0.147594    \ntreatmentB   1.6180     0.7783   2.079 0.050717 .  \ntreatmentC   2.0753     0.7783   2.666 0.014831 *  \ntreatmentD   3.7262     0.7783   4.787 0.000112 ***\ntreatmentE   8.5865     0.7783  11.032 5.92e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.74 on 20 degrees of freedom\nMultiple R-squared:  0.8878,    Adjusted R-squared:  0.8598 \nF-statistic: 31.66 on 5 and 20 DF,  p-value: 7.605e-09\n\ncoef(no.intercept.model)\n\ntreatmentA treatmentB treatmentC treatmentD treatmentE \n  1.172494   1.618019   2.075270   3.726161   8.586458 \n\n\nWithout the intercept, the coefficients here estimate the mean in each level of treatment:\n\ntreatmentmeans\n\n       A        B        C        D        E \n1.172494 1.618019 2.075270 3.726161 8.586458 \n\n\nThe no-intercept model is the SAME model as the reference group coded model, in the sense that it gives the same estimate for any comparison between groups:\nTreatment B - treatment A, reference group coded model:\n\ncoefs <- coef(oneway.model)\ncoefs[\"treatmentB\"]\n\ntreatmentB \n 0.4455249 \n\n\nTreatment B - treatment A, no-intercept model:\n\ncoefs <- coef(no.intercept.model)\ncoefs[\"treatmentB\"] - coefs[\"treatmentA\"]\n\ntreatmentB \n 0.4455249"
  },
  {
    "objectID": "session4/linear_models.html#batch-adjustment",
    "href": "session4/linear_models.html#batch-adjustment",
    "title": "21  Linear Models",
    "section": "21.4 Batch Adjustment",
    "text": "21.4 Batch Adjustment\nSuppose we want to adjust for batch differences in our model. We do this by adding the covariate “batch” to the model formula:\n\nbatch.model <- lm(expression ~ treatment + batch, data = dat)\nsummary(batch.model)\n\n\nCall:\nlm(formula = expression ~ treatment + batch, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9310 -0.8337  0.0415  0.7725  3.6114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.1725     0.7757   1.512 0.147108    \ntreatmentB    0.4455     1.0970   0.406 0.689186    \ntreatmentC    1.9154     1.4512   1.320 0.202561    \ntreatmentD    4.2414     1.9263   2.202 0.040231 *  \ntreatmentE    9.1017     1.9263   4.725 0.000147 ***\nbatchBatch2  -1.6877     1.5834  -1.066 0.299837    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.735 on 19 degrees of freedom\nMultiple R-squared:  0.7667,    Adjusted R-squared:  0.7053 \nF-statistic: 12.49 on 5 and 19 DF,  p-value: 1.835e-05\n\ncoef(batch.model)\n\n(Intercept)  treatmentB  treatmentC  treatmentD  treatmentE batchBatch2 \n  1.1724940   0.4455249   1.9153967   4.2413688   9.1016661  -1.6877019 \n\n\nFor a model with more than one coefficient, summary provides estimates and tests for each coefficient adjusted for all the other coefficients in the model."
  },
  {
    "objectID": "session4/linear_models.html#two-factor-analysis",
    "href": "session4/linear_models.html#two-factor-analysis",
    "title": "21  Linear Models",
    "section": "21.5 Two-factor analysis",
    "text": "21.5 Two-factor analysis\nSuppose our experiment involves two factors, treatment and time. lm can be used to fit a two-way ANOVA model:\n\ntwoway.model <- lm(expression ~ treatment*time, data = dat)\nsummary(twoway.model)\n\n\nCall:\nlm(formula = expression ~ treatment * time, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0287 -0.4463  0.1082  0.4915  1.7623 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           0.97965    0.69239   1.415  0.17752    \ntreatmentB            0.40637    1.09476   0.371  0.71568    \ntreatmentC            1.00813    0.97918   1.030  0.31953    \ntreatmentD            3.07266    1.09476   2.807  0.01328 *  \ntreatmentE            9.86180    0.97918  10.071 4.55e-08 ***\ntimetime2             0.48211    1.09476   0.440  0.66594    \ntreatmentB:timetime2 -0.09544    1.54822  -0.062  0.95166    \ntreatmentC:timetime2 -0.26339    1.54822  -0.170  0.86718    \ntreatmentD:timetime2 -1.02568    1.54822  -0.662  0.51771    \ntreatmentE:timetime2 -6.11958    1.54822  -3.953  0.00128 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.199 on 15 degrees of freedom\nMultiple R-squared:  0.912, Adjusted R-squared:  0.8591 \nF-statistic: 17.26 on 9 and 15 DF,  p-value: 2.242e-06\n\ncoef(twoway.model)\n\n         (Intercept)           treatmentB           treatmentC \n          0.97965110           0.40636785           1.00813264 \n          treatmentD           treatmentE            timetime2 \n          3.07265513           9.86179766           0.48210723 \ntreatmentB:timetime2 treatmentC:timetime2 treatmentD:timetime2 \n         -0.09544075          -0.26339279          -1.02568281 \ntreatmentE:timetime2 \n         -6.11958364 \n\n\nThe notation treatment*time refers to treatment, time, and the interaction effect of treatment by time.\nInterpretation of coefficients:\n\nEach coefficient for treatment represents the difference between the indicated group and the reference group at the reference level for the other covariates\nFor example, “treatmentB” is the difference in expression between treatment B and treatment A at time 1\nSimilarly, “timetime2” is the difference in expression between time2 and time1 for treatment A\nThe interaction effects (coefficients with “:”) estimate the difference between treatment groups in the effect of time\nThe interaction effects ALSO estimate the difference between times in the effect of treatment\n\nTo estimate the difference between treatment B and treatment A at time 2, we need to include the interaction effects:\n\n# A - B at time 2\ncoefs <- coef(twoway.model)\ncoefs[\"treatmentB\"] + coefs[\"treatmentB:timetime2\"]\n\ntreatmentB \n 0.3109271 \n\n\nWe can see from summary that one of the interaction effects is significant. Here’s what that interaction effect looks like graphically:\n\ninteraction.plot(x.factor = dat$time, trace.factor = dat$treatment, response = dat$expression)\n\n\n\n\nIn the pasilla data, we can consider the affects of both the type and condition variables.\n\npasillaTwoFactor = pasilla\ndesign(pasillaTwoFactor) = formula(~ type + condition)\npasillaTwoFactor = DESeq(pasillaTwoFactor)\n\nWe access the results using the results function, which returns a dataframe with the statistics of each gene.\n\nres2 = results(pasillaTwoFactor)\nhead(res2, n = 3)\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE       stat    pvalue      padj\n            <numeric>      <numeric> <numeric>  <numeric> <numeric> <numeric>\nFBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA\nFBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975\nFBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA\n\n\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material. and the UCDavis Bioinformatics Core"
  },
  {
    "objectID": "session4/ggplot.html#data-visualization-with-ggplot2",
    "href": "session4/ggplot.html#data-visualization-with-ggplot2",
    "title": "22  Data Visualization in R",
    "section": "22.1 Data Visualization with ggplot2",
    "text": "22.1 Data Visualization with ggplot2\n\nFor this lesson, you will need the new_metadata data frame. Load it into your environment as follows:\n\n## load the new_metadata data frame into your environment from a .RData object\nload(\"../data/new_metadata.RData\")\n\nNext, let’s check if it was successfully loaded into the environment:\n\n# this data frame should have 12 rows and 5 columns\nView(new_metadata)\n\n\nWhen we are working with large sets of numbers it can be useful to display that information graphically to gain more insight. In this lesson we will be plotting with the popular Bioconductor package ggplot2.\nThe ggplot2 syntax takes some getting used to, but once you get it, you will find it’s extremely powerful and flexible. We will start with drawing a simple x-y scatterplot of samplemeans versus age_in_days from the new_metadata data frame. Please note that ggplot2 expects a dataframe or a tibble (the Tidyverse version of a dataframe) as input.\nLet’s start by loading the ggplot2 library:\n\nlibrary(ggplot2)\n\nThe ggplot() function is used to initialize the basic graph structure, then we add to it. The basic idea is that you specify different parts of the plot using additional functions one after the other and combine them into a “code chunk” using the + operator; the functions in the resulting code chunk are called layers.\nLet’s start:\n\nload(\"../data/new_metadata.RData\")\nggplot(new_metadata) # what happens? \n\n\n\n\nYou get an blank plot, because you need to specify additional layers using the + operator.\nThe geom (geometric) object is the layer that specifies what kind of plot we want to draw. A plot must have at least one geom; there is no upper limit. Examples include:\n\npoints (geom_point, geom_jitter for scatter plots, dot plots, etc)\nlines (geom_line, for time series, trend lines, etc)\nboxplot (geom_boxplot, for, well, boxplots!)\n\nLet’s add a “geom” layer to our plot using the + operator, and since we want a scatter plot so we will use geom_point().\nggplot(new_metadata) +\n  geom_point() # note what happens here\nWhy do we get an error? Is the error message easy to decipher?\nWe get an error because each type of geom usually has a required set of aesthetics to be set. “Aesthetics” are set with the aes() function and can be set either nested within geom_point() (applies only to that layer) or within ggplot() (applies to the whole plot).\nThe aes() function has many different arguments, and all of those arguments take columns from the original data frame as input. It can be used to specify many plot elements including the following:\n\nposition (i.e., on the x and y axes)\ncolor (“outside” color)\nfill (“inside” color)\nshape (of points)\nlinetype\nsize\n\nTo start, we will specify x- and y-axis since geom_point requires the most basic information about a scatterplot, i.e. what you want to plot on the x and y axes. All of the other plot elements mentioned above are optional.\n\nggplot(new_metadata) +\n     geom_point(aes(x = age_in_days, y= samplemeans))\n\n\n\n\nNow that we have the required aesthetics, let’s add some extras like color to the plot. We can color the points on the plot based on the genotype column within aes(). You will notice that there are a default set of colors that will be used so we do not have to specify. Note that the legend has been conveniently plotted for us.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype)) \n\n\n\n\nLet’s try to have both celltype and genotype represented on the plot. To do this we can assign the shape argument in aes() the celltype column, so that each celltype is plotted with a different shaped data point.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype)) \n\n\n\n\nThe data points are quite small. We can adjust the size of the data points within the geom_point() layer, but it should not be within aes() since we are not mapping it to a column in the input data frame, instead we are just specifying a number.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) \n\n\n\n\nThe labels on the x- and y-axis are also quite small and hard to read. To change their size, we need to add an additional theme layer. The ggplot2 theme system handles non-data plot elements such as:\n\nAxis label aesthetics\nPlot background\nFacet label backround\nLegend appearance\n\nThere are built-in themes we can use (i.e. theme_bw()) that mostly change the background/foreground colours, by adding it as additional layer. Or we can adjust specific elements of the current default theme by adding the theme() layer and passing in arguments for the things we wish to change. Or we can use both.\nLet’s add a layer theme_bw().\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=3.0) +\n  theme_bw() \n\n\n\n\nDo the axis labels or the tick labels get any larger by changing themes?\nNo, they don’t. But, we can add arguments using theme() to change the size of axis labels ourselves. Since we will be adding this layer “on top”, or after theme_bw(), any features we change will override what is set by the theme_bw() layer.\nLet’s increase the size of both the axes titles to be 1.5 times the default size. When modifying the size of text the rel() function is commonly used to specify a change relative to the default.\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) +\n  theme_bw() +\n  theme(axis.title = element_text(size=rel(1.5)))           \n\n\n\n\nWe can also make a boxplot of the data:"
  },
  {
    "objectID": "session4/ggplot.html#histogram",
    "href": "session4/ggplot.html#histogram",
    "title": "22  Data Visualization in R",
    "section": "22.2 Histogram",
    "text": "22.2 Histogram\nTo plot a histogram we require another type of geometric object called geom_histogram, which requires a statistical transformation. Some plot types (such as scatterplots) do not require transformations, each point is plotted at x and y coordinates equal to the original value. Other plots, such as boxplots, histograms, prediction lines etc. need to be transformed. Usually these objects have has a default statistic for the transformation, but that can be changed via the stat_bin argument.\nLet’s plot a histogram of sample mean expression in our data:\n\nggplot(new_metadata) +\n  geom_histogram(aes(x = samplemeans))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou will notice that even though the histogram is plotted, R gives a warning message `stat_bin() using bins = 30. Pick better value with binwidth.` These are the transformations we discussed. Apparently the default is not good enough.\nLet’s change the binwidth values. How does the plot differ?\n\nggplot(new_metadata) +\n  geom_histogram(aes(x = samplemeans), stat = \"bin\", binwidth=0.8)\n\n\n\n\n\n\nNOTE: You can use the example(\"geom_point\") function here to explore a multitude of different aesthetics and layers that can be added to your plot. As you scroll through the different plots, take note of how the code is modified. You can use this with any of the different geometric object layers available in ggplot2 to learn how you can easily modify your plots!\n\n\nNOTE: RStudio provide this very useful cheatsheet for plotting using ggplot2. Different example plots are provided and the associated code (i.e which geom or theme to use in the appropriate situation.) We also encourage you to persuse through this useful online reference for working with ggplot2.\n\nExercise 1: Themeing\nLet’s return to our scatterplot:\n\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=2.25) +\n  theme_bw() +\n  theme(axis.title = element_text(size=rel(1.5)))           \n\n\n\n\n\n\n\n\n\n\nBasic\n\n\n\n\nThe current axis label text defaults to what we gave as input to geom_point (i.e the column headers). We can change this by adding additional layers called xlab() and ylab() for the x- and y-axis, respectively. Add these layers to the current plot such that the x-axis is labeled “Age (days)” and the y-axis is labeled “Mean expression”.\nUse the ggtitle layer to add a plot title of your choice.\nAdd the following new layer to the code chunk theme(plot.title=element_text(hjust=0.5)).\n\n\nWhat does it change?\nHow many theme() layers can be added to a ggplot code chunk, in your estimation?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nWhen publishing, it is helpful to ensure all plots have similar formatting. To do this we can create a custom function with our preferences for the theme. Create a function called personal_theme which takes no arguments and\n\ncalls one of the ggplot2 themes such as theme_bw()\nsets the title text size to size=rel(1.5)\nsets the axis text size (you can use axis.title)\n\nOnce you have your function, call it to change your histogram’s theme.\n\n\n\n\n\n\n\n\nChallenge: Interactive Plots\n\n\n\nPlotly is another plotting library which has packages for multiple programming languages, including R and Python.\nOne of Plotly’s strengths is it’s ability to create interactive plots.\nFirst try making a simply interactive scatterplot with new_metadata and the same axes as the ggplot scatterplot. If you are able to do so, try adding dropdown menus which allow you to choose which column of new_metadata to color the points by.\n\n\nExercise 2: Boxplots\nA boxplot provides a graphical view of the distribution of data based on a five number summary: * The top and bottom of the box represent the (1) first and (2) third quartiles (25th and 75th percentiles, respectively). * The line inside the box represents the (3) median (50th percentile). * The whiskers extending above and below the box represent the (4) maximum, and (5) minimum of a data set. * The whiskers of the plot reach the minimum and maximum values that are not outliers.\n\nIn this case, outliers are determined using the interquartile range (IQR), which is defined as: Q3 - Q1. Any values that exceeds 1.5 x IQR below Q1 or above Q3 are considered outliers and are represented as points above or below the whiskers.\n\n\nBoxplot\n\nGenerate a boxplot using the data in the new_metadata dataframe. Create a ggplot2 code chunk with the following instructions:\n\nUse the geom_boxplot() layer to plot the differences in sample means between the Wt and KO genotypes.\nUse the fill aesthetic to look at differences in sample means between the celltypes within each genotype.\nAdd a title to your plot.\nAdd labels, ‘Genotype’ for the x-axis and ‘Mean expression’ for the y-axis.\nMake the following theme() changes:\n\nUse the theme_bw() function to make the background white.\nChange the size of your axes labels to 1.25x larger than the default.\nChange the size of your plot title to 1.5x larger than default.\nCenter the plot title.\n\n\nAfter running the above code the boxplot should look something like that provided below.\n\n\n\n\nChanging the order of genotype on the Boxplot\n\nLet’s say you wanted to have the “Wt” boxplots displayed first on the left side, and “KO” on the right. How might you go about doing this?\nTo do this, your first question should be - How does ggplot2 determine what to place where on the X-axis? * The order of the genotype on the X axis is in alphabetical order. * To change it, you need to make sure that the genotype column is a factor * And, the factor levels for that column are in the order you want on the X-axis\n\nFactor the new_metadata$genotype column without creating any extra variables/objects and change the levels to c(\"Wt\", \"KO\")\nRe-run the boxplot code chunk you created for the “Boxplot!” exercise above.\nChanging default colors\n\nYou can color the boxplot differently by using some specific layers:\n\nAdd a new layer scale_color_manual(values=c(\"purple\",\"orange\")).\n\nDo you observe a change?\n\nReplace scale_color_manual(values=c(\"purple\",\"orange\")) with scale_fill_manual(values=c(\"purple\",\"orange\")).\n\nDo you observe a change?\nIn the scatterplot we drew in class, add a new layer scale_color_manual(values=c(\"purple\",\"orange\")), do you observe a difference?\nWhat do you think is the difference between scale_color_manual() and scale_fill_manual()?\n\nBack in your boxplot code, change the colors in the scale_fill_manual() layer to be your 2 favorite colors.\n\nAre there any colors that you tried that did not work?\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/exporting.html#writing-data-to-file",
    "href": "session4/exporting.html#writing-data-to-file",
    "title": "23  Saving Data and Figures in R",
    "section": "23.1 Writing data to file",
    "text": "23.1 Writing data to file\nEverything we have done so far has only modified the data in R; the files have remained unchanged. Whenever we want to save our datasets to file, we need to use a write function in R.\nTo write our matrix to file in comma separated format (.csv), we can use the write.csv function. There are two required arguments: the variable name of the data structure you are exporting, and the path and filename that you are exporting to. By default the delimiter or column separator is set, and columns will be separated by a comma:\n# Save a data frame to file\nwrite.csv(sub_meta, file=\"data/subset_meta.csv\")\nOftentimes the output is not exactly what you might want. You can modify the output using the arguments for the function. We can explore the arguments using the ?. This can help elucidate what each of the arguments can adjust the output.\n?write.csv\nSimilar to reading in data, there are a wide variety of functions available allowing you to export data in specific formats. Another commonly used function is write.table, which allows you to specify the delimiter or separator you wish to use. This function is commonly used to create tab-delimited files.\n\nNOTE: Sometimes when writing a data frame using row names to file with write.table(), the column names will align starting with the row names column. To avoid this, you can include the argument col.names = NA when writing to file to ensure all of the column names line up with the correct column values.\n\nWriting a vector of values to file requires a different function than the functions available for writing dataframes. You can use write() to save a vector of values to file. For example:\n# Save a vector to file\nwrite(glengths, file=\"data/genome_lengths.txt\")\nIf we wanted the vector to be output to a single column instead of five, we could explore the arguments:\n?write\nNote, the ncolumns argument that it defaults to five columns unless specified, so to get a single column:\n# Save a vector to file as a single column\nwrite(glengths, file=\"data/genome_lengths.txt\", ncolumns = 1)"
  },
  {
    "objectID": "session4/exporting.html#exporting-figures-to-file",
    "href": "session4/exporting.html#exporting-figures-to-file",
    "title": "23  Saving Data and Figures in R",
    "section": "23.2 Exporting figures to file",
    "text": "23.2 Exporting figures to file\nThere are two ways in which figures and plots can be output to a file (rather than simply displaying on screen).\n\nThe first (and easiest) is to export directly from the RStudio ‘Plots’ panel, by clicking on Export when the image is plotted. This will give you the option of png or pdf and selecting the directory to which you wish to save it to. It will also give you options to dictate the size and resolution of the output image.\nThe second option is to use R functions and have the write to file hard-coded in to your script. This would allow you to run the script from start to finish and automate the process (not requiring human point-and-click actions to save). In R’s terminology, output is directed to a particular output device and that dictates the output format that will be produced. A device must be created or “opened” in order to receive graphical output and, for devices that create a file on disk, the device must also be closed in order to complete the output.\n\nIf we wanted to print our scatterplot to a pdf file format, we would need to initialize a plot using a function which specifies the graphical format you intend on creating i.e.pdf(), png(), tiff() etc. Within the function you will need to specify a name for your image, and the with and height (optional). This will open up the device that you wish to write to:\n## Open device for writing\npdf(\"figures/scatterplot.pdf\")\nIf you wish to modify the size and resolution of the image you will need to add in the appropriate parameters as arguments to the function when you initialize. Then we plot the image to the device, using the ggplot scatterplot that we just created.\n## Make a plot which will be written to the open device, in this case the temp file created by pdf()/png()\nggplot(new_metadata) +\n  geom_point(aes(x = age_in_days, y= samplemeans, color = genotype,\n            shape=celltype), size=rel(3.0)) \nFinally, close the “device”, or file, using the dev.off() function. There are also bmp, tiff, and jpeg functions, though the jpeg function has proven less stable than the others.\n## Closing the device is essential to save the temporary file created by pdf()/png()\ndev.off()\nNote 1: You will not be able to open and look at your file using standard methods (Adobe Acrobat or Preview etc.) until you execute the dev.off() function.\nNote 2: In the case of pdf(), if you had made additional plots before closing the device, they will all be stored in the same file with each plot usually getting its own page, unless otherwise specified.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/other_plots.html#pca-plot",
    "href": "session4/other_plots.html#pca-plot",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.1 PCA plot",
    "text": "24.1 PCA plot\nFirst we can create a principle component analysis (PCA) of the data.\n\nplotPCA(rld, intgroup = c(\"dex\", \"cell\"))\n\n\n\n\nPCA is a dimentionality reduction method. It reduced the dimensionality of our data while maintaining as much variation as possible. In the above data, it would be impossible to view how each of our samples compare across every gene at the same time. PCA finds linear combinations of genes which best explain the variance between each sample. We can see how much variance is explained by each principle component. When examining a PCA plot, we want to make sure that our samples group as expected, mainly, that replicates are closer to each other than to other samples.\nFor other data, we can use the prcomp function to perform a PCA analysis.\n\nlibrary(ggfortify, quietly=TRUE)\n\nWarning: package 'ggfortify' was built under R version 4.2.2\n\ndf <- iris[1:4]\npca_res <- prcomp(df, scale. = TRUE)\nautoplot(pca_res, data = iris, colour = 'Species')"
  },
  {
    "objectID": "session4/other_plots.html#tsne-plots",
    "href": "session4/other_plots.html#tsne-plots",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.2 tSNE Plots",
    "text": "24.2 tSNE Plots\nt-Distributed Neighbor Embedding (tSNE) is another dimensionality reduction method mainly used for visualization which can preform non-linear transformations. It finds the distances between points in the original, high-dimensional space, then attempts to find a low-dimensional space which maintains distances between points and their close neighbors. tSNE is stochastic, meaning that there is some randomness in its final embedding. Running tSNE multiple times on the same data will give slightly different results.\nMany biological analysis pipelines also have built-in TSNE analyses and visualizations. This one is a part of the Bioconductor scater package.\n\nlibrary(scRNAseq)\nlibrary(scater)\nlibrary(scran)\nlibrary(BiocSingular)\n\n\nload(\"../data/processedZeisel.RData\")\nsce.zeisel <- runTSNE(sce.zeisel, dimred=\"PCA\")\nplotTSNE(sce.zeisel, colour_by=\"label\")"
  },
  {
    "objectID": "session4/other_plots.html#volcano-plot",
    "href": "session4/other_plots.html#volcano-plot",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.3 Volcano plot",
    "text": "24.3 Volcano plot\nA volcano plot is a common visualization to see the distribution of fold-changes and p values accross our dataset. It plots the \\(log_2\\) fold-change against the p values from our statistical analysis.\n\nres %>% \n  mutate(sig = padj < 0.05 & abs(log2FoldChange) > 2) %>%\n  ggplot(aes(x = log2FoldChange, y = -log10(padj), col=sig)) +\n       geom_point() +\n       geom_vline(xintercept=c(-2, 2), col=\"red\") +\n       geom_hline(yintercept=-log10(0.05), col=\"red\")\n\nWarning: Removed 8077 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "session4/other_plots.html#heatmapclustergram",
    "href": "session4/other_plots.html#heatmapclustergram",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.4 Heatmap/Clustergram",
    "text": "24.4 Heatmap/Clustergram\nHeatmaps are a common visualization in a variety of analyses. By default in the heatmap package pheatmap, the rows and columns of our data are clustered using a hierarchical clustering method. This allows us to view which samples are most similar, and here how the most differentially expressed genes cluster and change across different samples.\nThe data we are using below is a microarray dataset investigating empryo development in mice which can be read about here.\n\nlibrary(pheatmap, quietly=TRUE)\n\nWarning: package 'pheatmap' was built under R version 4.2.2\n\nlibrary(\"Hiiragi2013\", quietly=TRUE)\n\nWarning: package 'boot' was built under R version 4.2.2\n\n\nWarning: package 'clue' was built under R version 4.2.2\n\n\nWarning: package 'cluster' was built under R version 4.2.2\n\n\nWarning: package 'genefilter' was built under R version 4.2.2\n\n\n\nAttaching package: 'genefilter'\n\n\nThe following object is masked from 'package:readr':\n\n    spec\n\n\nThe following objects are masked from 'package:MatrixGenerics':\n\n    rowSds, rowVars\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    rowSds, rowVars\n\n\n\nAttaching package: 'lattice'\n\n\nThe following object is masked from 'package:boot':\n\n    melanoma\n\n\n\nAttaching package: 'AnnotationDbi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nWarning: package 'XML' was built under R version 4.2.2\n\n\nWarning: package 'gplots' was built under R version 4.2.2\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:IRanges':\n\n    space\n\n\nThe following object is masked from 'package:S4Vectors':\n\n    space\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\nWarning: package 'gtools' was built under R version 4.2.2\n\n\n\nAttaching package: 'gtools'\n\n\nThe following objects are masked from 'package:boot':\n\n    inv.logit, logit\n\n\nWarning: package 'MASS' was built under R version 4.2.2\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:AnnotationDbi':\n\n    select\n\n\nThe following object is masked from 'package:genefilter':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n\n\n\n\n\nWarning: package 'xtable' was built under R version 4.2.2\n\nlibrary(\"dplyr\", quietly = TRUE)\ndata(\"x\") \n\n# Create a small dataframe summarizing each group\ngroups = group_by(pData(x), sampleGroup) %>%\n  summarise(n = n(), color = unique(sampleColour))\n# Get a color for that group\ngroupColor = setNames(groups$color, groups$sampleGroup)\n\ntopGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]\nrowCenter = function(x) { x - rowMeans(x) }\npheatmap( rowCenter(Biobase::exprs(x)[ topGenes, ] ),\n  show_rownames = FALSE, show_colnames = FALSE,\n  breaks = seq(-5, +5, length = 101),\n  annotation_col =\n    pData(x)[, c(\"sampleGroup\", \"Embryonic.day\", \"ScanDate\") ],\n  annotation_colors = list(\n    sampleGroup = groupColor,\n    genotype = c(`FGF4-KO` = \"chocolate1\", `WT` = \"azure2\"),\n    Embryonic.day = setNames(brewer.pal(9, \"Blues\")[c(3, 6, 9)],\n                             c(\"E3.25\", \"E3.5\", \"E4.5\")),\n    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), \"YlGn\"),\n                        levels(x$ScanDate))\n  ),\n  cutree_rows = 4\n)"
  },
  {
    "objectID": "session4/other_plots.html#network-visualization",
    "href": "session4/other_plots.html#network-visualization",
    "title": "24  Common visualizations in biological analyses",
    "section": "24.5 Network visualization",
    "text": "24.5 Network visualization\nWe also sometimes want to visualize networks in R. Network visualization can be difficult, and it is reccomended to use programs like cytoscape for creating publication-ready network figures. However, igraph is a popular package for handling and visualizing network data in R.\n\nlibrary(igraph, quietly = TRUE)\n\n\ng <- make_ring(10)\nplot(g, layout=layout_with_kk, vertex.color=\"green\")\n\n\n\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session4/pSet4.html#problem-1",
    "href": "session4/pSet4.html#problem-1",
    "title": "25  Problem Set 4",
    "section": "25.1 Problem 1",
    "text": "25.1 Problem 1\nR actually also has built-in plotting functionality, though it is rarely used in modern analyses. Let’s make some visualizations of another ELISA assay dataset which is included with R, DNase.\n\ndata(DNase)\nhead(DNase)\n\n  Run       conc density\n1   1 0.04882812   0.017\n2   1 0.04882812   0.018\n3   1 0.19531250   0.121\n4   1 0.19531250   0.124\n5   1 0.39062500   0.206\n6   1 0.39062500   0.215\n\n\nThis assay was used to quantify the activity of the enzyme deoxyribonuclease (DNase).\nWe can make a boxplot of the density of each run:\n\nhist(DNase$density, breaks=25, main = \"\")\n\n\n\nboxplot(density ~ Run, data = DNase)\n\n\n\n\nCreate a ggplot2 boxplot displaying the density distribution for each run of the DNAse object. Order the boxes in numerical order along the \\(x\\)-axis instead of lexicographical order (hint: as.numeric). Display each box with a different color (hint: rainbow)."
  },
  {
    "objectID": "session4/pSet4.html#problem-2",
    "href": "session4/pSet4.html#problem-2",
    "title": "25  Problem Set 4",
    "section": "25.2 Problem 2",
    "text": "25.2 Problem 2\nWe continue working with a gene expression microarray dataset that reports the gene expression of around 100 individual cells from mouse embryos at different time points in early development (the Hiiragi2013 data: Ohnishi et al., 2014).\n\npdat <- read.delim(\"../data/Hiiragi2013_pData.txt\", as.is = TRUE)\nhead(pdat, n = 2)\n\n  File.name Embryonic.day Total.number.of.cells lineage genotype   ScanDate\n1  1_C32_IN         E3.25                    32               WT 2011-03-16\n2  2_C32_IN         E3.25                    32               WT 2011-03-16\n  sampleGroup sampleColour\n1       E3.25      #CAB2D6\n2       E3.25      #CAB2D6\n\n\nCreate a ggplot2 barplot displaying the total number of cells accross each sample group of the pdat dataset. You can use the aggregate function with sum to calculate these totals. Flip the \\(x\\)- and \\(y\\)-aesthetics to produce a horizontal barplot. Rotate the group labels by 90 degrees Hint, element_text has an angle argument, and a single axis’ text can be accessed by axis.text.x or axis.text.y."
  },
  {
    "objectID": "session4/pSet4.html#problem-3",
    "href": "session4/pSet4.html#problem-3",
    "title": "25  Problem Set 4",
    "section": "25.3 Problem 3",
    "text": "25.3 Problem 3\nChoose a plot you created during Session 4, or another plot from your own research. Show the original plot, then work to get the plot into a ‘publication-ready’ state, either for a paper, poster, or presentation. You can choose which one of these 3 scenarios you want to create your figure for. Some things to consider:\n\nAre your colors colorblind safe?\nFont sizes in posters need to be very large, followed by presentation and then paper font sizes. We also need to consider things like line thickness and the size of any points in a scatterplot. The Python plotting library Seaborn has nice examples of how the sizes should differ.\nText should not overlap.\nLegends should be clear and use neat, human readable labels as opposed to the names of columns in R (i.e. something like “Number of Cells” or “# Cells” as opposed to “Total.number.of.cells”).\nPoster and presentation figures typically have titles, while a paper figure typically does not.\n\nInclude code for saving your publication-ready figure as a pdf."
  },
  {
    "objectID": "session5/summarized_experiment.html",
    "href": "session5/summarized_experiment.html",
    "title": "26  Working with summarized experimental data",
    "section": "",
    "text": "This section introduces another broadly useful package and data structure, the SummarizedExperiment package and SummarizedExperiment object.\n\nThe SummarizedExperiment object has matrix-like properties – it has two dimensions and can be subset by ‘rows’ and ‘columns’. The assay() data of a SummarizedExperiment experiment contains one or more matrix-like objects where rows represent features of interest (e.g., genes), columns represent samples, and elements of the matrix represent results of a genomic assay (e.g., counts of reads overlaps genes in each sample of an bulk RNA-seq differential expression assay.\nObject construction\nThe SummarizedExperiment coordinates assays with (optional) descriptions of rows and columns. We start by reading in a simple data.frame describing 8 samples from an RNASeq experiment looking at dexamethasone treatment across 4 human smooth muscle cell lines; use browseVignettes(\"airway\") for a more complete description of the experiment and data processing. Read the column data in using file.choose() and read.csv().\n\nfname <- file.choose()  # airway_colData.csv\nfname\n\n\n\n\nWe want the first column the the data to be treated as row names (sample identifiers) in the data.frame, so read.csv() has an extra argument to indicate this.\n\ncolData <- read.csv(fname, row.names = 1)\nhead(colData)\n\n           SampleName    cell   dex albut        Run avgLength Experiment\nSRR1039508 GSM1275862  N61311 untrt untrt SRR1039508       126  SRX384345\nSRR1039509 GSM1275863  N61311   trt untrt SRR1039509       126  SRX384346\nSRR1039512 GSM1275866 N052611 untrt untrt SRR1039512       126  SRX384349\nSRR1039513 GSM1275867 N052611   trt untrt SRR1039513        87  SRX384350\nSRR1039516 GSM1275870 N080611 untrt untrt SRR1039516       120  SRX384353\nSRR1039517 GSM1275871 N080611   trt untrt SRR1039517       126  SRX384354\n              Sample    BioSample\nSRR1039508 SRS508568 SAMN02422669\nSRR1039509 SRS508567 SAMN02422675\nSRR1039512 SRS508571 SAMN02422678\nSRR1039513 SRS508572 SAMN02422670\nSRR1039516 SRS508575 SAMN02422682\nSRR1039517 SRS508576 SAMN02422673\n\n\nThe data are from the Short Read Archive, and the row names, SampleName, Run, Experiment, Sampel, and BioSample columns are classifications from the archive. Additional columns include:\n\ncell: the cell line used. There are four cell lines.\ndex: whether the sample was untreated, or treated with dexamethasone.\nalbut: a second treatment, which we ignore\navgLength: the sample-specific average length of the RNAseq reads estimated in the experiment.\n\nAssay data\nNow import the assay data from the file “airway_counts.csv”\n\nfname <- file.choose()  # airway_counts.csv\nfname\n\n\n\n\n\ncounts <- read.csv(fname, row.names=1)\n\nAlthough the data are read as a data.frame, all columns are of the same type (integer-valued) and represent the same attribute; the data is really a matrix rather than data.frame, so we coerce to matrix using as.matrix().\n\ncounts <- as.matrix(counts)\n\nWe see the dimensions and first few rows of the counts matrix\n\ndim(counts)\n\n[1] 33469     8\n\nhead(counts)\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516\nENSG00000000003        679        448        873        408       1138\nENSG00000000419        467        515        621        365        587\nENSG00000000457        260        211        263        164        245\nENSG00000000460         60         55         40         35         78\nENSG00000000938          0          0          2          0          1\nENSG00000000971       3251       3679       6177       4252       6721\n                SRR1039517 SRR1039520 SRR1039521\nENSG00000000003       1047        770        572\nENSG00000000419        799        417        508\nENSG00000000457        331        233        229\nENSG00000000460         63         76         60\nENSG00000000938          0          0          0\nENSG00000000971      11027       5176       7995\n\n\nIt’s interesting to think about what the counts mean – for ENSG00000000003, sample SRR1039508 had 679 reads that overlapped this gene, sample SRR1039509 had 448 reads, etc. Notice that for this gene there seems to be a consistent pattern – within a cell line, the read counts in the untreated group are always larger than the read counts for the treated group. This and other basic observations from ‘looking at’ the data motivate many steps in a rigorous RNASeq differential expression analysis.\nCreating a SummarizedExperiment object\nWe saw earlier that there was considerable value in tightly coupling the count of CpG islands overlapping each transcript with the GRanges describing the transcripts. We can anticipate that close coupling of the column data with the assay data will have similar benefits, e.g., reducing the chances of bookkeeping errors as we work with our data.\nAttach the SummarizedExperiment library to our R session.\n\nlibrary(\"SummarizedExperiment\")\n\nUse the SummarizedExperiment() function to coordinate the assay and column data; this function uses row and column names to make sure the correct assay columns are described by the correct column data rows.\n\nse <- SummarizedExperiment(assay = list(count=counts), colData = colData)\nse\n\nclass: SummarizedExperiment \ndim: 33469 8 \nmetadata(0):\nassays(1): count\nrownames(33469): ENSG00000000003 ENSG00000000419 ... ENSG00000273492\n  ENSG00000273493\nrowData names(0):\ncolnames(8): SRR1039508 SRR1039509 ... SRR1039520 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nIt is straight-forward to use subset() on SummarizedExperiment to create subsets of the data in a coordinated way. Remember that a SummarizedExperiment is conceptually two-dimensional (matrix-like), and in the example below we are subsetting on the second dimension.\n\nsubset(se, , dex == \"trt\")\n\nclass: SummarizedExperiment \ndim: 33469 4 \nmetadata(0):\nassays(1): count\nrownames(33469): ENSG00000000003 ENSG00000000419 ... ENSG00000273492\n  ENSG00000273493\nrowData names(0):\ncolnames(4): SRR1039509 SRR1039513 SRR1039517 SRR1039521\ncolData names(9): SampleName cell ... Sample BioSample\n\n\nAs with GRanges, there are accessors that extract data from the SummarizedExperiment. For instance, we can use assay() to extract the count matrix, and colSums() to calculate the library size (total number of reads overlapping genes in each sample).\n\ncolSums(assay(se))\n\nSRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 \n  20637971   18809481   25348649   15163415   24448408   30818215   19126151 \nSRR1039521 \n  21164133 \n\n\nNote that library sizes differ by a factor of 2 from largest to smallest; how would this influence the interpretation of counts in individual cells of the assay data?\nAs with GRanges, it might be useful to remember important computations in a way that is robust, e.g.,\n\nse$lib.size <- colSums(assay(se))\ncolData(se)\n\nDataFrame with 8 rows and 10 columns\n            SampleName        cell         dex       albut         Run\n           <character> <character> <character> <character> <character>\nSRR1039508  GSM1275862      N61311       untrt       untrt  SRR1039508\nSRR1039509  GSM1275863      N61311         trt       untrt  SRR1039509\nSRR1039512  GSM1275866     N052611       untrt       untrt  SRR1039512\nSRR1039513  GSM1275867     N052611         trt       untrt  SRR1039513\nSRR1039516  GSM1275870     N080611       untrt       untrt  SRR1039516\nSRR1039517  GSM1275871     N080611         trt       untrt  SRR1039517\nSRR1039520  GSM1275874     N061011       untrt       untrt  SRR1039520\nSRR1039521  GSM1275875     N061011         trt       untrt  SRR1039521\n           avgLength  Experiment      Sample    BioSample  lib.size\n           <integer> <character> <character>  <character> <numeric>\nSRR1039508       126   SRX384345   SRS508568 SAMN02422669  20637971\nSRR1039509       126   SRX384346   SRS508567 SAMN02422675  18809481\nSRR1039512       126   SRX384349   SRS508571 SAMN02422678  25348649\nSRR1039513        87   SRX384350   SRS508572 SAMN02422670  15163415\nSRR1039516       120   SRX384353   SRS508575 SAMN02422682  24448408\nSRR1039517       126   SRX384354   SRS508576 SAMN02422673  30818215\nSRR1039520       101   SRX384357   SRS508579 SAMN02422683  19126151\nSRR1039521        98   SRX384358   SRS508580 SAMN02422677  21164133\n\n\nExercises\n\n\n\n\n\n\nBasic\n\n\n\n\nSubset the SummarizedExperiment object se created above to genes (rows) with at least 8 reads mapped across all samples.\n\nHint: use rowSums.\n\nScale the read counts by library size, i.e. divide each column of assay(se) by the corresponding lib.size of each sample (column). Multiply the resulting scaled counts by 10^6 to obtain counts per million reads mapped.\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nCarry out a t-test for each of the selected 100 genes. Test for differences in mean read count per million reads mapped between the dexamethasone treated and untreated sample group. Annotate the resulting p-values as a new column in the rowData slot of your SummarizedExperiment.\nHint: use apply and t.test.\n\n\n\nThis lesson was adapted from materials created by Ludwig Geistlinger"
  },
  {
    "objectID": "session5/de_analysis.html#volcano-plot",
    "href": "session5/de_analysis.html#volcano-plot",
    "title": "27  Differential expression analysis with DESeq2",
    "section": "27.1 Volcano plot",
    "text": "27.1 Volcano plot\nA useful illustration of differential expression results is to plot the fold change against the p-value in a volcano plot. This allows to inspect direction and magnitude (fold change) as well as the statistical significance (p-value) of the expression change.\n\nlibrary(ggplot2)\nggplot(as.data.frame(res), \n       aes(x = log2FoldChange, y = -log10(padj))) + geom_point()\n\nWarning: Removed 8077 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "session5/de_analysis.html#ma-plot",
    "href": "session5/de_analysis.html#ma-plot",
    "title": "27  Differential expression analysis with DESeq2",
    "section": "27.2 MA plot",
    "text": "27.2 MA plot\nAnother useful illustration of differential expression results is to plot the fold changes as a function of the mean of the expression level (normalized counts) across samples in an MA plot.\nPoints will be colored if the adjusted p-value is less than a defined significance threshold (default: 0.1). Points which fall out of the window are plotted as open triangles pointing either up or down.\n\nplotMA(res)\n\n\n\n\nThe DESeq2 vignette also describes several other useful result exploration and data quality assessment plots.\nExercises\n\n\n\n\n\n\nBasic\n\n\n\n\nUse the coef function to examine the actual coefficients of the model.\nGet the number of genes considered significantly expressed at the alpha level of 0.1 (for the adjusted p value).\nNow see how many genes would be considered differentially expressed at an alpha level of 0.05 and a log2 fold change cutoff of at least 1. Note: Take a look again at the arguments of the results function. Are there any you should change?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nLet’s imagine that, instead of all being untreated, half of the samples had been treated with albuterol:\n\nfake_se <- se\n#Currently the factor only has the untrt level, so we need to add another\nlevels(colData(fake_se)$albut) <- c(levels(colData(fake_se)$albut), \"trt\")\ncolData(fake_se)$albut[c(1,3,4,8)] <- \"trt\"\n\nRemake the dds object such that the albut column is an additional covartiate in the experimental design.\nIf there is time, compare the number of significant results (for comparing cell and dex) when albut is and is not accounted for. Does it make a difference?\n\n\n\n\n\n\n\n\nBonus\n\n\n\nTry installing the Glimma package from bioconductor. Use it to create an interactive multidimensional scaling (MDS) plot of your results.\n\n\n\nThis lesson was adapted from materials created by Ludwig Geistlinger"
  },
  {
    "objectID": "session5/pset5.html#problem-1",
    "href": "session5/pset5.html#problem-1",
    "title": "29  Problem Set 5",
    "section": "29.1 Problem 1",
    "text": "29.1 Problem 1\nThe experimental data package fission stores time course RNA-seq data studying the stress response in fission yeast. Conduct a differential expression analysis with DESeq2 between the mutant and the wild type strain, using time as a covariate. How many genes are differentially expressed, based on an adjusted significance level of 0.05? Inspect the results with a volcano plot and an MA plot."
  },
  {
    "objectID": "session5/pset5.html#problem-2",
    "href": "session5/pset5.html#problem-2",
    "title": "29  Problem Set 5",
    "section": "29.2 Problem 2",
    "text": "29.2 Problem 2\nPerform gene set enrichment analysis on the fission dataset using the same experiment setup as above:\n\nChoose an appropriate collection of gene sets. Remember that this dataset is in yeast (org=\"sce\").\nPerform gene set enrichment analysis (the gsea method) and at least one other form of enrichment analysis such as over representation analysis or a network-based enrichment method on the data.\nExamine the top 20 gene set for each method you ran. How do they compare? Do the results make sense given the experimental context of the stress response in fission yeast?"
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This chapter contains relevant literature, additional educational resources, and some guides on specific actions you might want to take using the workbook."
  },
  {
    "objectID": "resources/git.html#what-is-git-github",
    "href": "resources/git.html#what-is-git-github",
    "title": "30  Getting Started with Git & Github",
    "section": "30.1 What is Git / GitHub",
    "text": "30.1 What is Git / GitHub\nGit is a file version control system that helps you keep track of any changes you make to specific documents (e.g., code). This a much more elegant solution than copying a file over and over and changing the name to things like: file_version1, file_version2, file_final, file_finalVersion, file_final_finalVersion …\nWatch this video to get a little more of an introduction to Git.\nGitHub is an online service that allows you to share Git repositories with other people. You can either Pull an existing repository to you machine and start working on it yourself, or you can Push any of your repositories (or changes made to someone else’s) to GitHub to share them with others (or even yourself if you have multiple computers).\nWatch this video to see how GitHub can help distribute code safely between many people without causing issues."
  },
  {
    "objectID": "resources/git.html#create-a-github-account",
    "href": "resources/git.html#create-a-github-account",
    "title": "30  Getting Started with Git & Github",
    "section": "30.2 Create a Github Account",
    "text": "30.2 Create a Github Account\nIf you do not have one already go to github.com and register for a new account. We recommend you use a personal email address to sign up as your GitHub is seen as a personal asset. However, with an academic email you can unlock more features so once registered you can add your Harvard or other .edu email address to get educational benefits as a student or as a teacher/researcher."
  },
  {
    "objectID": "resources/git.html#option-1-github-desktop-reccomended",
    "href": "resources/git.html#option-1-github-desktop-reccomended",
    "title": "30  Getting Started with Git & Github",
    "section": "30.3 Option 1: Github Desktop (reccomended)",
    "text": "30.3 Option 1: Github Desktop (reccomended)\nIf you are unfamiliar with using the command line, Github Desktop can be a good place to start.\nYou can then go to the workbook repository and connect it to your Github Desktop:\n\n\n\nConnecting the repo to Github Desktop\n\n\nFinally, you can click fetch (the button may say pull) from within the Github Desktop client to download files locally.\n\n\n\nFetching files"
  },
  {
    "objectID": "resources/git.html#option-2-command-line",
    "href": "resources/git.html#option-2-command-line",
    "title": "30  Getting Started with Git & Github",
    "section": "30.4 Option 2: Command line",
    "text": "30.4 Option 2: Command line\nFollow the instructions here to install Git or a Git client on your computer.\nIt is recommended for you to setup your local username and email address before using Git, and in some cases is required. This does not need to match GitHub (we’ll do that next). You will need to use a terminal window for this (Command Prompt in Windows or Terminal on Mac). You can also use the Terminal window in RStudio if you prefer (different from the Console window)\n`git config --global user.name \"First Last\"`\n\n`git config --global user.email \"me@email.com\"`\nWatch detailed instructions in this video if needed\nYou can then clone the repository locally.\ngit clone https://github.com/ccb-hms/systems-immunology-workshops.git\nWhenever the workbook is updated, you can pull it to download the changes. Within the systems-immunology-workshops directory, simply enter:\ngit pull"
  },
  {
    "objectID": "resources/git.html#option-3-integrate-git-github-with-rstudio",
    "href": "resources/git.html#option-3-integrate-git-github-with-rstudio",
    "title": "30  Getting Started with Git & Github",
    "section": "30.5 Option 3: Integrate Git /GitHub with RStudio",
    "text": "30.5 Option 3: Integrate Git /GitHub with RStudio\nRStudio has an integrated Git user interface that makes it very easy to use both Git and GitHub.\nTo get a copy of the workbook repository in RStudio do the following:\n\nClick File → New Project\nSelect Version Control → Git\nFor the URL choose: https://github.com/ccb-hms/systems-immunology-workshops.git\nYou can choose the name of the project directory.\nChoose the folder in which you want to store the R project and Git (depends on how you organize your files)\nClick Create Project\nCheck the Files tab to see if you have successfully created the project\n\nWhenever you are working in an RStudio project that has a dedicated Git repository, you can interact with Git through the Git tab (same pane as Environment tab)"
  },
  {
    "objectID": "resources/git.html#stashing-changes-if-needed",
    "href": "resources/git.html#stashing-changes-if-needed",
    "title": "30  Getting Started with Git & Github",
    "section": "30.6 Stashing changes if needed",
    "text": "30.6 Stashing changes if needed\nIf you edit your local copy of the workbook, when you try to pull or fetch files in the future you may run into an error. This is because Git isn’t sure whether you want to discard your local changes or not. You can stash your local changes, pull/fetch, and then pop your changes to download the new files and integrate your changes. However, if you have edited files which have also been updated, such as writing a solution in a file which then had an added solution, you may get a merge conflict."
  },
  {
    "objectID": "resources/git.html#resources",
    "href": "resources/git.html#resources",
    "title": "30  Getting Started with Git & Github",
    "section": "30.7 Resources:",
    "text": "30.7 Resources:\n\nA good git reference book\nGit desktop environments: https://desktop.github.com/ and those from https://git-scm.com/downloads\nRStudio and git guide\nA great interactive site for learning Git\nA useful git cheat sheet\nSoftware Carpentry’s git lessons\nGithub’s Git guides"
  }
]