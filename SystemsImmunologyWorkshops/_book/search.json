[
  {
    "objectID": "session1/types.html#basic-data-types",
    "href": "session1/types.html#basic-data-types",
    "title": "3  R Syntax and Data Structures",
    "section": "3.1 Basic Data Types",
    "text": "3.1 Basic Data Types\nVariables can contain values of specific types within R. The six data types that R uses include:\n\n\"numeric\" for any numerical value, including whole numbers and decimals. This is the most common data type for performing mathematical operations.\n\"character\" for text values, denoted by using quotes (““) around value. For instance, while 5 is a numeric value, if you were to put quotation marks around it, it would turn into a character value, and you could no longer use it for mathematical operations. Single or double quotes both work, as long as the same type is used at the beginning and end of the character value.\n\"integer\" for whole numbers (e.g., 2L, the L indicates to R that it’s an integer). It behaves similar to the numeric data type for most tasks or functions; however, it takes up less storage space than numeric data, so often tools will output integers if the data is known to be comprised of whole numbers. Just know that integers behave similarly to numeric values. If you wanted to create your own, you could do so by providing the whole number, followed by an upper-case L.\n\"logical\" for TRUE and FALSE (the Boolean data type). The logical data type can be specified using four values, TRUE in all capital letters, FALSE in all capital letters, a single capital T or a single capital F.\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1+4i) and that’s all we’re going to say about them\n\"raw\" that we won’t discuss further\n\nThe table below provides examples of each of the commonly used data types:\n\n\n\nData Type\nExamples\n\n\n\n\nNumeric:\n1, 1.5, 20, pi\n\n\nCharacter:\n“anytext”, “5”, “TRUE”\n\n\nInteger:\n2L, 500L, -17L\n\n\nLogical:\nTRUE, FALSE, T, F\n\n\n\nThe type of data will determine what you can do with it. For example, if you want to perform mathematical operations, then your data type cannot be character or logical. Whereas if you want to search for a word or pattern in your data, then you data should be of the character data type. The task or function being performed on the data will determine what type of data can be used."
  },
  {
    "objectID": "session1/types.html#data-structures",
    "href": "session1/types.html#data-structures",
    "title": "3  R Syntax and Data Structures",
    "section": "3.2 Data Structures",
    "text": "3.2 Data Structures\nWe know that variables are like buckets, and so far we have seen that bucket filled with a single value. Even when number was created, the result of the mathematical operation was a single value. Variables can store more than just a single value, they can store a multitude of different data structures. These include, but are not limited to, vectors (c), factors (factor), matrices (matrix), data frames (data.frame) and lists (list).\n\n3.2.1 Vectors\nA vector is the most common and basic data structure in R, and is pretty much the workhorse of R. It’s basically just a collection of values, mainly either numbers,\n\n\n\nnumeric vector\n\n\nor characters,\n\n\n\ncharacter vector\n\n\nor logical values,\n\n\n\nlogical vector\n\n\nNote that all values in a vector must be of the same data type. If you try to create a vector with more than a single data type, R will try to coerce it into a single data type.\nFor example, if you were to try to create the following vector:\n\n\n\nmixed vector\n\n\nR will coerce it into:\n\n\n\ntransformed vector\n\n\nThe analogy for a vector is that your bucket now has different compartments; these compartments in a vector are called elements.\nEach element contains a single value, and there is no limit to how many elements you can have. A vector is assigned to a single variable, because regardless of how many elements it contains, in the end it is still a single entity (bucket).\nLet’s create a vector of genome lengths and assign it to a variable called glengths.\nEach element of this vector contains a single numeric value, and three values will be combined together into a vector using c() (the combine function). All of the values are put within the parentheses and separated with a comma.\n\n# Create a numeric vector and store the vector as a variable called 'glengths'\nglengths <- c(4.6, 3000, 50000)\nglengths\n\n[1]     4.6  3000.0 50000.0\n\n\nNote your environment shows the glengths variable is numeric (num) and tells you the glengths vector starts at element 1 and ends at element 3 (i.e. your vector contains 3 values) as denoted by the [1:3].\nA vector can also contain characters. Create another vector called species with three elements, where each element corresponds with the genome sizes vector (in Mb).\n\n# Create a character vector and store the vector as a variable called 'species'\nspecies <- c(\"ecoli\", \"human\", \"corn\")\nspecies\n\n[1] \"ecoli\" \"human\" \"corn\" \n\n\nWhat do you think would happen if we forgot to put quotations around one of the values? Let’s test it out with corn.\n\n# Forget to put quotes around corn\nspecies <- c(\"ecoli\", \"human\", corn)\n\nNote that RStudio is quite helpful in color-coding the various data types. We can see that our numeric values are blue, the character values are green, and if we forget to surround corn with quotes, it’s black. What does this mean? Let’s try to run this code.\nWhen we try to run this code we get an error specifying that object ‘corn’ is not found. What this means is that R is looking for an object or variable in my Environment called ‘corn’, and when it doesn’t find it, it returns an error. If we had a character vector called ‘corn’ in our Environment, then it would combine the contents of the ‘corn’ vector with the values “ecoli” and “human”.\nSince we only want to add the value “corn” to our vector, we need to re-run the code with the quotation marks surrounding corn. A quick way to add quotes to both ends of a word in RStudio is to highlight the word, then press the quote key.\n\n# Create a character vector and store the vector as a variable called 'species'\nspecies <- c(\"ecoli\", \"human\", \"corn\")\n\n\nExercise\nTry to create a vector of numeric and character values by combining the two vectors that we just created (glengths and species). Assign this combined vector to a new variable called combined. Hint: you will need to use the combine c() function to do this. Print the combined vector in the console, what looks different compared to the original vectors?\n\n\n3.2.2 Factors\nA factor is a special type of vector that is used to store categorical data. Each unique category is referred to as a factor level (i.e. category = level). Factors are built on top of integer vectors such that each factor level is assigned an integer value, creating value-label pairs.\nFor instance, if we have four animals and the first animal is female, the second and third are male, and the fourth is female, we could create a factor that appears like a vector, but has integer values stored under-the-hood. The integer value assigned is a one for females and a two for males. The numbers are assigned in alphabetical order, so because the f- in females comes before the m- in males in the alphabet, females get assigned a one and males a two. In later lessons we will show you how you could change these assignments.\n\n\n\nfactors\n\n\nLet’s create a factor vector and explore a bit more. We’ll start by creating a character vector describing three different levels of expression. Perhaps the first value represents expression in mouse1, the second value represents expression in mouse2, and so on and so forth:\n\n# Create a character vector and store the vector as a variable called 'expression'\nexpression <- c(\"low\", \"high\", \"medium\", \"high\", \"low\", \"medium\", \"high\")\n\nNow we can convert this character vector into a factor using the factor() function:\n\n# Turn 'expression' vector into a factor\nexpression <- factor(expression)\n\nSo, what exactly happened when we applied the factor() function?\n\n\n\nfactor_new\n\n\nThe expression vector is categorical, in that all the values in the vector belong to a set of categories; in this case, the categories are low, medium, and high. By turning the expression vector into a factor, the categories are assigned integers alphabetically, with high=1, low=2, medium=3. This in effect assigns the different factor levels. You can view the newly created factor variable and the levels in the Environment window.\n\n\n\nFactor variables in environment\n\n\nSo now that we have an idea of what factors are, when would you ever want to use them?\nFactors are extremely valuable for many operations often performed in R. For instance, factors can give order to values with no intrinsic order. In the previous ‘expression’ vector, if I wanted the low category to be less than the medium category, then we could do this using factors. Also, factors are necessary for many statistical methods. For example, descriptive statistics can be obtained for character vectors if you have the categorical information stored as a factor. Also, if you want to denote which category is your base level for a statistical comparison, then you would need to have your category variable stored as a factor with the base level assigned to 1. Anytime that it is helpful to have the categories thought of as groups in an analysis, the factor function makes this possible. For instance, if you want to color your plots by treatment type, then you would need the treatment variable to be a factor.\nExercises\nLet’s say that in our experimental analyses, we are working with three different sets of cells: normal, cells knocked out for geneA (a very exciting gene), and cells overexpressing geneA. We have three replicates for each celltype.\n\nCreate a vector named samplegroup with nine elements: 3 control (“CTL”) values, 3 knock-out (“KO”) values, and 3 over-expressing (“OE”) values.\nTurn samplegroup into a factor data structure.\n\n\n\n3.2.3 Matrix\nA matrix in R is a collection of vectors of same length and identical datatype. Vectors can be combined as columns in the matrix or by row, to create a 2-dimensional structure.\n\n\n\nmatrix\n\n\nMatrices are used commonly as part of the mathematical machinery of statistics. They are usually of numeric datatype and used in computational algorithms to serve as a checkpoint. For example, if input data is not of identical data type (numeric, character, etc.), the matrix() function will throw an error and stop any downstream code execution.\n\n\n3.2.4 Data Frame\nA data.frame is the de facto data structure for most tabular data and what we use for statistics and plotting. A data.frame is similar to a matrix in that it’s a collection of vectors of the same length and each vector represents a column. However, in a dataframe each vector can be of a different data type (e.g., characters, integers, factors). In the data frame pictured below, the first column is character, the second column is numeric, the third is character, and the fourth is logical.\n\n\n\ndataframe\n\n\nA data frame is the most common way of storing data in R, and if used systematically makes data analysis easier.\nWe can create a dataframe by bringing vectors together to form the columns. We do this using the data.frame() function, and giving the function the different vectors we would like to bind together. This function will only work for vectors of the same length.\n\n# Create a data frame and store it as a variable called 'df'\ndf <- data.frame(species, glengths)\n\nWe can see that a new variable called df has been created in our Environment within a new section called Data. In the Environment, it specifies that df has 3 observations of 2 variables. What does that mean? In R, rows always come first, so it means that df has 3 rows and 2 columns. We can get additional information if we click on the blue circle with the white triangle in the middle next to df. It will display information about each of the columns in the data frame, giving information about what the data type is of each of the columns and the first few values of those columns.\nAnother handy feature in RStudio is that if we hover the cursor over the variable name in the Environment, df, it will turn into a pointing finger. If you click on df, it will open the data frame as it’s own tab next to the script editor. We can explore the table interactively within this window. To close, just click on the X on the tab.\nAs with any variable, we can print the values stored inside to the console if we type the variable’s name and run.\n\ndf\n\n  species glengths\n1   ecoli      4.6\n2   human   3000.0\n3    corn  50000.0\n\n\n\n\n3.2.5 Lists\nLists are a data structure in R that can be perhaps a bit daunting at first, but soon become amazingly useful. A list is a data structure that can hold any number of any types of other data structures.\nIf you have variables of different data structures you wish to combine, you can put all of those into one list object by using the list() function and placing all the items you wish to combine within parentheses:\n\nlist1 <- list(species, df, expression)\n\nWe see list1 appear within the Data section of our environment as a list of 3 components or variables. If we click on the blue circle with a triangle in the middle, it’s not quite as interpretable as it was for data frames.\nEssentially, each component is preceded by a colon. The first colon give the species vector, the second colon precedes the df data frame, with the dollar signs indicating the different columns, the last colon gives the single value, number.\nIf I click on list1, it opens a tab where you can explore the contents a bit more, but it’s still not super intuitive. The easiest way to view small lists is to print to the console.\nLet’s type list1 and print to the console by running it.\n\nlist1\n\n[[1]]\n[1] \"ecoli\" \"human\" \"corn\" \n\n[[2]]\n  species glengths\n1   ecoli      4.6\n2   human   3000.0\n3    corn  50000.0\n\n[[3]]\n[1] low    high   medium high   low    medium high  \nLevels: high low medium\n\n\nThere are three components corresponding to the three different variables we passed in, and what you see is that structure of each is retained. Each component of a list is referenced based on the number position.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "preWork/pre_work.html#install-r-and-rstudio",
    "href": "preWork/pre_work.html#install-r-and-rstudio",
    "title": "Pre-Work",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nBefore the first session, please install R and RStudio following the instructions Chapter 1.\nIf you already have R and RStudio installed, make sure that you have the latest versions installed (you can do this by simply following the installation instructions). While this will likely not cause an issue for the first few sessions, it will in later sessions when we use more advanced packages and software.\nIf you encounter issues installing R or RStudio, please reach out to christopher_magnano@hms.harvard.edu or one of the TAs. If we are unable to resolve your issue via email, we ask that you come 30 minutes early to the first session."
  },
  {
    "objectID": "preWork/pre_work.html#familiarize-yourself-with-rstudio",
    "href": "preWork/pre_work.html#familiarize-yourself-with-rstudio",
    "title": "Pre-Work",
    "section": "Familiarize yourself with RStudio",
    "text": "Familiarize yourself with RStudio\nIf you have never used RStudio or are completely new to programming, please review Chapter 2. This material will introduce you to the RStudio interface and how to assign values to variables in R."
  },
  {
    "objectID": "preWork/install.html#mac-users",
    "href": "preWork/install.html#mac-users",
    "title": "1  Installing R and RStudio",
    "section": "1.1 Mac Users",
    "text": "1.1 Mac Users\n\n1.1.1 To install R\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.1.2 To install RStudio\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder."
  },
  {
    "objectID": "preWork/install.html#windows-users",
    "href": "preWork/install.html#windows-users",
    "title": "1  Installing R and RStudio",
    "section": "1.2 Windows Users",
    "text": "1.2 Windows Users\n\n1.2.1 To install R\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n1.2.2 To install RStudio\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions."
  },
  {
    "objectID": "preWork/install.html#reference",
    "href": "preWork/install.html#reference",
    "title": "1  Installing R and RStudio",
    "section": "1.3 Reference",
    "text": "1.3 Reference\nInstructions adapted from guide developed by HMS Research computing"
  },
  {
    "objectID": "preWork/rstudio.html#learning-objectives",
    "href": "preWork/rstudio.html#learning-objectives",
    "title": "2  Introduction to RStudio",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nDescribe what R and RStudio are.\nInteract with R using RStudio.\nFamiliarize various components of RStudio."
  },
  {
    "objectID": "preWork/rstudio.html#what-is-rstudio",
    "href": "preWork/rstudio.html#what-is-rstudio",
    "title": "2  Introduction to RStudio",
    "section": "2.2 What is RStudio?",
    "text": "2.2 What is RStudio?\nRStudio is freely available open-source Integrated Development Environment (IDE). RStudio provides an environment with many features to make using R easier and is a great alternative to working on R in the terminal.\n\nGraphical user interface, not just a command prompt\nGreat learning tool\nFree for academic use\nPlatform agnostic\nOpen source"
  },
  {
    "objectID": "preWork/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "href": "preWork/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "title": "2  Introduction to RStudio",
    "section": "2.3 Creating a new project directory in RStudio",
    "text": "2.3 Creating a new project directory in RStudio\nLet’s create a new project directory for Systems Immunology.\n\nOpen RStudio\nGo to the File menu and select New Project.\nIn the New Project window, choose New Directory. Then, choose New Project. Name your new directory whatever you want and then “Create the project as subdirectory of:” the Desktop (or location of your choice).\nClick on Create Project.\nAfter your project is completed, if the project does not automatically open in RStudio, then go to the File menu, select Open Project, and choose [your project name].Rproj.\nWhen RStudio opens, you will see three panels in the window.\nGo to the File menu and select New File, and select R Script. The RStudio interface should now look like the screenshot below.\n\n\n\n\nRStudio interface\n\n\n\n2.3.1 What is a project in RStudio?\nIt is simply a directory that contains everything related your analyses for a specific project. RStudio projects are useful when you are working on context- specific analyses and you wish to keep them separate. When creating a project in RStudio you associate it with a working directory of your choice (either an existing one, or a new one). A . RProj file is created within that directory and that keeps track of your command history and variables in the environment. The . RProj file can be used to open the project in its current state but at a later date.\nWhen a project is (re) opened within RStudio the following actions are taken:\n\nA new R session (process) is started\nThe .RData file in the project’s main directory is loaded, populating the environment with any objects that were present when the project was closed.\nThe .Rhistory file in the project’s main directory is loaded into the RStudio History pane (and used for Console Up/Down arrow command history).\nThe current working directory is set to the project directory.\nPreviously edited source documents are restored into editor tabs\nOther RStudio settings (e.g. active tabs, splitter positions, etc.) are restored to where they were the last time the project was closed.\n\nInformation adapted from RStudio Support Site"
  },
  {
    "objectID": "preWork/rstudio.html#rstudio-interface",
    "href": "preWork/rstudio.html#rstudio-interface",
    "title": "2  Introduction to RStudio",
    "section": "2.4 RStudio Interface",
    "text": "2.4 RStudio Interface\nThe RStudio interface has four main panels:\n\nConsole: where you can type commands and see output. The console is all you would see if you ran R in the command line without RStudio.\nScript editor: where you can type out commands and save to file. You can also submit the commands to run in the console.\nEnvironment/History: environment shows all active objects and history keeps track of all commands run in console\nFiles/Plots/Packages/Help"
  },
  {
    "objectID": "preWork/rstudio.html#organizing-your-working-directory-setting-up",
    "href": "preWork/rstudio.html#organizing-your-working-directory-setting-up",
    "title": "2  Introduction to RStudio",
    "section": "2.5 Organizing your working directory & setting up",
    "text": "2.5 Organizing your working directory & setting up\n\n2.5.1 Viewing your working directory\nBefore we organize our working directory, let’s check to see where our current working directory is located by typing into the console:\n\ngetwd()\n\nYour working directory should be the Intro-to-R folder constructed when you created the project. The working directory is where RStudio will automatically look for any files you bring in and where it will automatically save any files you create, unless otherwise specified.\nYou can visualize your working directory by selecting the Files tab from the Files/Plots/Packages/Help window.\n\n\n\nViewing your working directory\n\n\nIf you wanted to choose a different directory to be your working directory, you could navigate to a different folder in the Files tab, then, click on the More dropdown menu and select Set As Working Directory.\n\n\n\nSetting your working directory\n\n\n\n\n2.5.2 Structuring your working directory\nTo organize your working directory for a particular analysis, you typically want to separate the original data (raw data) from intermediate datasets. For instance, you may want to create a data/ directory within your working directory that stores the raw data, and have a results/ directory for intermediate datasets and a figures/ directory for the plots you will generate.\nLet’s create these three directories within your working directory by clicking on New Folder within the Files tab.\n\n\n\nStructuring your working directory\n\n\nWhen finished, your working directory should look like:\n\n\n\nYour organized working directory\n\n\n\n\n2.5.3 Setting up\nThis is more of a housekeeping task. We will be writing long lines of code in our script editor and want to make sure that the lines “wrap” and you don’t have to scroll back and forth to look at your long line of code.\nClick on “Tools” at the top of your RStudio screen and click on “Global Options” in the pull down menu.\n\n\n\noptions\n\n\nOn the left, select “Code” and put a check against “Soft-wrap R source files”. Make sure you click the “Apply” button at the bottom of the Window before saying “OK”.\n\n\n\nwrap_options"
  },
  {
    "objectID": "preWork/rstudio.html#interacting-with-r",
    "href": "preWork/rstudio.html#interacting-with-r",
    "title": "2  Introduction to RStudio",
    "section": "2.6 Interacting with R",
    "text": "2.6 Interacting with R\nNow that we have our interface and directory structure set up, let’s start playing with R! There are two main ways of interacting with R in RStudio: using the console or by using script editor (plain text files that contain your code).\n\n2.6.1 Console window\nThe console window (in RStudio, the bottom left panel) is the place where R is waiting for you to tell it what to do, and where it will show the results of a command. You can type commands directly into the console, but they will be forgotten when you close the session.\n\n\n\nRunning in the console\n\n\n\n\n2.6.2 Script editor\nBest practice is to enter the commands in the script editor, and save the script. You are encouraged to comment liberally to describe the commands you are running using #. This way, you have a complete record of what you did, you can easily show others how you did it and you can do it again later on if needed.\nThe Rstudio script editor allows you to ‘send’ the current line or the currently highlighted text to the R console by clicking on the Run button in the upper-right hand corner of the script editor. Alternatively, you can run by simply pressing the Ctrl and Enter keys at the same time as a shortcut.\nNow let’s try entering commands to the script editor and using the comments character # to add descriptions and highlighting the text to run:\n\n    # Session 1\n    # Feb 3, 2023\n\n    # Interacting with R\n    \n    # I am adding 3 and 5. \n    3+5\n\n\n\n\nRunning in the script editor\n\n\nYou should see the command run in the console and output the result.\n\n\n\nScript editor output\n\n\nWhat happens if we do that same command without the comment symbol #? Re-run the command after removing the # sign in the front:\n\nI am adding 3 and 5. R is fun!\n3+5\n\nNow R is trying to run that sentence as a command, and it doesn’t work. We get an error in the console “Error: unexpected symbol in”I am” means that the R interpreter did not know what to do with that command.”\n\n\n2.6.3 Console command prompt\nInterpreting the command prompt can help understand when R is ready to accept commands. Below lists the different states of the command prompt and how you can exit a command:\nConsole is ready to accept commands: >.\nIf R is ready to accept commands, the R console shows a > prompt.\nWhen the console receives a command (by directly typing into the console or running from the script editor (Ctrl-Enter), R will try to execute it.\nAfter running, the console will show the results and come back with a new > prompt to wait for new commands.\nConsole is waiting for you to enter more data: +.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. Often this can be due to you having not ‘closed’ a parenthesis or quotation.\nEscaping a command and getting a new prompt: esc\nIf you’re in Rstudio and you can’t figure out why your command isn’t running, you can click inside the console window and press esc to escape the command and bring back a new prompt >.\n\n\n2.6.4 Keyboard shortcuts in RStudio\nIn addition to some of the shortcuts described earlier in this lesson, we have listed a few more that can be helpful as you work in RStudio.\n\n\n\n\n\n\n\nkey\naction\n\n\n\n\nCtrl+Enter\nRun command from script editor in console\n\n\nESC\nEscape the current command to return to the command prompt\n\n\nCtrl+1\nMove cursor from console to script editor\n\n\nCtrl+2\nMove cursor from script editor to console\n\n\nTab\nUse this key to complete a file path\n\n\nCtrl+Shift+C\nComment the block of highlighted text"
  },
  {
    "objectID": "preWork/rstudio.html#r-syntax",
    "href": "preWork/rstudio.html#r-syntax",
    "title": "2  Introduction to RStudio",
    "section": "2.7 R syntax",
    "text": "2.7 R syntax\nNow that we know how to talk with R via the script editor or the console, we want to use R for something more than adding numbers. To do this, we need to know more about the R syntax.\nThe main “parts of speech” in R (syntax) include:\n\nthe comments # and how they are used to document function and its content\nvariables and functions\nthe assignment operator <-\nthe = for arguments in functions\n\nNOTE: indentation and consistency in spacing is used to improve clarity and legibility\nWe will go through each of these “parts of speech” in more detail, starting with the assignment operator."
  },
  {
    "objectID": "preWork/rstudio.html#assignment-operator",
    "href": "preWork/rstudio.html#assignment-operator",
    "title": "2  Introduction to RStudio",
    "section": "2.8 Assignment operator",
    "text": "2.8 Assignment operator\nTo do useful and interesting things in R, we need to assign values to variables using the assignment operator, <-. For example, we can use the assignment operator to assign the value of 3 to x by executing:\n\nx <- 3\n\nThe assignment operator (<-) assigns values on the right to variables on the left.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key, on Mac type option + -) will write <- in a single keystroke."
  },
  {
    "objectID": "preWork/rstudio.html#variables",
    "href": "preWork/rstudio.html#variables",
    "title": "2  Introduction to RStudio",
    "section": "2.9 Variables",
    "text": "2.9 Variables\nA variable is a symbolic name for (or reference to) information. Variables in computer programming are analogous to “buckets”, where information can be maintained and referenced. On the outside of the bucket is a name. When referring to the bucket, we use the name of the bucket, not the data stored in the bucket.\nIn the example above, we created a variable or a ‘bucket’ called x. Inside we put a value, 3.\nLet’s create another variable called y and give it a value of 5.\n\ny <- 5\n\nWhen assigning a value to an variable, R does not print anything to the console. You can force to print the value by using parentheses or by typing the variable name.\n\ny\n\nYou can also view information on the variable by looking in your Environment window in the upper right-hand corner of the RStudio interface.\n\n\n\nViewing your environment\n\n\nNow we can reference these buckets by name to perform mathematical operations on the values contained within. What do you get in the console for the following operation:\n\nx + y\n\nTry assigning the results of this operation to another variable called number.\n\nnumber <- x + y\n\n\n2.9.1 Tips on variable names\nVariables can be given almost any name, such as x, current_temperature, or subject_id. However, there are some rules / suggestions you should keep in mind:\n\nMake your names explicit and not too long.\nAvoid names starting with a number (2x is not valid but x2 is)\nAvoid names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data) as variable names. When in doubt check the help to see if the name is already in use.\nAvoid dots (.) within a variable name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them.\nUse nouns for object names and verbs for function names\nKeep in mind that R is case sensitive (e.g., genome_length is different from Genome_length)\nBe consistent with the styling of your code (where you put spaces, how you name variable, etc.). In R, two popular style guides are Hadley Wickham’s style guide and Google’s."
  },
  {
    "objectID": "preWork/rstudio.html#best-practices",
    "href": "preWork/rstudio.html#best-practices",
    "title": "2  Introduction to RStudio",
    "section": "2.10 Best practices",
    "text": "2.10 Best practices\nBefore we move on to more complex concepts and getting familiar with the language, we want to point out a few things about best practices when working with R which will help you stay organized in the long run:\n\nCode and workflow are more reproducible if we can document everything that we do. Our end goal is not just to “do stuff”, but to do it in a way that anyone can easily and exactly replicate our workflow and results. All code should be written in the script editor and saved to file, rather than working in the console.\nThe R console should be mainly used to inspect objects, test a function or get help.\nUse # signs to comment. Comment liberally in your R scripts. This will help future you and other collaborators know what each line of code (or code block) was meant to do. Anything to the right of a # is ignored by R. A shortcut for this is Ctrl+Shift+C if you want to comment an entire chunk of text.\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session1/session1.html#learning-objectives",
    "href": "session1/session1.html#learning-objectives",
    "title": "Session 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nExplore how probability distributions inform the mathematical form of statistical tests.\nExplore different types of hypothesis tests and when they should be used.\nApply hypothesis tests commonly used in biological systems analyses.\nInstall and manage packages from CRAN and Bioconductor.\nIdentify and use different data types in R."
  },
  {
    "objectID": "session1/session1.html#note",
    "href": "session1/session1.html#note",
    "title": "Session 1",
    "section": "Note",
    "text": "Note\nIf you haven’t been to get R/RStudio running on your laptop, you can use this collab notebook today."
  },
  {
    "objectID": "session1/prob.html#defining-probability",
    "href": "session1/prob.html#defining-probability",
    "title": "4  Probability Primer",
    "section": "4.1 Defining Probability",
    "text": "4.1 Defining Probability\nInformally, we usually think of probability as a number that describes the likelihood of some event occurring, which ranges from zero (impossibility) to one (certainty).\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets. For a coin flip, the sample space is {heads, tails}. For a six-sided die, the sample space is each of the possible numbers that can appear: {1,2,3,4,5,6}. For the amount of time it takes to get to work, the sample space is all possible real numbers greater than zero (since it can’t take a negative amount of time to get somewhere, at least not yet).\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome. For example, this could be obtaining heads in a single coin flip, rolling a 4 on a throw of the die, or taking 21 minutes to get home by the new route.\n\nLet’s say that we have a sample space defined by N independent events, \\({E_1, E_2, ... , E_N}\\), and \\(X\\) is a random variable denoting which of the events has occurred. \\(P(X=E_i)\\) is the probability of event \\(i\\):\n\nProbability cannot be negative: \\(P(X=E_i) \\ge 0\\)\nThe total probability of all outcomes in the sample space is 1; that is, if the , if we take the probability of each Ei and add them up, they must sum to 1. We can express this using the summation symbol \\(\\sum\\): \\[\n\\sum_{i=1}^N{P(X=E_i)} = P(X=E_1) + P(X=E_2) + ... + P(X=E_N) = 1\n\\] This is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\n\nThe probability of any individual event cannot be greater than one: \\(P(X=E_i)\\le 1\\). This is implied by the previous point; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.\n\n\n4.1.1 Conditional probability\nThese definitions allow us to examine simple probabilities - that is, the probability of a single event or combination of events.\nHowever, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.\nTo compute the conditional probability of A given B (which we write as \\(P(A|B)\\), “probability of A, given B”), we need to know the joint probability (that is, the probability of both A and B occurring) as well as the overall probability of B:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThat is, we want to know the probability that both things are true, given that the one being conditioned upon is true.\n\n\n4.1.2 Independence\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other. This can be expressed as:\n\\[\nP(A|B) = P(A)\n\\]\nThat is, the probability of A given some value of B is just the same as the overall probability of A."
  },
  {
    "objectID": "session1/prob.html#probability-distributions",
    "href": "session1/prob.html#probability-distributions",
    "title": "4  Probability Primer",
    "section": "4.2 Probability distributions",
    "text": "4.2 Probability distributions\nA probability distribution describes the probability of all of the possible outcomes in an experiment. To help understand distributions and how they can be used, let’s look at a few discrete probability distributions, meaning distributions which can only output integers.\n\n4.2.1 Binomial success counts\nTossing a coin has two possible outcomes. This simple experiment, called a Bernoulli trial, is modeled using a so-called Bernoulli random variable.\nR has special functions tailored to generate outcomes for each type of distribution. They all start with the letter r, followed by a specification of the model, here rbinom, where binom is the abbreviation used for binomial.\nSuppose we want to simulate a sequence of 15 fair coin tosses. To get the outcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a fair coin), we write:\n\nrbinom(15, prob = 0.5, size = 1)\n\n [1] 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1\n\n\nWe use the rbinom function with a specific set of parameters (called arguments in programming): the first parameter is the number of trials we want to observe; here we chose 15. We designate by prob the probability of success. By size=1 we declare that each individual trial consists of just one single coin toss.\nFor binary events such as heads or tails, success or failure, CpG or non-CpG, M or F, Y = pyrimidine or R = purine, diseased or healthy, true or false, etc. we only need the probability \\(p\\) of one of the events (which we, often arbitrarily, will label “success”) because “failure” (the complementary event) will occur with probability \\(1-p\\). We can then simply count the number of successes for a certain number of trials:\n\nrbinom(1, prob = 0.3, size = 15)\n\n[1] 6\n\n\nThis gives us the number of successes for \\(15\\) trials where the probability of success was \\(0.3\\). We would call this number a binomial random variable or a random variable that follows the \\(B(15, 0.3)\\) distribution.\nWe can plot the probability mass distribution using dbinom:\n\nprobabilities <- dbinom(0:15, prob = 0.3, size = 15)\nbarplot(probabilities, names.arg = 0:15, col = \"brown\")\n\n\n\n\nFor \\(X\\) distributed as a binomial distribution with parameters \\((n,p)\\), written \\(X ~ B(n,p)\\) the probability of seeing \\(X=k\\) sucesses is: \\[\nP(k; n,p) = P(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}\n\\]\n\n\n4.2.2 Poisson distributions\nWhen the probability of success \\(p\\) is small and the number of trials \\(n\\) large, the binomial distribution \\(B(n,p)\\) can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter \\(\\lambda = np\\)\nThe Poisson distribution comes up often in biology as we often are naturally dealing very low probability events and large numbers of trials, such as mutations in a genome.\n\nsimulations = rbinom(n = 300000, prob = 5e-4, size = 10000)\nbarplot(table(simulations), col = \"lavender\")\n\n\n\nprobabilities <- dpois(0:18, lambda=(10000 * 5e-4))\nbarplot(probabilities, names.arg = 0:18, col = \"brown\")\n\n\n\n\n\n\n4.2.3 Multinomial distributions\nWhen modeling four possible outcomes, for instance when studying counts of the four nucleotides [A,C,G] and [T], we need to extend the binomial model.\nWe won’t go into detail on the formulation, but we can examine probabilities of observations using a vector of counts for each observed outcome, and a vector of probabilities for each outcome (which must sum to 1).\n\ncounts <- c(4,2,0,0)\nprobs <- c(0.25,0.25,0.25,0.25)\ndmultinom(counts, prob = probs)\n\n[1] 0.003662109\n\n\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session1/hypo_tests.html#sec-epitope",
    "href": "session1/hypo_tests.html#sec-epitope",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.1 Calculating the chance of an event",
    "text": "5.1 Calculating the chance of an event\nWhen testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes.\nEpitope: A specific portion of a macromolecular antigen to which an antibody binds. In the case of a protein antigen recognized by a T-cell, the epitope or determinant is the peptide portion or site that binds to a Major Histocompatibility Complex (MHC) molecule for recognition by the T cell receptor (TCR).\nEnzyme-Linked ImmunoSorbent Assays (ELISA) are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:\n\nThe baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this \\(P(declare epitope|no epitope)\\)\nThe protein is tested at 100 different positions, supposed to be independent.\nWe are going to examine a collection of 50 patient samples.\n\nThe data for one patient’s assay look like this:\n[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nwhere the 1 signifies a hit (and thus the potential for an allergic reaction), and the zeros signify no reaction at that position.\nWe’re going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed \\((0,1)\\) variables to have a Poisson distribution with parameter \\(0.5\\).\n\nload(\"../data/e100.RData\")\nbarplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n  names.arg = seq(along = e100), col = \"darkolivegreen\")\n\n\n\n\nThe spike is striking. What are the chances of seeing a value as large as 7, if no epitope is present? If we look for the probability of seeing a number as big as 7 (or larger) when considering one \\(Poisson(0.5)\\) random variable, the answer can be calculated in closed form as\n\\[\nP(X \\ge 7) = \\sum\\limits_{k=7}^{\\infty}P(X=k)\n\\] This is, of course, the same as \\(1-P(X \\le 6)\\). The probability is the so-called cumulative distribution function at 6, and R has the function ppois for computing it, which we can use in either of the following two ways:\n\n1 - ppois(6, 0.5)\n\n[1] 1.00238e-06\n\nppois(6, 0.5, lower.tail = FALSE)\n\n[1] 1.00238e-06\n\n\nYou can use the command ?ppois to see the argument definitions for the function.\nWe denote this number, our chance of seeing such an extreme result, as \\(\\epsilon\\). However, in this case it would be the incorrect calculation.\nInstead of asking what the chances are of seeing a Poisson(0.5) as large as 7, we need to instead ask, what are the chances that the maximum of 100 Poisson(0.5) trials is as large as 7? We order the data values \\(x_1, x_2, ... , x_{100}\\) and rename them \\(x_{(1)}, x_{(2)}, ... , x_{(100)}\\), so that denotes \\(x_{(1)}\\) the smallest and \\(x_{(100)}\\) the largest of the counts over the 100 positions. Together, are called the rank statistic of this sample of 100 values.\nThe maximum value being as large as 7 is the complementary event of having all 100 counts be smaller than or equal to 6. Two complementary events have probabilities that sum to 1. Because the positions are supposed to be independent, we can now do the computation:\n\\[\nP(x_{(100)} \\ge 7) = \\prod\\limits_{i=1}^{100}P(x_i \\le 6) = (P(x_i \\le 6))^{100}\n\\] which, using our notation, is \\((1-\\epsilon)^{100}\\) and is approximately \\(10^{-4}\\). This is a very small chance, so we would determine it is most likely that we did detect real epitopes."
  },
  {
    "objectID": "session1/hypo_tests.html#computing-probabilities-with-simulations",
    "href": "session1/hypo_tests.html#computing-probabilities-with-simulations",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.2 Computing probabilities with simulations",
    "text": "5.2 Computing probabilities with simulations\nIn the case we just saw, the theoretical probability calculation was quite simple and we could figure out the result by an explicit calculation. In practice, things tend to be more complicated, and we are better to compute our probabilities using the Monte Carlo method: a computer simulation based on our generative model that finds the probabilities of the events we’re interested in. Below, we generate 100,000 instances of picking the maximum from 100 Poisson distributed numbers.\n\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n    1     2     3     4     5     6     7     8 \n    7 23439 60557 14282  1563   142     8     2 \n\n\nSo we can approximate the probability of seeing a \\(7\\) as:\n\nmean( maxes >= 7 )\n\n[1] 1e-04\n\n\nWe arrive at a similarly small number, and in both cases would determine that there are real epitopes in the dataset."
  },
  {
    "objectID": "session1/hypo_tests.html#an-example-coin-tossing",
    "href": "session1/hypo_tests.html#an-example-coin-tossing",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.3 An example: coin tossing",
    "text": "5.3 An example: coin tossing\nLet’s look a simpler example: flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like HHTTHTHTT...\nLet’s simulate the experiment in R, using a biased coin:\n\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\n# Sample is a function in base R which let's us take a random sample from a vector, with or without replacement. \n# This line is sampling numFlips times from the vector ['H','T'] with replacement, with the probabilities for \n# each item in the vector being defined in the prob argument as [probHead, 1-probHead]\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\n# Thus, coinFlips is a character vector of a random sequence of 'T' and 'H'. \nhead(coinFlips)\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n\n\nNow, if the coin were fair, we would expect half of the time to get heads. Let’s see.\n\ntable(coinFlips)\n\ncoinFlips\n H  T \n59 41 \n\n\nThat is different from 50/50. However, does the data deviates strong enough to conclude that this coin isn’t fair? We know that the total number of heads seen in 100 coin tosses for a fair coin follows \\(B(100, 0.5)\\), making it a suitable test statistic.\nTo decide, let’s look at the sampling distribution of our test statistic – the total number of heads seen in 100 coin tosses – for a fair coin. As we learned, we can do this with the binomial distribution. Let’s plot a fair coin and mark our observation with a blue line:\n\nlibrary(\"dplyr\")\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(\"ggplot2\")\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n# This line sets k as the vector [0, 1, 2,...,numFlips]\nk <- 0:numFlips\n# Recall that binary variables (TRUE and FALSE) are interpreted as 1 and 0, so we can use this operation\n# to count the number of heads in coinFlips. We practice these kinds of operations in session 3. \nnumHeads <- sum(coinFlips == \"H\")\n# We use dbinom here to get the probability mass at every integer from 1-numFlips so that we can plot the distribution. \np <- dbinom(k, size = numFlips, prob = 0.5)\n# We then convert it into a dataframe for easier plotting. \nbinomDensity <- data.frame(k = k, p = p)\nhead(binomDensity)\n\n  k            p\n1 0 7.888609e-31\n2 1 7.888609e-29\n3 2 3.904861e-27\n4 3 1.275588e-25\n5 4 3.093301e-24\n6 5 5.939138e-23\n\n# Here, we are plotting the binomial distribution, with a vertical line representing\n# the number of heads we actually observed. We will learn how to create plots in session 4. \n# Thus, to complete our test we simply need to identify whether or not the blue line\n# is in our rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n\n\n\nHow do we quantify whether the observed value is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased?\nWe divide the set of all possible \\(k(0-100)\\) in two complementary subsets, the rejection region and the region of no rejection. We want to make the rejection region as large as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\alpha\\)(say, 0.05).\n\nalpha <- 0.05\n# We get the density of our plot in sorted order, meaning that we'll see binomDensity\n# jump back and forth between the distribution's tails as p increases. \nbinomDensity <- binomDensity[order(p),]\n# We then manually calculate our rejection region by finding where the cumulative sum in the distribution\n# is less than or equal to our chosen alpha level. \nbinomDensity$reject <- cumsum(binomDensity$p) <= alpha\nhead(binomDensity)\n\n      k            p reject\n1     0 7.888609e-31   TRUE\n101 100 7.888609e-31   TRUE\n2     1 7.888609e-29   TRUE\n100  99 7.888609e-29   TRUE\n3     2 3.904861e-27   TRUE\n99   98 3.904861e-27   TRUE\n\n# Now we recreate the same plot as before, but adding red borders around the parts of our distribution\n# in the rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nWe sorted the \\(p\\)-values from lowest to highest (order), and added a column reject by computing the cumulative sum (cumsum) of the \\(p\\)-values and thresholding it against alpha.\nThe logical column reject therefore marks with TRUE a set of \\(k\\)s whose total probability is less than \\(\\alpha\\).\nThe rejection region is marked in red, containing both very large and very small values of \\(k\\), which can be considered unlikely under the null hypothesis.\nR provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom). Those are more precise and faster than cumsum over the probabilities.\nThe (cumulative) distribution function is defined as the probability that a random variable \\(X\\) will take a value less than or equal to \\(x\\).\n\\[F(x) = P(X \\le x)\\]\nWe have just gone through the steps of a binomial test. This is a frequently used test and therefore available in R as a single function.\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\n\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59"
  },
  {
    "objectID": "session1/hypo_tests.html#hypothesis-tests",
    "href": "session1/hypo_tests.html#hypothesis-tests",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.4 Hypothesis Tests",
    "text": "5.4 Hypothesis Tests\nWe can summarize what we just did with a series of steps:\n\nDecide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\nSet up a null hypothesis, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\nDecide on the rejection region, i.e., a subset of possible outcomes whose total probability is small.\nDo the experiment and collect the data; compute the test statistic.\nMake a decision: reject the null hypothesis if the test statistic is in the rejection region."
  },
  {
    "objectID": "session1/hypo_tests.html#types-of-error",
    "href": "session1/hypo_tests.html#types-of-error",
    "title": "5  Distributions to Hypothesis Tests",
    "section": "5.5 Types of Error",
    "text": "5.5 Types of Error\n\n\n\nFrom “Modern Statistics for Modern Biology”\n\n\nHaving set out the mechanics of testing, we can assess how well we are doing. The following table, called a confusion matrix, compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n\n\n\n\n\n\n\nTest vs reality\nNull is true\nNull is false\n\n\n\n\nReject null\nType I error (false positive)\nTrue postitive\n\n\nDo not reject null\nTrue negative\nType II error (false negative)\n\n\n\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade-off between both of them. We can always decrease the false positive rate (FPR) by shifting the threshold to the right. We can become more “conservative”. But this happens at the price of higher false negative rate (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. T he FPR is the same as the probability \\(\\alpha\\) that we mentioned above. \\(1-\\alpha\\) is also called the specificity of a test. The FNR is sometimes also called \\(\\beta\\), and \\(1-\\beta\\) the power, sensitivity or true positive rate of a test. The power of a test can be understood as the likelihood of it “catching” a true positive, or correctly rejecting the null hypothesis.\nGenerally, there are three factors that can affect statistical power:\n\nSample size: Larger samples provide greater statistical power\nEffect size: A given design will always have greater power to find a large effect than a small effect (because finding large effects is easier)\nType I error rate: There is a relationship between Type I error and power such that (all else being equal) decreasing Type I error will also decrease power.\n\nIn a future session, we will also see how hypothesis tests can be seen as types of linear models.\n\nThe materials in this lesson have been adapted from: - Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes. - Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session1/factors.html#factors",
    "href": "session1/factors.html#factors",
    "title": "6  Categorical Data in R",
    "section": "6.1 Factors",
    "text": "6.1 Factors\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression <- c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\")\n\nThe elements of this expression factor created previously has following categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression, and let’s using nesting here:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] \"high\" \"high\"\n\n\n\nNesting note:\nThe piece of code above was more efficient with nesting; we used a single step instead of two steps as shown below:\nStep1 (no nesting): idx <- expression == \"high\"\nStep2 (no nesting): expression[idx]"
  },
  {
    "objectID": "session1/factors.html#releveling-factors",
    "href": "session1/factors.html#releveling-factors",
    "title": "6  Categorical Data in R",
    "section": "6.2 Releveling factors",
    "text": "6.2 Releveling factors\nWe have briefly talked about factors, but this data type only becomes more intuitive once you’ve had a chance to work with it. Let’s take a slight detour and learn about how to relevel categories within a factor.\nTo view the integer assignments under the hood you can use str():\n\nexpression\n\n [1] \"high\"   \"low\"    \"low\"    \"medium\" \"high\"   \"medium\" \"medium\" \"low\"   \n [9] \"low\"    \"low\"   \n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3, i.e. it makes sense for us to “relevel” the categories in this factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression <- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can re-factor a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nNote: Releveling becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment."
  },
  {
    "objectID": "session1/choosing_tests.html#performing-a-hypothesis-test",
    "href": "session1/choosing_tests.html#performing-a-hypothesis-test",
    "title": "7  Performing and choosing hypothesis tests",
    "section": "7.1 Performing a Hypothesis Test",
    "text": "7.1 Performing a Hypothesis Test\nMany experimental measurements are reported as rational numbers, and the simplest comparison we can make is between two groups, say, cells treated with a substance compared to cells that are not. The basic test for such situations is the t-test. The test statistic is defined as\n\\[\nt = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\\]\nwhere \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are the means of the two groups, \\(S^2_1\\) and \\(S^2_2\\) are the estimated variances of the groups, and \\(n_1\\) and \\(n_2\\) are the sizes of the two groups. Because the variance of a difference between two independent variables is the sum of the variances of each individual variable (\\(var(A - B) = var(A) + var(B)\\)), we add the variances for each group divided by their sample sizes in order to compute the standard error of the difference. Thus, one can view the the t statistic as a way of quantifying how large the difference between groups is in relation to the sampling variability of the difference between means.\nLet’s try this out with the PlantGrowth data from R’s datasets package.\n\nlibrary(\"ggbeeswarm\")\n\nWarning: package 'ggbeeswarm' was built under R version 4.2.2\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\ndata(\"PlantGrowth\")\nggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n  geom_beeswarm() + theme(legend.position = \"none\")\n\n\n\ntt1 = t.test(PlantGrowth$weight[PlantGrowth$group ==\"ctrl\"],\n      PlantGrowth$weight[PlantGrowth$group ==\"trt1\"],\n      var.equal = TRUE)\ntt2 = t.test(PlantGrowth$weight[PlantGrowth$group ==\"ctrl\"],\n      PlantGrowth$weight[PlantGrowth$group ==\"trt2\"],\n      var.equal = TRUE)\ntt1\n\n\n    Two Sample t-test\n\ndata:  PlantGrowth$weight[PlantGrowth$group == \"ctrl\"] and PlantGrowth$weight[PlantGrowth$group == \"trt1\"]\nt = 1.1913, df = 18, p-value = 0.249\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2833003  1.0253003\nsample estimates:\nmean of x mean of y \n    5.032     4.661 \n\ntt2\n\n\n    Two Sample t-test\n\ndata:  PlantGrowth$weight[PlantGrowth$group == \"ctrl\"] and PlantGrowth$weight[PlantGrowth$group == \"trt2\"]\nt = -2.134, df = 18, p-value = 0.04685\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.980338117 -0.007661883\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n\nTo compute the p-value, the t.test function uses the asymptotic theory for the t-statistic. This theory states that under the null hypothesis of equal means in both groups, the statistic follows a known, mathematical distribution, the so-called t-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. The theory uses additional technical assumptions, namely that the data are independent and come from a normal distribution with the same standard deviation.\nIn fact, most of the tests we will look at assume that the data come from a normal distribution. That the normal distribution comes up so often is largly explained by the central limit theorem in statistics. The Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is also known as the Gaussian distribution. The normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution).\nThe bell-like shape of the distribution never changes, only its location and width.\nAn important note about the central limit theorem is that it is asymptotic, meaning that it is true as the size of our dataset approaches infinity. For very small sample sizes, even if we are taking the mean of our samples the data might not follow the normal distribution closely enough for tests which assume it to make sense.\nThe independence assumption\nNow let’s try something peculiar: duplicate the data.\n\nwith(rbind(PlantGrowth, PlantGrowth),\n       t.test(weight[group == \"ctrl\"],\n              weight[group == \"trt2\"],\n              var.equal = TRUE))\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -3.1007, df = 38, p-value = 0.003629\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8165284 -0.1714716\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n\nNote that estimates of the group means (and thus the difference) are unchanged, but the \\(p\\)-value is now much smaller!"
  },
  {
    "objectID": "session1/choosing_tests.html#choosing-the-right-test",
    "href": "session1/choosing_tests.html#choosing-the-right-test",
    "title": "7  Performing and choosing hypothesis tests",
    "section": "7.2 Choosing the Right Test",
    "text": "7.2 Choosing the Right Test\n\n7.2.1 Variable Types (Effect)\nThe types of our variables need to be considered. We will go through some choices if our variables are quantitative (continuous; a number or qualitative (discrete; a category or factor). However, note that other tests exist for some specific properties like proportions.\nIf we wish to consider the relationship between two quantitative variables, we need to perform a correlation analysis. The Pearson correlation directly analyses the numbers (is parametric) while Spearman’s rank correlation considers ranks (and is nonparametric).\nFor two qualitative variables, we typically will use a Chi-square test of independence, though we may be able to use Fisher’s exact test if the dataset is small enough.\nWe often are interested in the case where we want to see the relationship between one quantitative variable and one qualitative variable. In this case, we most commonly use some variation of a t-test if we have only have 2 groups we are considering, and some variation of an ANOVA test if we have more than 2. We will get into more detail about ANOVA tests in a future session.\n\n\n7.2.2 Paired vs Unpaired\nPaired and unpaired tests refer to whether or not there is a 1:1 correspondence between our different observations. Experiments which involve measuring the same set of biological samples, often as before and after some kind of treatment, are paired. In paired experiments we can look at each observation, see whether it individually changed between groups.\nIn unpaired tests we consider our samples to be independent across groups. This is the case if we have two different groups, such as a control group and a treatment group.\nPerforming a paired or unpaired test can be set as an argument in R’s t.test function, but nonparametric tests have different names, the Mann-Whitney U test for unpaired samples and the Wilcoxon signed-rank test for paired samples in tests with 2 groups, and the Kruskal-Wallis test and Friedman test for more than two groups.\n\n\n7.2.3 Parametric vs Non-Parametric\nSo far, we have only seen parametric tests. These are tests which are based on a statistical distribution, and thus depends on having defined parameters. These tests inherently assume that the collected data follows some distribution, typically a normal distribution as discussed above.\nA nonparametric test makes many fewer assumptions about the distribution of our data. Instead of dealing with values directly, they typically perform their calculations on rank. This makes them especially good at dealing with extreme values and outliers. However, they are typically less powerful than parametric tests; they will be less likely to reject the null hypothesis (return a higher p-value) if the data did follow a normal distribution and you had performed a parametric test on it. Thus, they should only be used if necessary.\nA typical rule of thumb is that around \\(30\\) samples is enough to not have to worry about the underlying distribution of your data. However, they are types of data, such as directly collecting ranking data or ratings, which should be analyzed with nonparametric methods.\n\n\n7.2.4 One-tailed and Two-tailed tests\nAll tests have one-tailed and two-tailed versions. A two-tailed test considers a result significant if it is extreme in either direction; it can be higher or lower than what would be expected under the null hypothesis. A one-tailed test will only consider a single direction, either higher or lower. Usually, the p value for the two-tailed test is twice as large as that for the one-tailed test, which reflects the fact that an extreme value is less surprising since it could have occurred in either direction.\nHow do you choose whether to use a one-tailed versus a two-tailed test? The two-tailed test is always going to be more conservative, so it’s always a good bet to use that one, unless you had a very strong prior reason for using a one-tailed test. This is set through the alternative argument in t.test.\n\n\n7.2.5 Variance\nAnother underlying assumption of many statistical tests is that different groups have the same variance. The t-test will perform a slightly more conservative calculation if equal variance is not assumed (called Welch’s t-test instead of Student’s t-test). This can be set as the var.equal argument of t.test.\nWe often can assume equal variance, but as we will see in a later session, many modern sequencing technologies can produce data with patterns in its variance we will have to adjust for.\n\n\n7.2.6 How Many Variables of Interest?\nAll of the above discussion is for experiments with where we are interested in looking at the relationship between two variables. These, slightly confusingly, are called 2 sample tests, and line up with the classical experimental paradigm of a single dependent and a single independent variable. However, there are other options.\n\nOne Sample: Instead of wanting to compare how a categorical variable (like treatment) affects some outcome variable, we could imagine comparing against some known value. When we considered whether or not a coin was fair, we were not comparing two coins, but instead comparing the output of one coin against a known value.\nMore than two samples: Modern observational studies often, by necessity, need to consider how many variables affect some outcome. These analyses are performed via regression models, multiple linear regression for a quantitative dependent variable and logistic regression for a qualitative dependent variable."
  },
  {
    "objectID": "session1/pSet1.html#problem-1",
    "href": "session1/pSet1.html#problem-1",
    "title": "8  Problem Set 1",
    "section": "8.1 Problem 1",
    "text": "8.1 Problem 1\nR can generate numbers from all known distributions. We now know how to generate random discrete data using the specialized R functions tailored for each type of distribution. We use the functions that start with an r as in rXXXX, where XXXX could be pois, binom, multinom. If we need a theoretical computation of a probability under one of these models, we use the functions dXXXX, such as dbinom, which computes the probabilities of events in the discrete binomial distribution, and dnorm, which computes the probability density function for the continuous normal distribution. When computing tail probabilities such as \\(P(X > a)\\) it is convenient to use the cumulative distribution functions, which are called pXXXX. Find two other discrete distributions that could replace the XXXX above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOther discrete distributions in R:\n\nGeometric distribution: geom\nHypergeometric distribution: hyper\nNegative binomial distribution: nbinom\n\nYou can type in ?Distributions to see a list of available distributions in base R. You can also view this information online here, and a list of distributions included in other packages here."
  },
  {
    "objectID": "session1/pSet1.html#problem-2",
    "href": "session1/pSet1.html#problem-2",
    "title": "8  Problem Set 1",
    "section": "8.2 Problem 2",
    "text": "8.2 Problem 2\nHow would you calculate the probability mass at the value \\(X=2\\) for a binomial \\(B(10, 0.3)\\) with dbinom? Use dbinom to compute the cumulative distribution at the value 2, corresponding to \\(P(X \\leq 2)\\), and check your answer with another R function. Hint: You will probably want to use the sum function.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe dbinom function directly gives us the probabilty mass:\n\ndbinom(2, 10, 0.3)\n\n[1] 0.2334744\n\n\nSince the binomial distribution is discrete, we can get the cumulative distribution function by simply summing the mass at 0, 1, and 2. Note that if this were a continuous distribution, we would have to integrate the mass function over the range instead.\nRecall that we can pass a vector into functions like dbinom to get multiple values at once:\n\ndbinom(0:2, 10, 0.3)\n\n[1] 0.02824752 0.12106082 0.23347444\n\n\nWe can then simply sum the result:\n\nsum(dbinom(0:2, 10, 0.3))\n\n[1] 0.3827828\n\n\nWe can now check our answer with the pbinom function which directly gives the cumulative distribution function:\n\npbinom(2, 10, 0.3)\n\n[1] 0.3827828"
  },
  {
    "objectID": "session1/pSet1.html#problem-3",
    "href": "session1/pSet1.html#problem-3",
    "title": "8  Problem Set 1",
    "section": "8.3 Problem 3",
    "text": "8.3 Problem 3\nIn the epitope example (Section 5.1), use a simulation to find the probability of having a maximum of 9 or larger in 100 trials. How many simulations do you need if you would like to prove that “the probability is smaller than 0.000001”?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimulation solution (what was asked for)\nWe can re-examine the results of the simulation we ran during class:\n\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n    1     2     3     4     5     6     7     8 \n    8 23287 60587 14420  1550   137     9     2 \n\n\nHowever, most of the time we don’t even get a single 9! We need to increase the number of trials in order to see more extreme numbers:\n\nmaxes = replicate(10000000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\nmaxes\n      1       2       3       4       5       6       7       8       9 \n    761 2345330 6046383 1436857  156284   13338     975      69       3 \n\n\nThis calculation may take awhile to run. When running it I got 6 instances of 9 counts, so we can estimate the probability as: \\(6/10000000 = 6\\times 10^{-7}\\). We can see that the lower-probability of an event we want to estimate, the more simulations we need to run and the more computational power we need.\nWe would need at least a million runs in order to be able to estimate a probability of \\(0.000001\\), as \\(1/0.000001 = 1000000\\).\nHow you would caluclate things exactly\nIn the epitope example we were able to calculate the probability of a single assay having a count of at least 7 as:\n\n1 - ppois(6, 0.5)\n\n[1] 1.00238e-06\n\n\nAnd then the probability of seeing a number this extreme at least once among 100 assays as:\n\n1 - ppois(6, 0.5)^100\n\n[1] 0.000100233\n\n\nIn order to calculate the probability of a maximum of 9 or larger, we simply need to alter our complementary event probabilty calculation to 8:\n\n1 - ppois(8, 0.5)^100\n\n[1] 3.43549e-07"
  },
  {
    "objectID": "session1/pSet1.html#problem-4",
    "href": "session1/pSet1.html#problem-4",
    "title": "8  Problem Set 1",
    "section": "8.4 Problem 4",
    "text": "8.4 Problem 4\nFind a paper in your research area which uses a hypothesis test. Cite the paper and note:\n\nThe null hypothesis.\nThe alternative hypothesis.\nWas the test two-tailed or one-tailed?\nWhat types of variables were compared?\nWas the test parametric or non-parametric?\nCan we safely assume equal variance?\nWhat was the sample size?\n\nIf the necessary details to determine any of the above are not in the paper, you can note that instead.\nGiven what you’ve written and the author’s decisions, do you agree with the choice of hypothesis test and the conclusions drawn?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution here obviously varies. In order to determine whether or not a test was used correctly, we need to at least consider: - The validity of the null and alternative hypotheses - Whether or not the assumptions of the test (independent samples, variable type, parametric or non-parametric, etc., uniform variance, etc.) hold or at least probably mostly hold for the experiment. - Whether there is any indication of p-hacking or sources of experimental bias.\n\n\n\n\nThe materials in this lesson have been adapted from: Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "session2/session2.html#learning-objectives",
    "href": "session2/session2.html#learning-objectives",
    "title": "Session 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nConvert and re-level factor data.\nDetermine which hypothesis test is appropriate for common biological analyses.\nUse and create functions in R.\nApply and interpret multiple hypotheses testing corrections.\nImplement hypothesis tests using R."
  },
  {
    "objectID": "session2/packages.html",
    "href": "session2/packages.html",
    "title": "9  Packages and Libraries",
    "section": "",
    "text": "Packages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages and growing.\nThere are a set of standard (or base) packages which are considered part of the R source code and automatically available as part of your R installation. Base packages contain the basic functions that allow R to work, and enable standard statistical and graphical functions on datasets; for example, all of the functions that we have been using so far in our examples.\nThe directories in R where the packages are stored are called the libraries. The terms package and library are sometimes used synonymously and there has been discussion amongst the community to resolve this. It is somewhat counter-intuitive to load a package using the library() function and so you can see how confusion can arise.\nYou can check what libraries are loaded in your current R session by typing into the console:\nsessionInfo() #Print version information about R, the OS and attached or loaded packages\n\n# OR\n\nsearch() #Gives a list of attached packages\nPreviously we have introduced you to functions from the standard base packages. However, the more you work with R, you will come to realize that there is a cornucopia of R packages that offer a wide variety of functionality. To use additional packages will require installation. Many packages can be installed from the CRAN or Bioconductor repositories.\n\n9.0.1 Helpful tips for package installations\n\nPackage names are case sensitive!\nAt any point (especially if you’ve used R/Bioconductor in the past), in the console R may ask you if you want to “update any old packages by asking Update all/some/none? [a/s/n]:”. If you see this, type “a” at the prompt and hit Enter to update any old packages. Updating packages can sometimes take awhile to run. If you are short on time, you can choose “n” and proceed. Without updating, you run the risk of conflicts between your old packages and the ones from your updated R version later down the road.\nIf you see a message in your console along the lines of “binary version available but the source version is later”, followed by a question, “Do you want to install from sources the package which needs compilation? y/n”, type n for no, and hit enter.\n\n\n\n9.0.2 Package installation from CRAN\nCRAN is a repository where the latest downloads of R (and legacy versions) are found in addition to source code for thousands of different user contributed R packages.\n\nPackages for R can be installed from the CRAN package repository using the install.packages function. This function will download the source code from on the CRAN mirrors and install the package (and any dependencies) locally on your computer.\nAn example is given below for the ggplot2 package that will be required for some plots we will create later on. Run this code to install ggplot2.\ninstall.packages(\"ggplot2\")\n\n\n9.0.3 Package installation from Bioconductor\nAlternatively, packages can also be installed from Bioconductor, another repository of packages which provides tools for the analysis and comprehension of high-throughput genomic data. These packages includes (but is not limited to) tools for performing statistical analysis, annotation packages, and accessing public datasets.\n\nThere are many packages that are available in CRAN and Bioconductor, but there are also packages that are specific to one repository. Generally, you can find out this information with a Google search or by trial and error.\nTo install from Bioconductor, you will first need to install BiocManager. This only needs to be done once ever for your R installation.\n# DO NOT RUN THIS!\n\ninstall.packages(\"BiocManager\")\nNow you can use the install() function from the BiocManager package to install a package by providing the name in quotations.\nHere we have the code to install ggplot2, through Bioconductor:\n# DO NOT RUN THIS!\n\nBiocManager::install(\"ggplot2\")\n\nThe code above may not be familiar to you - it is essentially using a new operator, a double colon :: to execute a function from a particular package. This is the syntax: package::function_name().\n\n\n\n9.0.4 Package installation from source\nFinally, R packages can also be installed from source. This is useful when you do not have an internet connection (and have the source files locally), since the other two methods are retrieving the source files from remote sites.\nTo install from source, we use the same install.packages function but we have additional arguments that provide specifications to change from defaults:\n# DO NOT RUN THIS!\n\ninstall.packages(\"~/Downloads/ggplot2_1.0.1.tar.gz\", type=\"source\", repos=NULL)\n\n\n9.0.5 Loading libraries\nOnce you have the package installed, you can load the library into your R session for use. Any of the functions that are specific to that package will be available for you to use by simply calling the function as you would for any of the base functions. Note that quotations are not required here.\nlibrary(ggplot2)\nYou can also check what is loaded in your current environment by using sessionInfo() or search() and you should see your package listed as:\nother attached packages:\n[1] ggplot2_2.0.0\nIn this case there are several other packages that were also loaded along with ggplot2.\nWe only need to install a package once on our computer. However, to use the package, we need to load the library every time we start a new R/RStudio environment. You can think of this as installing a bulb versus turning on the light.\n\nAnalogy and image credit to Dianne Cook of Monash University.\n\n\n9.0.6 Finding functions specific to a package\nThis is your first time using ggplot2, how do you know where to start and what functions are available to you? One way to do this, is by using the Package tab in RStudio. If you click on the tab, you will see listed all packages that you have installed. For those libraries that you have loaded, you will see a blue checkmark in the box next to it. Scroll down to ggplot2 in your list:\n\nIf your library is successfully loaded you will see the box checked, as in the screenshot above. Now, if you click on ggplot2 RStudio will open up the help pages and you can scroll through.\nAn alternative is to find the help manual online, which can be less technical and sometimes easier to follow. For example, this website is much more comprehensive for ggplot2 and is the result of a Google search. Many of the Bioconductor packages also have very helpful vignettes that include comprehensive tutorials with mock data that you can work with.\nIf you can’t find what you are looking for, you can use the rdocumention.org website that search through the help files across all packages available.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/loading_functions.html#read.csv",
    "href": "session2/loading_functions.html#read.csv",
    "title": "10  Reading data into R",
    "section": "10.1 read.csv()",
    "text": "10.1 read.csv()\nYou can check the arguments for the function using the ? to ensure that you are entering all the information appropriately:\n?read.csv\n\nThe first thing you will notice is that you’ve pulled up the documentation for read.table(), this is because that is the parent function and all the other functions are in the same family.\nThe next item on the documentation page is the function Description, which specifies that the output of this set of functions is going to be a data frame - “Reads a file in table format and creates a data frame from it, with cases corresponding to lines and variables to fields in the file.”\nIn usage, all of the arguments listed for read.table() are the default values for all of the family members unless otherwise specified for a given function. Let’s take a look at 2 examples: 1. The separator - * in the case of read.table() it is sep = \"\" (space or tab) * whereas for read.csv() it is sep = \",\" (a comma). 2. The header - This argument refers to the column headers that may (TRUE) or may not (FALSE) exist in the plain text file you are reading in. * in the case of read.table() it is header = FALSE (by default, it assumes you do not have column names) * whereas for read.csv() it is header = TRUE (by default, it assumes that all your columns have names listed).\nThe take-home from the “Usage” section for read.csv() is that it has one mandatory argument, the path to the file and filename in quotations.\n\n\n10.1.0.1 Note on stringsAsFactors\nNote that the read.table {utils} family of functions has an argument called stringsAsFactors, which by default will take the value of default.stringsAsFactors().\nType out default.stringsAsFactors() in the console to check what the default value is for your current R session. Is it TRUE or FALSE?\nIf default.stringsAsFactors() is set to TRUE, then stringsAsFactors = TRUE. In that case any function in this family of functions will coerce character columns in the data you are reading in to factor columns (i.e. coerce from vector to factor) in the resulting data frame.\nIf you want to maintain the character vector data structure (e.g. for gene names), you will want to make sure that stringsAsFactors = FALSE (or that default.stringsAsFactors() is set to FALSE).\n\n\n10.1.1 List of functions for data inspection\nWe already saw how the functions head() and str() (in the releveling section) can be useful to check the content and the structure of a data.frame. Below is a non-exhaustive list of functions to get a sense of the content/structure of data. The list has been divided into functions that work on all types of objects, some that work only on vectors/factors (1 dimensional objects), and others that work on data frames and matrices (2 dimensional objects).\nWe have some exercises below that will allow you to gain more familiarity with these. You will definitely be using some of them in the next few homework sections.\n\nAll data structures - content display:\n\nstr(): compact display of data contents (similar to what you see in the Global environment)\nclass(): displays the data type for vectors (e.g. character, numeric, etc.) and data structure for dataframes, matrices, lists\nsummary(): detailed display of the contents of a given object, including descriptive statistics, frequencies\nhead(): prints the first 6 entries (elements for 1-D objects, rows for 2-D objects)\ntail(): prints the last 6 entries (elements for 1-D objects, rows for 2-D objects)\n\nVector and factor variables:\n\nlength(): returns the number of elements in a vector or factor\n\nDataframe and matrix variables:\n\ndim(): returns dimensions of the dataset (number_of_rows, number_of_columns) [Note, row numbers will always be displayed before column numbers in R]\nnrow(): returns the number of rows in the dataset\nncol(): returns the number of columns in the dataset\nrownames(): returns the row names in the dataset\n\ncolnames(): returns the column names in the dataset\n\n\nExercises\n\nRead the tab-delimited project-summary.txt file in the data folder it in to R using read.table() and store it as the variable proj_summary. As you use read.table(), keep in mind that:\n\nall the columns in the input text file have column names\nyou want the first column of the text file to be used as row names (hint: look up the input for the row.names = argument in read.table())\n\nDisplay the contents of proj_summary in your console\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/pvals.html#interpreting-p-values",
    "href": "session2/pvals.html#interpreting-p-values",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.1 Interpreting p values",
    "text": "11.1 Interpreting p values\nLet’s start by checking our understanding of a p value.\n\n\n\n\n\n\nAre these statements correct or incorrect interpretations of p values?\n\nWe can use the quantity \\(1-p\\) to represent the probability that the alternative hypothesis is true.\nA p value can let us know how incompatible an observation is with a specified statistical model.\nA p value tells us how likely we would be to randomly see the observed value with minimal assumptions.\nA p value indicates an important result."
  },
  {
    "objectID": "session2/pvals.html#p-value-hacking",
    "href": "session2/pvals.html#p-value-hacking",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.2 P-value hacking",
    "text": "11.2 P-value hacking\nLet’s go back to the coin tossing example. We did not reject the null hypothesis (that the coin is fair) at a level of 5%—even though we “knew” that it is unfair. After all, probHead was chosen as 0.6. Let’s suppose we now start looking at different test statistics. Perhaps the number of consecutive series of 3 or more heads. Or the number of heads in the first 50 coin flips. And so on. A t some point we will find a test that happens to result in a small p-value, even if just by chance (after all, the probability for the p-value to be less than 0.05 under the null hypothesis—fair coin—is one in twenty).\nThere is a xkcd comic which illustrates this issue in the context of selective reporting. We just did what is called p-value hacking. You see what the problem is: in our zeal to prove our point we tortured the data until some statistic did what we wanted. A related tactic is hypothesis switching or HARKing – hypothesizing after the results are known: we have a dataset, maybe we have invested a lot of time and money into assembling it, so we need results. We come up with lots of different null hypotheses and test statistics, test them, and iterate, until we can report something.\n\n\n\nLet’s try running our binomial test on a fair coin, and see what we get:\n\nnumFlips = 100\nprobHead = 0.5\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nnumHeads <- sum(coinFlips == \"H\")\npval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\npval\n\n[1] 0.4841184\n\n\nThis p value is probably relatively large. But what if we keep on repeating the experiment?\n\n#Let's make a function for performing our experiment\nflip_coin <- function(numFlips, probHead){\n  numFlips = 100\n  probHead = 0.50\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n    replace = TRUE, prob = c(probHead, 1 - probHead))\n  numHeads <- sum(coinFlips == \"H\")\n  pval <- binom.test(x = numHeads, n = numFlips, p = 0.5)$p.value\n  return(pval)\n}\n\n#And then run it 10000 times\nparray <- replicate(10000, flip_coin(1000, 0.5), simplify=TRUE)\nhist(parray, breaks=100)\n\n\n\nmin(parray)\n\n[1] 3.216002e-05"
  },
  {
    "objectID": "session2/pvals.html#the-multiple-testing-problem",
    "href": "session2/pvals.html#the-multiple-testing-problem",
    "title": "11  P Values and Multiple Hypotheses",
    "section": "11.3 The Multiple Testing Problem",
    "text": "11.3 The Multiple Testing Problem\nIn modern biology, we are often conducting hundreds or thousands of statistical tests on high-throughput data. This means that even a low false positive rate can cause there to be a large number of cases where we falsely reject the null hypothesis. Luckily, there are ways we can correct our rejection threshold or p values to limit the type I error."
  },
  {
    "objectID": "session2/mult.html#definitions",
    "href": "session2/mult.html#definitions",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.1 Definitions",
    "text": "12.1 Definitions\nLet’s redefine our error table from earlier, in the framework of multiple hypotheses. Thus, each of the following variables represents a count out of the total number of tests performed.\n\n\n\nTest vs reality\nNull is true\nNull is false\nTotal\n\n\n\n\nRejected\n\\(V\\)\n\\(S\\)\n\\(R\\)\n\n\nNot Rejected\n\\(U\\)\n\\(T\\)\n\\(m - R\\)\n\n\nTotal\n\\(m_0\\)\n\\(m-m_0\\)\n\\(m\\)\n\n\n\n\n\\(m\\): total number of tests (and null hypotheses)\n\\(m_0\\): number of true null hypotheses\n\\(m - m_0\\): number of false null hypotheses\n\\(V\\): number of false positives (a measure of type I error)\n\\(T\\): number of false negatives (a measure of type II error)\n\\(S, U\\): number of true positives and true negatives\n\\(R\\): number of rejections"
  },
  {
    "objectID": "session2/mult.html#family-wise-error-rate",
    "href": "session2/mult.html#family-wise-error-rate",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.2 Family wise error rate",
    "text": "12.2 Family wise error rate\nThe family wise error rate (FWER) is the probability that $V>0$, i.e., that we make one or more false positive errors.\nWe can compute it as the complement of making no false positive errors at all. Recall that \\(\\alpha\\) is our probability threshold for rejecting the null hypothesis.\n\\[\nP(V>0) = 1 - P(V=0) = 1 - (1-\\alpha)^{m_0}\n\\]\nNote that, as \\(m_0\\) approaches \\(\\infty\\), the FWER approaches 1. In other words, with enough tests we are guaranteed to have at least 1 false positive."
  },
  {
    "objectID": "session2/mult.html#bonferroni-method",
    "href": "session2/mult.html#bonferroni-method",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.3 Bonferroni method",
    "text": "12.3 Bonferroni method\nThe Bonferroni method uses the FWER to adjust \\(\\alpha\\) such that we can choose a false positive rate across all tests. In other words, to control the FWER to the level \\(\\alpha_{FWER}\\) a new threshold is chosen, \\(\\alpha = \\alpha_{FWER}/m\\).\nThis means that, for \\(10000\\) tests, to set \\(alpha_{FWER} = 0.05\\) our new p value threshold for individual tests would be \\(5 \\times 10{-6}\\). Often FWER control is too conservative, and would lead to an ineffective use of the time and money that was spent to generate and assemble the data."
  },
  {
    "objectID": "session2/mult.html#false-discovery-rate",
    "href": "session2/mult.html#false-discovery-rate",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.4 False discovery rate",
    "text": "12.4 False discovery rate\nThe false discovery rate takes a more relaxed approach than Bonferroni correction. Instead of trying to have no or a fixed total rate of false positives, what if we allowed a small proportion of our null hypothesis rejections to be false positives?\nIt uses the total number of null hypotheses rejected to inform what is an acceptable number of false positive errors to let through. It makes the claim that, for instance, making \\(4\\) type I errors out of \\(10\\) rejected null hypotheses is a worse error than making \\(20\\) type I errors out of \\(100\\) rejected null hypotheses.\nTo see an example, we will load up the RNA-Seq dataset airway, which contains gene expression measurements (gene-level counts) of four primary human airway smooth muscle cell lines with and without treatment with dexamethasone, a synthetic glucocorticoid.\nConceptually, the tested null hypothesis is similar to that of the t-test, although the details are slightly more involved since we are dealing with count data.\n\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\nlibrary(\"tidyverse\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\n# This next line filters out NA p values from the dataset\nawde = as.data.frame(results(aw)) |> dplyr::filter(!is.na(pvalue))\n\nIn this dataset, we have performed a statistical test for each of \\(33,469\\) measured genes. We can look at a histogram of the p values:\n\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)\n\n\n\n\nLet’s say we reject the null hypothesis for all p values less than \\(\\alpha\\). We can see how many null hypotheses we reject:\n\nalpha <- 0.025\n\n# Recall that TRUE and FALSE are stored as 0 and 1, so we can sum to get a count\nsum(awde$pvalue <= alpha)\n\n[1] 4772\n\n\nAnd we can estimate \\(V\\), how many false positives we have:\n\nalpha * nrow(awde)\n\n[1] 836.725\n\n\nWe can then estimate the fraction of false rejections as:\n\n(alpha * nrow(awde))/sum(awde$pvalue <= alpha)\n\n[1] 0.1753405\n\n\nFormally, the false discovery rate (FDR) is defined as: \\[\nFDR = E\\left[\\frac{V}{max(R,1)}\\right]\n\\] Which is the average proportion of rejections that are false rejections."
  },
  {
    "objectID": "session2/mult.html#the-benjamini-hochberg-algorithm-for-controlling-the-fdr",
    "href": "session2/mult.html#the-benjamini-hochberg-algorithm-for-controlling-the-fdr",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.5 The Benjamini-Hochberg algorithm for controlling the FDR",
    "text": "12.5 The Benjamini-Hochberg algorithm for controlling the FDR\nThe Benjamini-Hochberg algorithm controls for a chosen FDR threshold via the following steps:\n\nFirst, order the p values in increasing order, \\(p_{(1)}...p_{(m)}\\)\nThen for some choice of the target FDR, \\(\\varphi\\), find the largest value of \\(k\\) that satisfies \\(p_{(k)} < \\varphi k/m\\)\nReject hypotheses \\(1\\) through \\(k\\)\n\nWe can see how this procedure works when applied to our RNA-Seq p value distribution:\n\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")\n\n\n\n\nWe find the rightmost point where our p-values and the expected null false discoveries intersect, then reject all tests to the left."
  },
  {
    "objectID": "session2/mult.html#multiple-hypothesis-correction-in-r",
    "href": "session2/mult.html#multiple-hypothesis-correction-in-r",
    "title": "12  Multiple Hypothesis Correction",
    "section": "12.6 Multiple Hypothesis Correction in R",
    "text": "12.6 Multiple Hypothesis Correction in R\nWe can use Bonferroni correction or the Benjamini-Hochberg algorithm using the function p.adjust.\n\np.adjust(awde$pvalue, method=\"bonferroni\")\np.adjust(awde$pvalue, method=\"BH\")"
  },
  {
    "objectID": "session2/making_functions.html#functions-and-their-arguments",
    "href": "session2/making_functions.html#functions-and-their-arguments",
    "title": "13  Functions",
    "section": "13.1 Functions and their arguments",
    "text": "13.1 Functions and their arguments\n\n13.1.1 What are functions?\nA key feature of R is functions. Functions are “self contained” modules of code that accomplish a specific task. Functions usually take in some sort of data structure (value, vector, dataframe etc.), process it, and return a result.\nThe general usage for a function is the name of the function followed by parentheses:\nfunction_name(input)\nThe input(s) are called arguments, which can include:\n\nthe physical object (any data structure) on which the function carries out a task\nspecifications that alter the way the function operates (e.g. options)\n\nNot all functions take arguments, for example:\ngetwd()\nHowever, most functions can take several arguments. If you don’t specify a required argument when calling the function, you will either receive an error or the function will fall back on using a default.\nThe defaults represent standard values that the author of the function specified as being “good enough in standard cases”. An example would be what symbol to use in a plot. However, if you want something specific, simply change the argument yourself with a value of your choice.\n\n\n13.1.2 Basic functions\nWe have already used a few examples of basic functions in the previous lessons i.e getwd(), c(), and factor(). These functions are available as part of R’s built in capabilities, and we will explore a few more of these base functions below.\nLet’s revisit a function that we have used previously to combine data c() into vectors. The arguments it takes is a collection of numbers, characters or strings (separated by a comma). The c() function performs the task of combining the numbers or characters into a single vector. You can also use the function to add elements to an existing vector:\n\nglengths <- c(4.6, 3000, 50000)\nglengths <- c(glengths, 90) # adding at the end \nglengths <- c(30, glengths) # adding at the beginning\n\nWhat happens here is that we take the original vector glengths (containing three elements), and we are adding another item to either end. We can do this over and over again to build a vector or a dataset.\nSince R is used for statistical computing, many of the base functions involve mathematical operations. One example would be the function sqrt(). The input/argument must be a number, and the output is the square root of that number. Let’s try finding the square root of 81:\n\nsqrt(81)\n\n[1] 9\n\n\nNow what would happen if we called the function (e.g. ran the function), on a vector of values instead of a single value?\n\nsqrt(glengths)\n\n[1]   5.477226   2.144761  54.772256 223.606798   9.486833\n\n\nIn this case the task was performed on each individual value of the vector glengths and the respective results were displayed.\nLet’s try another function, this time using one that we can change some of the options (arguments that change the behavior of the function), for example round:\n\nround(3.14159)\n\n[1] 3\n\n\nWe can see that we get 3. That’s because the default is to round to the nearest whole number. What if we want a different number of significant digits? Let’s first learn how to find available arguments for a function.\n\n\n13.1.3 Seeking help on arguments for functions\nThe best way of finding out this information is to use the ? followed by the name of the function. Doing this will open up the help manual in the bottom right panel of RStudio that will provide a description of the function, usage, arguments, details, and examples:\n?round\nAlternatively, if you are familiar with the function but just need to remind yourself of the names of the arguments, you can use:\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\nEven more useful is the example() function. This will allow you to run the examples section from the Online Help to see exactly how it works when executing the commands. Let’s try that for round():\n\nexample(\"round\")\n\n\nround> round(.5 + -2:4) # IEEE / IEC rounding: -2  0  0  2  2  4  4\n[1] -2  0  0  2  2  4  4\n\nround> ## (this is *good* behaviour -- do *NOT* report it as bug !)\nround> \nround> ( x1 <- seq(-2, 4, by = .5) )\n [1] -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0\n\nround> round(x1) #-- IEEE / IEC rounding !\n [1] -2 -2 -1  0  0  0  1  2  2  2  3  4  4\n\nround> x1[trunc(x1) != floor(x1)]\n[1] -1.5 -0.5\n\nround> x1[round(x1) != floor(x1 + .5)]\n[1] -1.5  0.5  2.5\n\nround> (non.int <- ceiling(x1) != floor(x1))\n [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n[13] FALSE\n\nround> x2 <- pi * 100^(-1:3)\n\nround> round(x2, 3)\n[1]       0.031       3.142     314.159   31415.927 3141592.654\n\nround> signif(x2, 3)\n[1] 3.14e-02 3.14e+00 3.14e+02 3.14e+04 3.14e+06\n\n\nIn our example, we can change the number of digits returned by adding an argument. We can type digits=2 or however many we may want:\n\nround(3.14159, digits=2)\n\n[1] 3.14\n\n\n\nNOTE: If you provide the arguments in the exact same order as they are defined (in the help manual) you don’t have to name them:\nround(3.14159, 2)\nHowever, it’s usually not recommended practice because it involves a lot of memorization. In addition, it makes your code difficult to read for your future self and others, especially if your code includes functions that are not commonly used. (It’s however OK to not include the names of the arguments for basic functions like mean, min, etc…). Another advantage of naming arguments, is that the order doesn’t matter. This is useful when a function has many arguments.\n\nExercise\n\n\n\n\n\n\nBasic\n\n\n\n\nLet’s use base R function to calculate mean value of the glengths vector. You might need to search online to find what function can perform this task.\nCreate a new vector test <- c(1, NA, 2, 3, NA, 4). Use the same base R function from exercise 1 (with addition of proper argument), and calculate mean value of the test vector. The output should be 2.5. > NOTE: In R, missing values are represented by the symbol NA (not available). It’s a way to make sure that users know they have missing data, and make a conscious decision on how to deal with it. There are ways to ignore NA during statistical calculation, or to remove NA from the vector. If you want more information related to missing data or NA you can go to this page (please note that there are many advanced concepts on that page that have not been covered in class).\nAnother commonly used base function is sort(). Use this function to sort the glengths vector in descending order.\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\n\nUse rnorm and the matrix functions to create a random square matrix with \\(6\\) rows/columns.\nCalculate the mean of each row in the matrix, so you should have 6 means total.\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nCreate vector c_data <- c(1, NA, 2, 3, NA, 4, 4, 3, 2, NA, NA, 2, 4, 2, 3, 4, 4, 2, 1, NA, 1, 1, 1). Fill in the NA values with the mean of all non-missing values.\nRe-create the vector with its NAs. Instead of filling in the missing data with the mean, estimate the parameter of a Poisson distribution from the data and sample from it to fill in the missing data.\n\n\n\n\n\n\n13.1.4 User-defined Functions\nOne of the great strengths of R is the user’s ability to add functions. Sometimes there is a small task (or series of tasks) you need done and you find yourself having to repeat it multiple times. In these types of situations, it can be helpful to create your own custom function. The structure of a function is given below:\nname_of_function <- function(argument1, argument2) {\n    statements or code that does something\n    return(something)\n}\n\nFirst you give your function a name.\nThen you assign value to it, where the value is the function.\n\nWhen defining the function you will want to provide the list of arguments required (inputs and/or options to modify behaviour of the function), and wrapped between curly brackets place the tasks that are being executed on/using those arguments. The argument(s) can be any type of object (like a scalar, a matrix, a dataframe, a vector, a logical, etc), and it’s not necessary to define what it is in any way.\nFinally, you can “return” the value of the object from the function, meaning pass the value of it into the global environment. The important idea behind functions is that objects that are created within the function are local to the environment of the function – they don’t exist outside of the function.\nLet’s try creating a simple example function. This function will take in a numeric value as input, and return the squared value.\n\nsquare_it <- function(x) {\n    square <- x * x\n    return(square)\n}\n\nOnce you run the code, you should see a function named square_it in the Environment panel (located at the top right of Rstudio interface). Now, we can use this function as any other base R functions. We type out the name of the function, and inside the parentheses we provide a numeric value x:\n\nsquare_it(5)\n\n[1] 25\n\n\nPretty simple, right? In this case, we only had one line of code that was run, but in theory you could have many lines of code to get obtain the final results that you want to “return” to the user.\n\n13.1.4.1 Do I always have to return() something at the end of the function?\nIn the example above, we created a new variable called square inside the function, and then return the value of square. If you don’t use return(), by default R will return the value of the last line of code inside that function. That is to say, the following function will also work.\n\nsquare_it <- function(x) {\n   x * x\n}\n\nHowever, we recommend always using return at the end of a function as the best practice.\n\nWe have only scratched the surface here when it comes to creating functions! We will revisit this in later lessons, but if interested you can also find more detailed information on this R-bloggers site, which is where we adapted this example from.\nExercise\n\n\n\n\n\n\nBasic\n\n\n\n\nLet’s create a function temp_conv(), which converts the temperature in Fahrenheit (input) to the temperature in Kelvin (output).\n\nWe could perform a two-step calculation: first convert from Fahrenheit to Celsius, and then convert from Celsius to Kelvin.\nThe formula for these two calculations are as follows: temp_c = (temp_f - 32) * 5 / 9; temp_k = temp_c + 273.15. To test your function,\nif your input is 70, the result of temp_conv(70) should be 294.2611.\n\nNow we want to round the temperature in Kelvin (output of temp_conv()) to a single decimal place. Use the round() function with the newly-created temp_conv() function to achieve this in one line of code. If your input is 70, the output should now be 294.3.\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nThe Fibonacci sequence is \\(0, 1, 1, 2, 3, 5, 8, ...\\) where the first two terms are 0 and 1, and for all other terms \\(n^{th}\\) term is the sum of the \\((n-1)^{th}\\) and \\((n-2)^{th}\\) terms.\n\nWrite a function fibonacci which takes in a single integer argument n and returns the \\(n^{th}\\) term in the Fibonacci sequence.\nHave your function stop with an appropriate message if the argument n is not an integer. Stop allows you to create your own errors in R. This StackOverflow thread contains useful informtion on how to tell if something is or is not an integer in R.\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRe-write your fibonacci function so that it calculates the Fibonacci sequence recursively, meaning that it calls itself. Your function should contain no loops or iterative code.\nYou will need to define two base cases, where the function does not call itself.\n\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session2/session2problems.html",
    "href": "session2/session2problems.html",
    "title": "14  Practice Exercises",
    "section": "",
    "text": "Basic\n\n\n\nIn a spreadsheet editor like excel or Google sheets, open the file ../data/messy_temperature_data.csv.\n\nWhat problems will arise when we load this data into R? If you’re unsure, try it out and take a look at the data. Are the columns the types you expected? Does the data appear correct?\nInside your spreadsheet editor of choice, fix the problems with the data. Save it under a new file name in your data folder (so that the original data file is not overwritten).\nLoad the dataset into R.\nWhat are the dimensions of the dataset? How rows and columns does it have?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nAs opposed to manually fixing the problems with this dataset, try to fix the dataset problems using R.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nTry loading the dataset ../data/corrupted_data.txt. Take a look at the the gene symbols. Some of the gene symbols appear to be dates! This is actually a common problem in biology.\nTry installing the HGCNhelper package and using it to correct the date-converted gene symbols.\n\n\n\nWorking with distributions\n\n\n\n\n\n\n\nBasic\n\n\n\nGenerate 100 instances of a Poisson(3) random variable.\n\nWhat is the mean?\nWhat is the variance as computed by the R function var?\n\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nConduct a binomial test for the following scenario: out of 1 million reads, 19 reads are mapped to a gene of interest, with the probability for mapping a read to that gene being \\(10^{-5}\\).\n\nAre these more or less reads than we would expect to be mapped to that gene?\nIs the finding statistically significant?\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a function, bh_correction, which takes in a vector of p-values and a target FDR, performs the Benjamini-Hochberg procedure, and returns a vector of p-values which should be rejected at that FDR."
  },
  {
    "objectID": "session2/pSet2.html#problem-1",
    "href": "session2/pSet2.html#problem-1",
    "title": "15  Problem Set 2",
    "section": "15.1 Problem 1",
    "text": "15.1 Problem 1\nWrite a function to compute the probability of having a maximum as big as m when looking across n Poisson variables with rate lambda. Give these arguments default values in your function declaration."
  },
  {
    "objectID": "session2/pSet2.html#problem-2",
    "href": "session2/pSet2.html#problem-2",
    "title": "15  Problem Set 2",
    "section": "15.2 Problem 2",
    "text": "15.2 Problem 2\nLet’s answer a question about C. elegans genome nucleotide frequency: Is the mitochondrial sequence of C. elegans consistent with a model of equally likely nucleotides?\nSetup: This is our opportunity to use Bioconductor for the first time. Since Bioconductor’s package management is more tightly controlled than CRAN’s, we need to use a special install function (from the BiocManager package) to install Bioconductor packages.\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"Biostrings\", \"BSgenome.Celegans.UCSC.ce2\"))\n\nAfter that, we can load the genome sequence package as we load any other R packages.\n\nlibrary(\"BSgenome.Celegans.UCSC.ce2\",quietly = TRUE )\n\nWarning: package 'BSgenome' was built under R version 4.2.2\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n    table, tapply, union, unique, unsplit, which.max, which.min\n\n\nWarning: package 'S4Vectors' was built under R version 4.2.2\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following object is masked from 'package:grDevices':\n\n    windows\n\n\nWarning: package 'GenomeInfoDb' was built under R version 4.2.2\n\n\nWarning: package 'GenomicRanges' was built under R version 4.2.2\n\n\n\nAttaching package: 'Biostrings'\n\n\nThe following object is masked from 'package:base':\n\n    strsplit\n\nCelegans\n\nWorm genome:\n# organism: Caenorhabditis elegans (Worm)\n# genome: ce2\n# provider: UCSC\n# release date: Mar. 2004\n# 7 sequences:\n#   chrI   chrII  chrIII chrIV  chrV   chrX   chrM                              \n# (use 'seqnames()' to see all the sequence names, use the '$' or '[[' operator\n# to access a given sequence)\n\nseqnames(Celegans)\n\n[1] \"chrI\"   \"chrII\"  \"chrIII\" \"chrIV\"  \"chrV\"   \"chrX\"   \"chrM\"  \n\nCelegans$chrM\n\n13794-letter DNAString object\nseq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\n\nclass(Celegans$chrM)\n\n[1] \"DNAString\"\nattr(,\"package\")\n[1] \"Biostrings\"\n\n\nWe can take advantage of the Biostrings library to get base counts:\n\nlibrary(\"Biostrings\", quietly = TRUE)\nlfM = letterFrequency(Celegans$chrM, letters=c(\"A\", \"C\", \"G\", \"T\"))\nlfM\n\n   A    C    G    T \n4335 1225 2055 6179 \n\n\nTest whether the C. elegans data is consistent with the uniform model (all nucleotide frequencies the same) using a simulation. For the purposes of this simulation, we can assume that all base pairs are independent from each other. Your solution should compute a simulated p-value based on \\(10,000\\) simulations.\nHint: The multinomial distribution is similar to the binomial distribution but can model experiments with more than \\(2\\) outcomes. For instance suppose we have 8 characters of four different, equally likely types:\n\npvec = rep(1/4, 4)\nt(rmultinom(1, prob = pvec, size = 8))\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    3    2    1"
  },
  {
    "objectID": "session2/pSet2.html#problem-3",
    "href": "session2/pSet2.html#problem-3",
    "title": "15  Problem Set 2",
    "section": "15.3 Problem 3",
    "text": "15.3 Problem 3\nInstead of testing across the entire mitochondria, let’s now see if we can find certain nucleotides being enriched locally. To do this, split up the mitochondrial sequence into 100 base pair chunks, and perform your test from problem \\(3\\) on each chunk. Perform a multiple hypothesis correction at an FDR of \\(0.01\\).\n\nThe materials in this lesson have been adapted from: Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Systems Immunology Workshops",
    "section": "",
    "text": "Systems Immunology Workshops\nWe will be working from this workbook for our first 4 workshop sessions.\nBefore the first session, be sure to complete all pre-work steps.\nMany of the exercise in this workbook have three levels: basic, advanced and challenge.\n\n\n\n\n\n\nBasic\n\n\n\nThis exercise is appropriate to those new to R/programming. Being able to complete these is enough to fulfill the objectives of the workshop.\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nThis exercise is appropriate for those new to R/programming who have already completed the basic exercise and want a challenge or those who already have computational experience. It may assume knowledge of concepts not yet covered in workshops.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nThis exercise provides an extra challenge and is geared towards those with significant computational experience. It may assume knowledge of concepts not yet covered in workshops."
  }
]